#!/usr/bin/env python3
"""
CLIPå¿«é€Ÿå¼€å§‹ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªç®€å•çš„CLIPæ¨¡å‹æ¨ç†ç¤ºä¾‹ï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ã€‚
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    import torch
    from transformers import CLIPModel, CLIPProcessor
    from PIL import Image
    import requests
    from io import BytesIO
except ImportError as e:
    print("âŒ ç¼ºå°‘ä¾èµ–åº“ï¼Œè¯·å…ˆå®‰è£…:")
    print("   pip install torch transformers pillow requests")
    print(f"\né”™è¯¯è¯¦æƒ…: {e}")
    sys.exit(1)


def download_sample_image():
    """ä¸‹è½½ç¤ºä¾‹å›¾åƒ"""
    print("ğŸ“¥ ä¸‹è½½ç¤ºä¾‹å›¾åƒ...")
    
    # ä½¿ç”¨ä¸€å¼ å…¬å¼€çš„ç¤ºä¾‹å›¾åƒ
    image_url = "https://images.unsplash.com/photo-1543466835-00a7907e9de1?w=400"
    
    try:
        response = requests.get(image_url, timeout=10)
        response.raise_for_status()
        image = Image.open(BytesIO(response.content))
        print("âœ… å›¾åƒä¸‹è½½æˆåŠŸ")
        return image
    except Exception as e:
        print(f"âš ï¸  ä¸‹è½½å¤±è´¥: {e}")
        print("ğŸ’¡ å°†åˆ›å»ºä¸€ä¸ªæ¼”ç¤ºå›¾åƒ...")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¼”ç¤ºå›¾åƒ
        image = Image.new('RGB', (224, 224), color=(73, 109, 137))
        return image


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸš€ CLIP å¿«é€Ÿå¼€å§‹ç¤ºä¾‹")
    print("=" * 70)
    
    # 1. æ£€æŸ¥è®¾å¤‡
    print("\nğŸ“Š æ­¥éª¤ 1/4: æ£€æŸ¥ç¯å¢ƒ")
    print("-" * 70)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    
    # 2. åŠ è½½æ¨¡å‹
    print("\nğŸ“¥ æ­¥éª¤ 2/4: åŠ è½½CLIPæ¨¡å‹")
    print("-" * 70)
    print("æ­£åœ¨åŠ è½½ openai/clip-vit-base-patch32...")
    
    try:
        model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        model = model.to(device)
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. æˆ–å…ˆä¸‹è½½æ¨¡å‹: ./scripts/download_models.sh clip")
        sys.exit(1)
    
    # 3. å‡†å¤‡æ•°æ®
    print("\nğŸ–¼ï¸  æ­¥éª¤ 3/4: å‡†å¤‡æµ‹è¯•æ•°æ®")
    print("-" * 70)
    
    # å‡†å¤‡å›¾åƒ
    image = download_sample_image()
    
    # å‡†å¤‡å€™é€‰æ–‡æœ¬
    texts = [
        "a photo of a dog",
        "a photo of a cat",
        "a photo of a car",
        "a photo of a bird",
        "a photo of a flower"
    ]
    
    print(f"å›¾åƒå°ºå¯¸: {image.size}")
    print(f"å€™é€‰æ–‡æœ¬æ•°: {len(texts)}")
    print(f"å€™é€‰æ–‡æœ¬: {texts}")
    
    # 4. æ¨ç†
    print("\nğŸ”® æ­¥éª¤ 4/4: æ‰§è¡Œæ¨ç†")
    print("-" * 70)
    
    try:
        # é¢„å¤„ç†
        inputs = processor(
            text=texts,
            images=image,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(**inputs)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1)
        
        print("âœ… æ¨ç†å®Œæˆ\n")
        
        # æ˜¾ç¤ºç»“æœ
        print("ğŸ“Š å›¾æ–‡åŒ¹é…ç»“æœ:")
        print("-" * 70)
        
        results = []
        for i, (text, prob) in enumerate(zip(texts, probs[0])):
            results.append((text, prob.item()))
        
        # æŒ‰æ¦‚ç‡æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        for i, (text, prob) in enumerate(results, 1):
            bar_length = int(prob * 50)
            bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
            print(f"{i}. {text:30s} {bar} {prob:6.2%}")
        
        print("\n" + "=" * 70)
        print("ğŸ‰ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹å®Œæˆï¼")
        print("=" * 70)
        
        # ä¸‹ä¸€æ­¥æç¤º
        print("\nğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ :")
        print("   1. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: docs/01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/")
        print("   2. è¿è¡ŒåŸºå‡†æµ‹è¯•: ./scripts/run_benchmarks.sh")
        print("   3. å°è¯•æ¨¡å‹å¾®è°ƒ: notebooks/01_lora_finetuning_tutorial.ipynb")
        print("   4. æ›´å¤šç¤ºä¾‹ä»£ç : code/01-model-evaluation/examples/")
        
        print("\nğŸ’¡ æç¤º:")
        print("   - ä¿®æ”¹ texts åˆ—è¡¨æ¥æµ‹è¯•ä¸åŒçš„æ–‡æœ¬")
        print("   - ä½¿ç”¨è‡ªå·±çš„å›¾åƒ: image = Image.open('your_image.jpg')")
        print("   - æŸ¥çœ‹å®Œæ•´æ•™ç¨‹: docs/05-ä½¿ç”¨è¯´æ˜/02-å¿«é€Ÿå¼€å§‹.md")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

