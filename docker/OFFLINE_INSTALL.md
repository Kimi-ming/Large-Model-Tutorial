# Dockerç¦»çº¿å®‰è£…æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨ç¦»çº¿ç¯å¢ƒä¸­æ„å»ºå’Œä½¿ç”¨Dockeré•œåƒã€‚

---

## ğŸ“‹ ç›®å½•

- [åä¸ºæ˜‡è…¾ç¦»çº¿å®‰è£…](#åä¸ºæ˜‡è…¾ç¦»çº¿å®‰è£…)
- [NVIDIA GPUç¦»çº¿å®‰è£…](#nvidia-gpuç¦»çº¿å®‰è£…)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## åä¸ºæ˜‡è…¾ç¦»çº¿å®‰è£…

### 1. å‡†å¤‡å·¥ä½œ

#### 1.1 ä¸‹è½½åŸºç¡€é•œåƒ

åœ¨æœ‰ç½‘ç»œçš„ç¯å¢ƒä¸­ä¸‹è½½åŸºç¡€é•œåƒï¼š

```bash
# ä¸‹è½½æ˜‡è…¾åŸºç¡€é•œåƒ
docker pull ascendhub.huawei.com/public-ascendhub/ascend-pytorch:23.0.RC3-ubuntu18.04

# ä¿å­˜é•œåƒä¸ºtaræ–‡ä»¶
docker save ascendhub.huawei.com/public-ascendhub/ascend-pytorch:23.0.RC3-ubuntu18.04 \
    -o ascend-pytorch-23.0.RC3.tar

# å‹ç¼©ï¼ˆå¯é€‰ï¼‰
gzip ascend-pytorch-23.0.RC3.tar
```

#### 1.2 å‡†å¤‡Pythonä¾èµ–åŒ…

åˆ›å»ºä¸‹è½½è„šæœ¬ `download_wheels.sh`ï¼š

```bash
#!/bin/bash
# ä¸‹è½½æ‰€æœ‰Pythonä¾èµ–åŒ…

mkdir -p wheels/ascend

# åŸºç¡€ä¾èµ–
pip download -d wheels/ascend \
    transformers==4.35.0 \
    pillow==10.1.0 \
    opencv-python==4.8.1.78 \
    numpy==1.24.3 \
    pandas==2.1.3 \
    scikit-learn==1.3.2 \
    albumentations==1.3.1 \
    peft==0.6.2 \
    accelerate==0.25.0 \
    onnx==1.15.0 \
    onnxruntime==1.16.3 \
    fastapi==0.104.1 \
    uvicorn==0.24.0 \
    pydantic==2.5.0 \
    python-multipart==0.0.6 \
    pyyaml==6.0.1 \
    python-dotenv==1.0.0 \
    loguru==0.7.2 \
    matplotlib==3.8.2 \
    seaborn==0.13.0 \
    tqdm==4.66.1 \
    requests==2.31.0 \
    aiohttp==3.9.1 \
    huggingface-hub==0.19.4 \
    datasets==2.15.0 \
    fire==0.5.0

echo "âœ… ä¾èµ–åŒ…ä¸‹è½½å®Œæˆï¼Œä¿å­˜åœ¨ wheels/ascend/ ç›®å½•"
```

æ‰§è¡Œä¸‹è½½ï¼š

```bash
chmod +x download_wheels.sh
./download_wheels.sh
```

#### 1.3 å‡†å¤‡torch-npuï¼ˆé‡è¦ï¼‰

**æ³¨æ„**ï¼štorch-npu **ä¸èƒ½**ä»PyPIå®‰è£…ï¼

æ–¹å¼ä¸€ï¼šä»åä¸ºå®˜æ–¹é•œåƒæºä¸‹è½½ï¼ˆæ¨èï¼‰

```bash
# è®¿é—®åä¸ºæ˜‡è…¾ç¤¾åŒºè·å–å¯¹åº”CANNç‰ˆæœ¬çš„torch-npuåŒ…
# https://www.hiascend.com/software/cann/community

# ç¤ºä¾‹ï¼šCANN 7.0 å¯¹åº”çš„ torch-npu
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/PyTorch/torch_npu/... \
    -O wheels/ascend/torch_npu-2.1.0-py3-none-linux_aarch64.whl
```

æ–¹å¼äºŒï¼šä»å·²å®‰è£…ç¯å¢ƒå¤åˆ¶

```bash
# å¦‚æœæœ‰å·²å®‰è£…torch-npuçš„ç¯å¢ƒï¼Œç›´æ¥å¤åˆ¶wheelåŒ…
cp /path/to/torch_npu-*.whl wheels/ascend/
```

æ–¹å¼ä¸‰ï¼šä½¿ç”¨åŸºç¡€é•œåƒè‡ªå¸¦çš„

åŸºç¡€é•œåƒ `ascend-pytorch:23.0.RC3` å·²åŒ…å« torch å’Œ torch_npuï¼Œæ— éœ€é¢å¤–å®‰è£…ã€‚

### 2. ä¿®æ”¹Dockerfileæ”¯æŒç¦»çº¿å®‰è£…

åˆ›å»º `Dockerfile.huawei.offline`ï¼š

```dockerfile
FROM ascendhub.huawei.com/public-ascendhub/ascend-pytorch:23.0.RC3-ubuntu18.04

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8

ENV ASCEND_HOME=/usr/local/Ascend \
    LD_LIBRARY_PATH=/usr/local/Ascend/driver/lib64:/usr/local/Ascend/add-ons:$LD_LIBRARY_PATH \
    PATH=/usr/local/Ascend/latest/bin:$PATH \
    PYTHONPATH=/usr/local/Ascend/latest/python/site-packages:$PYTHONPATH

WORKDIR /workspace

# å®‰è£…ç³»ç»Ÿä¾èµ–ï¼ˆéœ€è¦ç¼“å­˜debåŒ…ï¼‰
# å¦‚æœå®Œå…¨ç¦»çº¿ï¼Œéœ€è¦æå‰ä¸‹è½½debåŒ…
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    vim \
    libopencv-dev \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    htop \
    tmux \
    && rm -rf /var/lib/apt/lists/*

RUN python3 -m pip install --upgrade pip setuptools wheel

# å¤åˆ¶ç¦»çº¿ä¾èµ–åŒ…
COPY wheels/ascend /tmp/wheels

# ä»æœ¬åœ°wheelç›®å½•å®‰è£…ä¾èµ–ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
RUN pip install --no-index --find-links=/tmp/wheels \
    transformers \
    pillow \
    opencv-python \
    numpy \
    pandas \
    scikit-learn \
    albumentations \
    peft \
    accelerate \
    onnx \
    onnxruntime \
    fastapi \
    uvicorn \
    pydantic \
    python-multipart \
    pyyaml \
    python-dotenv \
    loguru \
    matplotlib \
    seaborn \
    tqdm \
    requests \
    aiohttp \
    huggingface-hub \
    datasets \
    fire

# éªŒè¯torch-npuï¼ˆåŸºç¡€é•œåƒå·²åŒ…å«ï¼‰
RUN python -c "import torch; import torch_npu; print(f'âœ… torch version: {torch.__version__}'); print(f'âœ… torch_npu available')" || \
    echo "âš ï¸  torch-npuéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥åŸºç¡€é•œåƒ"

# å¤åˆ¶é¡¹ç›®ä»£ç 
COPY . /workspace/

ENV PYTHONPATH=/workspace:$PYTHONPATH

RUN mkdir -p /workspace/logs \
    /workspace/outputs \
    /workspace/models \
    /workspace/data \
    /workspace/checkpoints

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
RUN rm -rf /tmp/wheels

EXPOSE 8000 8888

HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import torch; import torch_npu; assert torch.npu.is_available()" || exit 1

CMD ["/bin/bash"]
```

### 3. ç¦»çº¿æ„å»ºæ­¥éª¤

#### 3.1 ä¼ è¾“æ–‡ä»¶åˆ°ç¦»çº¿æœåŠ¡å™¨

```bash
# æ‰“åŒ…æ‰€æœ‰éœ€è¦çš„æ–‡ä»¶
tar -czf offline-build.tar.gz \
    ascend-pytorch-23.0.RC3.tar \
    wheels/ \
    Dockerfile.huawei.offline \
    .

# ä¼ è¾“åˆ°ç¦»çº¿æœåŠ¡å™¨
scp offline-build.tar.gz user@offline-server:/path/to/build/
```

#### 3.2 åœ¨ç¦»çº¿æœåŠ¡å™¨ä¸Šæ„å»º

```bash
# è§£å‹
cd /path/to/build/
tar -xzf offline-build.tar.gz

# åŠ è½½åŸºç¡€é•œåƒ
docker load -i ascend-pytorch-23.0.RC3.tar

# æ„å»ºé•œåƒ
docker build -f Dockerfile.huawei.offline \
    -t large-model-tutorial:ascend-offline \
    .
```

### 4. éªŒè¯å®‰è£…

```bash
# å¯åŠ¨å®¹å™¨
docker run --device=/dev/davinci0 \
    --device=/dev/davinci_manager \
    --device=/dev/devmm_svm \
    --device=/dev/hisi_hdc \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -it --rm \
    large-model-tutorial:ascend-offline \
    /bin/bash

# åœ¨å®¹å™¨å†…éªŒè¯
python -c "
import torch
import torch_npu

print(f'âœ… PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'âœ… NPUæ˜¯å¦å¯ç”¨: {torch.npu.is_available()}')
print(f'âœ… NPUè®¾å¤‡æ•°: {torch.npu.device_count()}')
"
```

---

## NVIDIA GPUç¦»çº¿å®‰è£…

### 1. å‡†å¤‡å·¥ä½œ

#### 1.1 ä¸‹è½½åŸºç¡€é•œåƒ

```bash
# ä¸‹è½½NVIDIA CUDAé•œåƒ
docker pull nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04

# ä¿å­˜é•œåƒ
docker save nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 \
    -o nvidia-cuda-11.8.tar

gzip nvidia-cuda-11.8.tar
```

#### 1.2 ä¸‹è½½Pythonä¾èµ–

```bash
#!/bin/bash
mkdir -p wheels/nvidia

# PyTorchï¼ˆCUDA 11.8ç‰ˆæœ¬ï¼‰
pip download -d wheels/nvidia \
    torch==2.0.1+cu118 \
    torchvision==0.15.2+cu118 \
    torchaudio==2.0.2+cu118 \
    --index-url https://download.pytorch.org/whl/cu118

# ONNX Runtime GPUç‰ˆæœ¬
pip download -d wheels/nvidia \
    onnxruntime-gpu==1.16.3

# å…¶ä»–ä¾èµ–
pip download -d wheels/nvidia \
    -r requirements-gpu.txt

echo "âœ… NVIDIAä¾èµ–åŒ…ä¸‹è½½å®Œæˆ"
```

### 2. ä¿®æ”¹Dockerfile

åˆ›å»º `Dockerfile.nvidia.offline`ï¼š

```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    CUDA_HOME=/usr/local/cuda \
    PATH=/usr/local/cuda/bin:$PATH \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

WORKDIR /workspace

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3.10-dev \
    python3-pip \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    libopencv-dev \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    vim \
    htop \
    tmux \
    && rm -rf /var/lib/apt/lists/*

RUN ln -sf /usr/bin/python3.10 /usr/bin/python && \
    ln -sf /usr/bin/python3.10 /usr/bin/python3 && \
    python -m pip install --upgrade pip setuptools wheel

# å¤åˆ¶ç¦»çº¿ä¾èµ–åŒ…
COPY wheels/nvidia /tmp/wheels

# ç¦»çº¿å®‰è£…PyTorch
RUN pip install --no-index --find-links=/tmp/wheels \
    torch torchvision torchaudio

# ç¦»çº¿å®‰è£…å…¶ä»–ä¾èµ–
RUN pip install --no-index --find-links=/tmp/wheels \
    onnxruntime-gpu \
    transformers \
    # ... å…¶ä»–ä¾èµ– ...

COPY . /workspace/

ENV PYTHONPATH=/workspace:$PYTHONPATH

RUN mkdir -p /workspace/logs \
    /workspace/outputs \
    /workspace/models \
    /workspace/data \
    /workspace/checkpoints

RUN rm -rf /tmp/wheels

EXPOSE 8000 8888 6006

HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import torch; assert torch.cuda.is_available()" || exit 1

CMD ["/bin/bash"]
```

### 3. ç¦»çº¿æ„å»º

```bash
# ä¼ è¾“æ–‡ä»¶
tar -czf offline-build-nvidia.tar.gz \
    nvidia-cuda-11.8.tar \
    wheels/ \
    Dockerfile.nvidia.offline \
    .

# åœ¨ç¦»çº¿æœåŠ¡å™¨ä¸Š
docker load -i nvidia-cuda-11.8.tar
docker build -f Dockerfile.nvidia.offline \
    -t large-model-tutorial:nvidia-offline \
    .
```

---

## å¸¸è§é—®é¢˜

### Q1: torch-npuå®‰è£…å¤±è´¥

**é—®é¢˜**ï¼šç›´æ¥ `pip install torch-npu` å¤±è´¥æˆ–ä¸‹è½½åˆ°CPUç‰ˆæœ¬

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨åŸºç¡€é•œåƒè‡ªå¸¦çš„torch-npuï¼ˆæ¨èï¼‰
2. ä»åä¸ºå®˜æ–¹æ¸ é“è·å–å¯¹åº”CANNç‰ˆæœ¬çš„wheelåŒ…
3. ä¸è¦ä½¿ç”¨PyPIä¸Šçš„é€šç”¨åŒ…

**éªŒè¯æ–¹æ³•**ï¼š
```python
import torch
import torch_npu

# æ­£ç¡®çš„torch-npuä¼šæœ‰è¿™äº›æ–¹æ³•
assert hasattr(torch, 'npu')
assert torch.npu.is_available()
print(f"âœ… NPUè®¾å¤‡: {torch.npu.get_device_name(0)}")
```

### Q2: onnxruntime-gpuåœ¨ç¦»çº¿ç¯å¢ƒå®‰è£…å¤±è´¥

**é—®é¢˜**ï¼šonnxruntime-gpuä¾èµ–CUDAåº“ï¼Œç¦»çº¿ç¯å¢ƒå¯èƒ½ç¼ºå¤±

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿åŸºç¡€é•œåƒåŒ…å«CUDA Runtime
2. ä½¿ç”¨å®Œæ•´çš„CUDAå¼€å‘é•œåƒï¼ˆcudnn8-develï¼‰
3. é¢„å…ˆä¸‹è½½æ‰€æœ‰ä¾èµ–ï¼š
```bash
pip download onnxruntime-gpu==1.16.3 -d wheels/
```

### Q3: å¦‚ä½•éªŒè¯ç¦»çº¿é•œåƒçš„å®Œæ•´æ€§

**éªŒè¯è„šæœ¬**ï¼š

```bash
#!/bin/bash
# verify_offline_image.sh

echo "ğŸ” éªŒè¯ç¦»çº¿é•œåƒ..."

# å¯åŠ¨å®¹å™¨
CONTAINER_ID=$(docker run -d large-model-tutorial:ascend-offline sleep 3600)

# éªŒè¯PythonåŒ…
docker exec $CONTAINER_ID python -c "
import torch
import transformers
import onnxruntime
print('âœ… æ‰€æœ‰ä¾èµ–åŒ…å¯¼å…¥æˆåŠŸ')
"

# éªŒè¯NPU
docker exec $CONTAINER_ID python -c "
import torch
import torch_npu
assert torch.npu.is_available(), 'NPUä¸å¯ç”¨'
print(f'âœ… NPUéªŒè¯æˆåŠŸ: {torch.npu.get_device_name(0)}')
"

# æ¸…ç†
docker stop $CONTAINER_ID
docker rm $CONTAINER_ID

echo "âœ… ç¦»çº¿é•œåƒéªŒè¯å®Œæˆ"
```

### Q4: ç¦»çº¿ç¯å¢ƒå¦‚ä½•æ›´æ–°æ¨¡å‹

**æ–¹æ¡ˆä¸€ï¼šæ‰“åŒ…æ¨¡å‹åˆ°é•œåƒ**

```dockerfile
# åœ¨Dockerfileä¸­
COPY models/clip-vit-base-patch32 /workspace/models/clip-vit-base-patch32
```

**æ–¹æ¡ˆäºŒï¼šä½¿ç”¨æ•°æ®å·**

```bash
# åœ¨æœ‰ç½‘ç»œçš„ç¯å¢ƒä¸‹è½½æ¨¡å‹
python -c "
from transformers import CLIPModel, CLIPProcessor
model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')
processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')
model.save_pretrained('./models/clip-vit-base-patch32')
processor.save_pretrained('./models/clip-vit-base-patch32')
"

# ä¼ è¾“åˆ°ç¦»çº¿ç¯å¢ƒ
tar -czf models.tar.gz models/

# åœ¨ç¦»çº¿ç¯å¢ƒä½¿ç”¨
docker run -v $(pwd)/models:/workspace/models \
    large-model-tutorial:ascend-offline
```

### Q5: debåŒ…ä¾èµ–ç¼ºå¤±

å¦‚æœå®Œå…¨ç¦»çº¿ï¼ˆæ— æ³•è®¿é—®aptæºï¼‰ï¼Œéœ€è¦æå‰ä¸‹è½½debåŒ…ï¼š

```bash
# åœ¨æœ‰ç½‘ç»œçš„ç¯å¢ƒ
apt-get download $(apt-cache depends --recurse --no-recommends \
    build-essential cmake git wget curl \
    libopencv-dev libglib2.0-0 libsm6 libxext6 | \
    grep "^\w" | sort -u)

# æ‰“åŒ…debæ–‡ä»¶
mkdir debs
mv *.deb debs/
tar -czf debs.tar.gz debs/
```

åœ¨Dockerfileä¸­ï¼š

```dockerfile
COPY debs /tmp/debs
RUN dpkg -i /tmp/debs/*.deb || apt-get install -f -y
RUN rm -rf /tmp/debs
```

---

## ğŸ“ æ£€æŸ¥æ¸…å•

ç¦»çº¿æ„å»ºå‰çš„æ£€æŸ¥æ¸…å•ï¼š

- [ ] åŸºç¡€Dockeré•œåƒå·²ä¸‹è½½å¹¶ä¿å­˜
- [ ] Python wheelåŒ…å·²å…¨éƒ¨ä¸‹è½½
- [ ] torch-npu wheelåŒ…å·²å‡†å¤‡ï¼ˆæ˜‡è…¾ï¼‰
- [ ] onnxruntime-gpuå·²ä¸‹è½½ï¼ˆNVIDIAï¼‰
- [ ] ç³»ç»ŸdebåŒ…å·²å‡†å¤‡ï¼ˆå¦‚éœ€å®Œå…¨ç¦»çº¿ï¼‰
- [ ] é¢„è®­ç»ƒæ¨¡å‹å·²ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
- [ ] Dockerfileå·²ä¿®æ”¹ä¸ºç¦»çº¿æ¨¡å¼
- [ ] éªŒè¯è„šæœ¬å·²å‡†å¤‡

---

## ğŸ”— å‚è€ƒèµ„æº

**åä¸ºæ˜‡è…¾**ï¼š
- æ˜‡è…¾ç¤¾åŒºï¼šhttps://www.hiascend.com/
- CANNæ–‡æ¡£ï¼šhttps://www.hiascend.com/document/
- torch-npuä»“åº“ï¼šhttps://gitee.com/ascend/pytorch

**NVIDIA**ï¼š
- CUDAé•œåƒï¼šhttps://hub.docker.com/r/nvidia/cuda
- PyTorchå®˜æ–¹ï¼šhttps://pytorch.org/
- ONNX Runtimeï¼šhttps://onnxruntime.ai/

---

**æç¤º**ï¼šç¦»çº¿å®‰è£…æ­¥éª¤è¾ƒä¸ºå¤æ‚ï¼Œå»ºè®®å…ˆåœ¨æœ‰ç½‘ç»œçš„æµ‹è¯•ç¯å¢ƒéªŒè¯æµç¨‹ï¼Œå†åº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒã€‚

