{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ€§èƒ½ä¼˜åŒ–å®æˆ˜æ•™ç¨‹\n",
    "\n",
    "## ğŸ“‹ æ•™ç¨‹æ¦‚è¿°\n",
    "\n",
    "æœ¬æ•™ç¨‹ä¸“æ³¨äºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¼˜åŒ–,æ¶µç›–æ¨ç†åŠ é€Ÿã€æ˜¾å­˜ä¼˜åŒ–ã€æ‰¹å¤„ç†ç­‰å…³é”®æŠ€æœ¯ã€‚\n",
    "\n",
    "**ä¼˜åŒ–ç›®æ ‡**:\n",
    "- âš¡ æå‡æ¨ç†é€Ÿåº¦ (2-5x)\n",
    "- ğŸ’¾ é™ä½æ˜¾å­˜å ç”¨ (30-50%)\n",
    "- ğŸ“Š æé«˜ååé‡ (3-10x)\n",
    "- ğŸ¯ ä¿æŒæ¨¡å‹ç²¾åº¦\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬æ•™ç¨‹å,æ‚¨å°†å­¦ä¼š:\n",
    "1. âœ… æ¨¡å‹é‡åŒ–æŠ€æœ¯ (INT8/FP16/BF16)\n",
    "2. âœ… æ‰¹å¤„ç†ä¼˜åŒ–ç­–ç•¥\n",
    "3. âœ… KV Cacheä¼˜åŒ–\n",
    "4. âœ… Flash AttentionåŠ é€Ÿ\n",
    "5. âœ… æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "6. âœ… æ€§èƒ½profilingå’Œåˆ†æ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å‰ç½®è¦æ±‚\n",
    "\n",
    "- Python 3.8+\n",
    "- PyTorch 2.0+\n",
    "- CUDA 11.7+ (æ¨è)\n",
    "- 16GB+ GPUæ˜¾å­˜\n",
    "- ç†è§£æ¨¡å‹æ¨ç†åŸºç¡€\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– æ•™ç¨‹ç»“æ„\n",
    "\n",
    "1. **åŸºå‡†æµ‹è¯•** - å»ºç«‹æ€§èƒ½åŸºçº¿\n",
    "2. **ç²¾åº¦ä¼˜åŒ–** - é‡åŒ–å’Œæ··åˆç²¾åº¦\n",
    "3. **æ˜¾å­˜ä¼˜åŒ–** - Gradient Checkpointingå’ŒKV Cache\n",
    "4. **æ¨ç†ä¼˜åŒ–** - æ‰¹å¤„ç†å’Œå¹¶å‘\n",
    "5. **é«˜çº§ä¼˜åŒ–** - Flash Attentionå’Œç¼–è¯‘å™¨ä¼˜åŒ–\n",
    "6. **æ€§èƒ½åˆ†æ** - Profilingå’Œç“¶é¢ˆè¯†åˆ«\n",
    "7. **æœ€ä½³å®è·µ** - ä¼˜åŒ–ç­–ç•¥æ€»ç»“\n",
    "\n",
    "**é¢„è®¡å®Œæˆæ—¶é—´**: 45-60åˆ†é’Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install torch torchvision transformers pillow datasets accelerate bitsandbytes flash-attn --upgrade -q\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForImageTextToText\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ£€æŸ¥ç¯å¢ƒ\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"æ”¯æŒBFloat16: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŸºå‡†æµ‹è¯•\n",
    "\n",
    "### 2.1 æ€§èƒ½æµ‹è¯•å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"æ€§èƒ½æµ‹è¯•ç»“æœ\"\"\"\n",
    "    name: str\n",
    "    avg_latency: float  # å¹³å‡å»¶è¿Ÿ(ç§’)\n",
    "    throughput: float   # ååé‡(samples/sec)\n",
    "    memory_allocated: float  # æ˜¾å­˜åˆ†é…(GB)\n",
    "    memory_reserved: float   # æ˜¾å­˜ä¿ç•™(GB)\n",
    "    peak_memory: float       # å³°å€¼æ˜¾å­˜(GB)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "ğŸ“Š {self.name}\n",
    "  â±ï¸  å¹³å‡å»¶è¿Ÿ: {self.avg_latency:.3f}s\n",
    "  ğŸš€ ååé‡: {self.throughput:.2f} samples/s\n",
    "  ğŸ’¾ æ˜¾å­˜åˆ†é…: {self.memory_allocated:.2f} GB\n",
    "  ğŸ“¦ æ˜¾å­˜ä¿ç•™: {self.memory_reserved:.2f} GB\n",
    "  â¬†ï¸  å³°å€¼æ˜¾å­˜: {self.peak_memory:.2f} GB\n",
    "\"\"\"\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "    \n",
    "    def benchmark(\n",
    "        self,\n",
    "        name: str,\n",
    "        func: Callable,\n",
    "        num_runs: int = 10,\n",
    "        warmup_runs: int = 3\n",
    "    ) -> BenchmarkResult:\n",
    "        \"\"\"æ‰§è¡Œæ€§èƒ½æµ‹è¯•\"\"\"\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        # é¢„çƒ­\n",
    "        for _ in range(warmup_runs):\n",
    "            func()\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # æµ‹è¯•\n",
    "        latencies = []\n",
    "        for _ in tqdm(range(num_runs), desc=f\"æµ‹è¯• {name}\"):\n",
    "            start = time.perf_counter()\n",
    "            func()\n",
    "            if self.device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            latencies.append(end - start)\n",
    "        \n",
    "        avg_latency = np.mean(latencies)\n",
    "        throughput = 1.0 / avg_latency\n",
    "        \n",
    "        # æ˜¾å­˜ç»Ÿè®¡\n",
    "        if self.device == \"cuda\":\n",
    "            memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "        else:\n",
    "            memory_allocated = memory_reserved = peak_memory = 0.0\n",
    "        \n",
    "        result = BenchmarkResult(\n",
    "            name=name,\n",
    "            avg_latency=avg_latency,\n",
    "            throughput=throughput,\n",
    "            memory_allocated=memory_allocated,\n",
    "            memory_reserved=memory_reserved,\n",
    "            peak_memory=peak_memory\n",
    "        )\n",
    "        \n",
    "        self.results.append(result)\n",
    "        print(result)\n",
    "        return result\n",
    "    \n",
    "    def compare_results(self):\n",
    "        \"\"\"å¯¹æ¯”æ‰€æœ‰ç»“æœ\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"âŒ æ²¡æœ‰æµ‹è¯•ç»“æœ\")\n",
    "            return\n",
    "        \n",
    "        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"æ–¹æ³•\": r.name,\n",
    "                \"å»¶è¿Ÿ(s)\": f\"{r.avg_latency:.3f}\",\n",
    "                \"ååé‡\": f\"{r.throughput:.2f}\",\n",
    "                \"æ˜¾å­˜(GB)\": f\"{r.peak_memory:.2f}\"\n",
    "            }\n",
    "            for r in self.results\n",
    "        ])\n",
    "        \n",
    "        print(\"\\nğŸ“Š æ€§èƒ½å¯¹æ¯”:\")\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        self._plot_comparison()\n",
    "    \n",
    "    def _plot_comparison(self):\n",
    "        \"\"\"å¯è§†åŒ–å¯¹æ¯”\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        names = [r.name for r in self.results]\n",
    "        latencies = [r.avg_latency for r in self.results]\n",
    "        throughputs = [r.throughput for r in self.results]\n",
    "        memories = [r.peak_memory for r in self.results]\n",
    "        \n",
    "        # å»¶è¿Ÿå¯¹æ¯”\n",
    "        axes[0].bar(names, latencies, color='skyblue')\n",
    "        axes[0].set_title('å¹³å‡å»¶è¿Ÿå¯¹æ¯”')\n",
    "        axes[0].set_ylabel('æ—¶é—´(ç§’)')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # ååé‡å¯¹æ¯”\n",
    "        axes[1].bar(names, throughputs, color='lightgreen')\n",
    "        axes[1].set_title('ååé‡å¯¹æ¯”')\n",
    "        axes[1].set_ylabel('samples/sec')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # æ˜¾å­˜å¯¹æ¯”\n",
    "        axes[2].bar(names, memories, color='coral')\n",
    "        axes[2].set_title('å³°å€¼æ˜¾å­˜å¯¹æ¯”')\n",
    "        axes[2].set_ylabel('æ˜¾å­˜(GB)')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"âœ… æ€§èƒ½æµ‹è¯•å·¥å…·å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 å‡†å¤‡æµ‹è¯•æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¤ºä¾‹å›¾åƒ\n",
    "def create_test_image(size=(224, 224)):\n",
    "    \"\"\"åˆ›å»ºæµ‹è¯•å›¾åƒ\"\"\"\n",
    "    return Image.new('RGB', size, color=(73, 109, 137))\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "test_image = create_test_image()\n",
    "test_prompt = \"Describe this image in detail.\"\n",
    "\n",
    "print(\"âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ\")\n",
    "test_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç²¾åº¦ä¼˜åŒ–\n",
    "\n",
    "### 3.1 åŸºçº¿æ¨¡å‹ (Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®\n",
    "model_name = \"OpenGVLab/InternVL2-2B\"  # ä½¿ç”¨è¾ƒå°æ¨¡å‹æ¼”ç¤º\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# åˆå§‹åŒ–benchmark\n",
    "benchmark = PerformanceBenchmark(device)\n",
    "\n",
    "# åŠ è½½Float32æ¨¡å‹\n",
    "print(\"ğŸ”„ åŠ è½½Float32åŸºçº¿æ¨¡å‹...\")\n",
    "model_fp32 = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "print(\"âœ… Float32æ¨¡å‹åŠ è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºçº¿æµ‹è¯•\n",
    "def inference_fp32():\n",
    "    inputs = processor(images=test_image, text=test_prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_fp32.generate(**inputs, max_new_tokens=50)\n",
    "    return outputs\n",
    "\n",
    "result_fp32 = benchmark.benchmark(\"Float32 åŸºçº¿\", inference_fp32, num_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Float16ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†æ˜¾å­˜\n",
    "del model_fp32\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# åŠ è½½Float16æ¨¡å‹\n",
    "print(\"ğŸ”„ åŠ è½½Float16æ¨¡å‹...\")\n",
    "model_fp16 = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "print(\"âœ… Float16æ¨¡å‹åŠ è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float16æµ‹è¯•\n",
    "def inference_fp16():\n",
    "    inputs = processor(images=test_image, text=test_prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_fp16.generate(**inputs, max_new_tokens=50)\n",
    "    return outputs\n",
    "\n",
    "result_fp16 = benchmark.benchmark(\"Float16\", inference_fp16, num_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 BFloat16ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    # æ¸…ç†æ˜¾å­˜\n",
    "    del model_fp16\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # åŠ è½½BFloat16æ¨¡å‹\n",
    "    print(\"ğŸ”„ åŠ è½½BFloat16æ¨¡å‹...\")\n",
    "    model_bf16 = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    ).to(device)\n",
    "    \n",
    "    print(\"âœ… BFloat16æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "    \n",
    "    # BFloat16æµ‹è¯•\n",
    "    def inference_bf16():\n",
    "        inputs = processor(images=test_image, text=test_prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bf16.generate(**inputs, max_new_tokens=50)\n",
    "        return outputs\n",
    "    \n",
    "    result_bf16 = benchmark.benchmark(\"BFloat16\", inference_bf16, num_runs=5)\nelse:\n",
    "    print(\"âš ï¸ å½“å‰GPUä¸æ”¯æŒBFloat16,è·³è¿‡æµ‹è¯•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 INT8é‡åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # æ¸…ç†æ˜¾å­˜\n",
    "    if 'model_bf16' in locals():\n",
    "        del model_bf16\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # INT8é‡åŒ–é…ç½®\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ”„ åŠ è½½INT8é‡åŒ–æ¨¡å‹...\")\n",
    "    model_int8 = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… INT8æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "    \n",
    "    # INT8æµ‹è¯•\n",
    "    def inference_int8():\n",
    "        inputs = processor(images=test_image, text=test_prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model_int8.generate(**inputs, max_new_tokens=50)\n",
    "        return outputs\n",
    "    \n",
    "    result_int8 = benchmark.benchmark(\"INT8é‡åŒ–\", inference_int8, num_runs=5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ INT8é‡åŒ–å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ‰¹å¤„ç†ä¼˜åŒ–\n",
    "\n",
    "### 4.1 å•æ ·æœ¬ vs æ‰¹å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨FP16æ¨¡å‹è¿›è¡Œæ‰¹å¤„ç†æµ‹è¯•\n",
    "if 'model_int8' in locals():\n",
    "    del model_int8\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# é‡æ–°åŠ è½½FP16æ¨¡å‹\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# å‡†å¤‡æ‰¹é‡æ•°æ®\n",
    "batch_sizes = [1, 2, 4, 8]\n",
    "test_images = [create_test_image() for _ in range(8)]\n",
    "test_prompts = [\"Describe this image.\" for _ in range(8)]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    def inference_batch():\n",
    "        images = test_images[:batch_size]\n",
    "        prompts = test_prompts[:batch_size]\n",
    "        \n",
    "        inputs = processor(images=images, text=prompts, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "        return outputs\n",
    "    \n",
    "    benchmark.benchmark(f\"Batch={batch_size}\", inference_batch, num_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KV Cacheä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸ä½¿ç”¨KV Cache\n",
    "def inference_no_cache():\n",
    "    inputs = processor(images=test_image, text=test_prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100, use_cache=False)\n",
    "    return outputs\n",
    "\n",
    "result_no_cache = benchmark.benchmark(\"æ— KV Cache\", inference_no_cache, num_runs=5)\n",
    "\n",
    "# ä½¿ç”¨KV Cache\n",
    "def inference_with_cache():\n",
    "    inputs = processor(images=test_image, text=test_prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "    return outputs\n",
    "\n",
    "result_with_cache = benchmark.benchmark(\"ä½¿ç”¨KV Cache\", inference_with_cache, num_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½åˆ†æ\n",
    "\n",
    "### 6.1 å¯¹æ¯”æ‰€æœ‰ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºå®Œæ•´å¯¹æ¯”\n",
    "benchmark.compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 è®¡ç®—åŠ é€Ÿæ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—ç›¸å¯¹åŸºçº¿çš„æ”¹è¿›\n",
    "if benchmark.results:\n",
    "    baseline = benchmark.results[0]  # Float32åŸºçº¿\n",
    "    \n",
    "    print(\"\\nğŸš€ ä¼˜åŒ–æ•ˆæœæ€»ç»“:\\n\")\n",
    "    for result in benchmark.results[1:]:\n",
    "        speedup = baseline.avg_latency / result.avg_latency\n",
    "        memory_saving = (baseline.peak_memory - result.peak_memory) / baseline.peak_memory * 100\n",
    "        \n",
    "        print(f\"ğŸ“Š {result.name}:\")\n",
    "        print(f\"  âš¡ é€Ÿåº¦æå‡: {speedup:.2f}x\")\n",
    "        print(f\"  ğŸ’¾ æ˜¾å­˜èŠ‚çœ: {memory_saving:.1f}%\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æœ€ä½³å®è·µæ€»ç»“\n",
    "\n",
    "### 7.1 ä¼˜åŒ–ç­–ç•¥å†³ç­–æ ‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_guide = \"\"\"\n",
    "## ğŸ¯ ä¼˜åŒ–ç­–ç•¥å†³ç­–æ ‘\n",
    "\n",
    "### åœºæ™¯1: æ˜¾å­˜ä¸è¶³\n",
    "ä¼˜å…ˆçº§é¡ºåº:\n",
    "1. âœ… INT8é‡åŒ– (èŠ‚çœ50%+æ˜¾å­˜)\n",
    "2. âœ… Gradient Checkpointing\n",
    "3. âœ… å‡å°batch size\n",
    "4. âœ… ä½¿ç”¨æ›´å°çš„æ¨¡å‹\n",
    "\n",
    "### åœºæ™¯2: æ¨ç†å»¶è¿Ÿé«˜\n",
    "ä¼˜å…ˆçº§é¡ºåº:\n",
    "1. âœ… BFloat16/Float16 (2xåŠ é€Ÿ)\n",
    "2. âœ… å¯ç”¨KV Cache (1.5-2xåŠ é€Ÿ)\n",
    "3. âœ… Flash Attention (1.2-1.5xåŠ é€Ÿ)\n",
    "4. âœ… å¢å¤§batch size\n",
    "\n",
    "### åœºæ™¯3: ååé‡ä½\n",
    "ä¼˜å…ˆçº§é¡ºåº:\n",
    "1. âœ… æ‰¹å¤„ç† (3-10xæå‡)\n",
    "2. âœ… å¼‚æ­¥æ¨ç†\n",
    "3. âœ… æ¨¡å‹å¹¶è¡Œ\n",
    "4. âœ… å¤šGPUéƒ¨ç½²\n",
    "\n",
    "### åœºæ™¯4: ç²¾åº¦è¦æ±‚é«˜\n",
    "æ¨èé…ç½®:\n",
    "- ä½¿ç”¨BFloat16 (ä¿æŒç²¾åº¦)\n",
    "- é¿å…INT8é‡åŒ–\n",
    "- ä½¿ç”¨å…¨ç²¾åº¦è®­ç»ƒ\n",
    "\n",
    "### åœºæ™¯5: è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²\n",
    "æ¨èé…ç½®:\n",
    "- INT8é‡åŒ–å¿…é€‰\n",
    "- ä½¿ç”¨å°æ¨¡å‹ (1-2B)\n",
    "- å¯ç”¨æ‰€æœ‰ä¼˜åŒ–\n",
    "\n",
    "## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æ•ˆæœé¢„æœŸ\n",
    "\n",
    "| ä¼˜åŒ–æ–¹æ³• | é€Ÿåº¦æå‡ | æ˜¾å­˜èŠ‚çœ | ç²¾åº¦å½±å“ |\n",
    "|---------|---------|---------|----------|\n",
    "| Float16 | 2x | 50% | å¾®å° |\n",
    "| BFloat16 | 2x | 50% | å‡ ä¹æ—  |\n",
    "| INT8 | 1.5-2x | 75% | å° |\n",
    "| KV Cache | 1.5-2x | +20% | æ—  |\n",
    "| Batch=4 | 3x | +50% | æ—  |\n",
    "| Batch=8 | 5x | +100% | æ—  |\n",
    "| Flash Attention | 1.2-1.5x | -10% | æ—  |\n",
    "\"\"\"\n",
    "\n",
    "print(optimization_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 å®ç”¨ä»£ç æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ä½³å®è·µ:ç»„åˆå¤šç§ä¼˜åŒ–\n",
    "class OptimizedInference:\n",
    "    \"\"\"ä¼˜åŒ–çš„æ¨ç†ç±»\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        device: str = \"cuda\",\n",
    "        use_fp16: bool = True,\n",
    "        use_int8: bool = False,\n",
    "        batch_size: int = 4\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # ç²¾åº¦é…ç½®\n",
    "        if use_int8:\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        else:\n",
    "            dtype = torch.float16 if use_fp16 else torch.float32\n",
    "            self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=dtype,\n",
    "                trust_remote_code=True\n",
    "            ).to(device)\n",
    "        \n",
    "        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def __call__(self, images, prompts):\n",
    "        \"\"\"æ‰¹é‡æ¨ç†\"\"\"\n",
    "        # è‡ªåŠ¨æ‰¹å¤„ç†\n",
    "        results = []\n",
    "        for i in range(0, len(images), self.batch_size):\n",
    "            batch_images = images[i:i+self.batch_size]\n",
    "            batch_prompts = prompts[i:i+self.batch_size]\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                images=batch_images,\n",
    "                text=batch_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    use_cache=True  # å¯ç”¨KV Cache\n",
    "                )\n",
    "            \n",
    "            batch_results = self.processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            results.extend(batch_results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"âœ… ä¼˜åŒ–æ¨ç†ç±»å®šä¹‰å®Œæˆ\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "print(\"\\nğŸ“ ä½¿ç”¨ç¤ºä¾‹:\")\n",
    "print(\"\"\"\n",
    "# åˆå§‹åŒ–ä¼˜åŒ–æ¨ç†å™¨\n",
    "inference = OptimizedInference(\n",
    "    model_name=\"OpenGVLab/InternVL2-2B\",\n",
    "    use_fp16=True,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# æ‰¹é‡æ¨ç†\n",
    "images = [Image.open(path) for path in image_paths]\n",
    "prompts = [\"Describe this image.\" for _ in images]\n",
    "results = inference(images, prompts)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ€»ç»“\n",
    "\n",
    "### 8.1 å…³é”®æ”¶è·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ğŸ‰ æ€§èƒ½ä¼˜åŒ–å®æˆ˜å®Œæˆ!\n",
    "\n",
    "âœ… æŒæ¡æŠ€èƒ½:\n",
    "1. æ¨¡å‹é‡åŒ– (INT8/FP16/BF16)\n",
    "2. æ‰¹å¤„ç†ä¼˜åŒ–\n",
    "3. KV Cacheä½¿ç”¨\n",
    "4. æ€§èƒ½åŸºå‡†æµ‹è¯•\n",
    "5. æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥\n",
    "\n",
    "ğŸ“Š å…¸å‹ä¼˜åŒ–æ•ˆæœ:\n",
    "- é€Ÿåº¦: 2-5xæå‡\n",
    "- æ˜¾å­˜: 30-75%èŠ‚çœ\n",
    "- ååé‡: 3-10xæå‡\n",
    "\n",
    "ğŸ¯ æœ€ä½³å®è·µ:\n",
    "- ä¼˜å…ˆä½¿ç”¨BFloat16/Float16\n",
    "- å¯ç”¨KV Cache\n",
    "- åˆç†è®¾ç½®batch size\n",
    "- æ˜¾å­˜ä¸è¶³æ—¶è€ƒè™‘INT8\n",
    "\n",
    "ğŸ’¡ ä¸‹ä¸€æ­¥:\n",
    "1. åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨ä¼˜åŒ–\n",
    "2. é’ˆå¯¹ç‰¹å®šåœºæ™¯è°ƒä¼˜\n",
    "3. ç›‘æ§ç”Ÿäº§ç¯å¢ƒæ€§èƒ½\n",
    "4. æŒç»­ä¼˜åŒ–å’Œæ”¹è¿›\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 ç»ƒä¹ ä»»åŠ¡\n",
    "\n",
    "1. **åŸºç¡€ç»ƒä¹ **:\n",
    "   - æµ‹è¯•ä¸åŒbatch sizeçš„æ€§èƒ½\n",
    "   - å¯¹æ¯”ä¸åŒæ¨¡å‹å¤§å°çš„ä¼˜åŒ–æ•ˆæœ\n",
    "   - è®°å½•ä¼˜åŒ–å‰åçš„è¯¦ç»†æ•°æ®\n",
    "\n",
    "2. **è¿›é˜¶ç»ƒä¹ **:\n",
    "   - å®ç°åŠ¨æ€batch sizeè°ƒæ•´\n",
    "   - æ·»åŠ æ›´å¤šprofilingæŒ‡æ ‡\n",
    "   - å°è¯•Flash Attention\n",
    "\n",
    "3. **é«˜çº§ç»ƒä¹ **:\n",
    "   - å®ç°æ¨¡å‹å¹¶è¡Œ\n",
    "   - æ„å»ºä¼˜åŒ–pipeline\n",
    "   - è¿›è¡Œç«¯åˆ°ç«¯æ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å‚è€ƒèµ„æº\n",
    "\n",
    "- [PyTorchæ€§èƒ½ä¼˜åŒ–](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "- [HuggingFaceä¼˜åŒ–æ–‡æ¡£](https://huggingface.co/docs/transformers/perf_train_gpu_one)\n",
    "- [BitsAndBytesæ–‡æ¡£](https://github.com/TimDettmers/bitsandbytes)\n",
    "- [Flash Attention](https://github.com/Dao-AILab/flash-attention)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ æ­å–œå®Œæˆæ€§èƒ½ä¼˜åŒ–å®æˆ˜æ•™ç¨‹!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
