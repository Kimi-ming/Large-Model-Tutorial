{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ InternVL vs Qwen-VL æ¨¡å‹å¯¹æ¯”æ•™ç¨‹\n",
    "\n",
    "**æ¬¢è¿æ¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æ¯”æ•™ç¨‹ï¼**\n",
    "\n",
    "æœ¬æ•™ç¨‹å°†æŒ‡å¯¼æ‚¨å¯¹æ¯”InternVLå’ŒQwen-VLä¸¤ä¸ªSOTAè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œç‰¹ç‚¹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "- âœ… ç†è§£InternVLå’ŒQwen-VLçš„ç‰¹ç‚¹\n",
    "- âœ… å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°\n",
    "- âœ… å­¦ä¼šæ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n",
    "- âœ… æŒæ¡æ¨¡å‹æ¨ç†å’Œè¯„ä¼°æ–¹æ³•\n",
    "\n",
    "---\n",
    "\n",
    "## â±ï¸ é¢„è®¡å­¦ä¹ æ—¶é—´\n",
    "\n",
    "- å®Œæ•´è¿è¡Œï¼šçº¦ 45-60 åˆ†é’Ÿ\n",
    "- å¿«é€Ÿæµè§ˆï¼šçº¦ 15-20 åˆ†é’Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ å‰ç½®è¦æ±‚\n",
    "\n",
    "```bash\n",
    "# å®‰è£…ä¾èµ–\n",
    "pip install torch transformers>=4.37.2 accelerate pillow matplotlib\n",
    "pip install transformers_stream_generator  # For Qwen-VL\n",
    "```\n",
    "\n",
    "**ç¡¬ä»¶è¦æ±‚**ï¼š\n",
    "- GPU: 16GB+ æ˜¾å­˜ï¼ˆæ¨èï¼‰\n",
    "- CPU: å¯è¿è¡Œä½†è¾ƒæ…¢\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡\n",
    "\n",
    "## 1.1 å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€åº“\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "sys.path.append('..')\n",
    "\n",
    "# æ·±åº¦å­¦ä¹ \n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForImageTextToText,  # For InternVL\n",
    "    AutoModelForCausalLM,         # For Qwen-VL\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# å›¾åƒå¤„ç†\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è¿›åº¦æ¡\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# é…ç½®ç±»\nclass Config:\n    # æ¨¡å‹é…ç½®\n    internvl_model = \"OpenGVLab/InternVL2-8B\"\n    qwenvl_model = \"Qwen/Qwen-VL-Chat\"\n    \n    # è®¾å¤‡é…ç½®\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    dtype = \"bfloat16\" if torch.cuda.is_available() else \"float32\"\n    \n    # æ¨ç†é…ç½®\n    max_new_tokens = 512\n    temperature = 0.7\n    \n    # æµ‹è¯•å›¾åƒç›®å½•\n    image_dir = \"./test_images\"\n\nconfig = Config()\nprint(f\"ğŸ“‹ è®¾å¤‡: {config.device}\")\nprint(f\"ğŸ“‹ ç²¾åº¦: {config.dtype}\")\nprint(f\"ğŸ“‹ InternVLæ¨¡å‹: {config.internvl_model}\")\nprint(f\"ğŸ“‹ Qwen-VLæ¨¡å‹: {config.qwenvl_model}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 1.3 å‡†å¤‡æµ‹è¯•å›¾åƒ\n\næˆ‘ä»¬æä¾›ä¸‰ç§æ–¹å¼å‡†å¤‡æµ‹è¯•å›¾åƒ:\n1. **ä½¿ç”¨ç¤ºä¾‹å›¾åƒ** - ä»HuggingFaceä¸‹è½½ç¤ºä¾‹å›¾åƒ\n2. **ä½¿ç”¨æœ¬åœ°å›¾åƒ** - ä½¿ç”¨æ‚¨è‡ªå·±çš„å›¾åƒæ–‡ä»¶\n3. **åˆ›å»ºæµ‹è¯•å›¾åƒ** - ç”Ÿæˆç®€å•çš„æµ‹è¯•å›¾åƒ(ç”¨äºå¿«é€Ÿæ¼”ç¤º)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import requests\nfrom pathlib import Path\nfrom io import BytesIO\n\ndef prepare_test_images():\n    \"\"\"å‡†å¤‡æµ‹è¯•å›¾åƒ\"\"\"\n    # åˆ›å»ºå›¾åƒç›®å½•\n    os.makedirs(config.image_dir, exist_ok=True)\n    \n    # æ–¹æ³•1: ä»ç½‘ç»œä¸‹è½½ç¤ºä¾‹å›¾åƒ (æ¨è)\n    sample_urls = [\n        \"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/vision-image-colorization/data/tower.jpg\",\n        \"https://raw.githubusercontent.com/pytorch/pytorch.github.io/master/assets/images/cat.jpg\"\n    ]\n    \n    test_images = []\n    print(\"ğŸ“¥ ä¸‹è½½æµ‹è¯•å›¾åƒ...\")\n    \n    for idx, url in enumerate(sample_urls):\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            \n            # ä¿å­˜å›¾åƒ\n            image_path = os.path.join(config.image_dir, f\"test_{idx+1}.jpg\")\n            img = Image.open(BytesIO(response.content))\n            img.save(image_path)\n            test_images.append(image_path)\n            print(f\"âœ… ä¸‹è½½æˆåŠŸ: {image_path}\")\n        except Exception as e:\n            print(f\"âš ï¸ ä¸‹è½½å¤±è´¥ {url}: {e}\")\n    \n    # æ–¹æ³•2: å¦‚æœä¸‹è½½å¤±è´¥,åˆ›å»ºæµ‹è¯•å›¾åƒ\n    if not test_images:\n        print(\"âš ï¸ ç½‘ç»œå›¾åƒä¸‹è½½å¤±è´¥,åˆ›å»ºæœ¬åœ°æµ‹è¯•å›¾åƒ...\")\n        for idx in range(2):\n            image_path = os.path.join(config.image_dir, f\"test_{idx+1}.jpg\")\n            # åˆ›å»ºå½©è‰²æµ‹è¯•å›¾åƒ\n            import numpy as np\n            colors = [(255, 0, 0), (0, 255, 0)]  # çº¢è‰²å’Œç»¿è‰²\n            img_array = np.ones((224, 224, 3), dtype=np.uint8) * np.array(colors[idx])\n            img = Image.fromarray(img_array)\n            img.save(image_path)\n            test_images.append(image_path)\n            print(f\"âœ… åˆ›å»ºæµ‹è¯•å›¾åƒ: {image_path}\")\n    \n    # æ–¹æ³•3: æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æœ¬åœ°å›¾åƒ\n    local_images = list(Path(config.image_dir).glob(\"*.jpg\")) + list(Path(config.image_dir).glob(\"*.png\"))\n    if local_images and not test_images:\n        test_images = [str(p) for p in local_images[:2]]\n        print(f\"âœ… ä½¿ç”¨æœ¬åœ°å›¾åƒ: {len(test_images)}å¼ \")\n    \n    return test_images\n\n# å‡†å¤‡æµ‹è¯•å›¾åƒ\ntest_images = prepare_test_images()\nprint(f\"\\nğŸ‰ æµ‹è¯•å›¾åƒå‡†å¤‡å®Œæˆ! å…±{len(test_images)}å¼ \")\n\n# æ›´æ–°é…ç½®\nconfig.test_images = test_images\n\n# æ˜¾ç¤ºæµ‹è¯•å›¾åƒ\nif test_images:\n    fig, axes = plt.subplots(1, len(test_images), figsize=(12, 4))\n    if len(test_images) == 1:\n        axes = [axes]\n    for idx, img_path in enumerate(test_images):\n        img = Image.open(img_path)\n        axes[idx].imshow(img)\n        axes[idx].set_title(f\"Test Image {idx+1}\")\n        axes[idx].axis('off')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"âš ï¸ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•å›¾åƒ,è¯·æ‰‹åŠ¨å‡†å¤‡å›¾åƒå¹¶æ”¾åˆ°test_imagesç›®å½•\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ç¬¬äºŒéƒ¨åˆ†ï¼šåŠ è½½æ¨¡å‹\n",
    "\n",
    "## 2.1 åŠ è½½InternVLæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ åŠ è½½InternVLæ¨¡å‹...\")\n",
    "\n",
    "# è®¾ç½®ç²¾åº¦\n",
    "if config.dtype == \"bfloat16\":\n",
    "    dtype = torch.bfloat16\n",
    "elif config.dtype == \"float16\":\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "# åŠ è½½InternVL\n",
    "internvl_processor = AutoProcessor.from_pretrained(\n",
    "    config.internvl_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "internvl_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    config.internvl_model,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ").eval()\n",
    "\n",
    "print(\"âœ… InternVLæ¨¡å‹åŠ è½½æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 åŠ è½½Qwen-VLæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ åŠ è½½Qwen-VLæ¨¡å‹...\")\n",
    "\n",
    "# åŠ è½½Qwen-VL\n",
    "qwenvl_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.qwenvl_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "qwenvl_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.qwenvl_model,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype\n",
    ").eval()\n",
    "\n",
    "print(\"âœ… Qwen-VLæ¨¡å‹åŠ è½½æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®šä¹‰æ¨ç†å‡½æ•°\n",
    "\n",
    "## 3.1 InternVLæ¨ç†å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internvl_inference(image_path, question):\n",
    "    \"\"\"InternVLæ¨ç†\"\"\"\n",
    "    # åŠ è½½å›¾åƒ\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # å‡†å¤‡æ¶ˆæ¯\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # åº”ç”¨èŠå¤©æ¨¡æ¿\n",
    "    prompt = internvl_processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # å¤„ç†è¾“å…¥\n",
    "    inputs = internvl_processor(\n",
    "        text=prompt,\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "    inputs = {k: v.to(config.device) if isinstance(v, torch.Tensor) else v\n",
    "             for k, v in inputs.items()}\n",
    "    \n",
    "    # è®¡æ—¶å¼€å§‹\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ç”Ÿæˆ\n",
    "    with torch.no_grad():\n",
    "        outputs = internvl_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config.max_new_tokens,\n",
    "            temperature=config.temperature,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # è®¡æ—¶ç»“æŸ\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # è§£ç \n",
    "    response = internvl_processor.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "    \n",
    "    # æå–å›ç­”\n",
    "    if \"assistant\\n\" in response:\n",
    "        answer = response.split(\"assistant\\n\")[-1].strip()\n",
    "    else:\n",
    "        answer = response.strip()\n",
    "    \n",
    "    return answer, inference_time\n",
    "\n",
    "print(\"âœ… InternVLæ¨ç†å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Qwen-VLæ¨ç†å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwenvl_inference(image_path, question):\n",
    "    \"\"\"Qwen-VLæ¨ç†\"\"\"\n",
    "    # æ„å»ºæŸ¥è¯¢\n",
    "    query = qwenvl_tokenizer.from_list_format([\n",
    "        {'image': image_path},\n",
    "        {'text': question},\n",
    "    ])\n",
    "    \n",
    "    # è®¡æ—¶å¼€å§‹\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    response, _ = qwenvl_model.chat(\n",
    "        qwenvl_tokenizer,\n",
    "        query=query,\n",
    "        history=None,\n",
    "        max_length=config.max_new_tokens\n",
    "    )\n",
    "    \n",
    "    # è®¡æ—¶ç»“æŸ\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    return response, inference_time\n",
    "\n",
    "print(\"âœ… Qwen-VLæ¨ç†å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ç¬¬å››éƒ¨åˆ†ï¼šä»»åŠ¡å¯¹æ¯”\n",
    "\n",
    "## 4.1 å›¾åƒæè¿°ç”Ÿæˆå¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©æµ‹è¯•å›¾åƒ\n",
    "test_image = config.test_images[0]\n",
    "\n",
    "# é—®é¢˜\n",
    "question = \"Please describe this image in detail.\"\n",
    "\n",
    "print(\"ğŸ–¼ï¸ å›¾åƒæè¿°ç”Ÿæˆå¯¹æ¯”\\n\")\n",
    "print(f\"æµ‹è¯•å›¾åƒ: {test_image}\")\n",
    "print(f\"é—®é¢˜: {question}\\n\")\n",
    "\n",
    "# InternVL\n",
    "print(\"ğŸ“ InternVL:\")\n",
    "internvl_answer, internvl_time = internvl_inference(test_image, question)\n",
    "print(f\"å›ç­”: {internvl_answer}\")\n",
    "print(f\"è€—æ—¶: {internvl_time:.2f}ç§’\\n\")\n",
    "\n",
    "# Qwen-VL\n",
    "print(\"ğŸ“ Qwen-VL:\")\n",
    "qwenvl_answer, qwenvl_time = qwenvl_inference(test_image, question)\n",
    "print(f\"å›ç­”: {qwenvl_answer}\")\n",
    "print(f\"è€—æ—¶: {qwenvl_time:.2f}ç§’\\n\")\n",
    "\n",
    "# æ˜¾ç¤ºå›¾åƒ\n",
    "img = Image.open(test_image)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Test Image')\n",
    "plt.show()\n",
    "\n",
    "# æ€§èƒ½å¯¹æ¯”\n",
    "print(\"âš¡ æ€§èƒ½å¯¹æ¯”:\")\n",
    "print(f\"InternVLè€—æ—¶: {internvl_time:.2f}ç§’\")\n",
    "print(f\"Qwen-VLè€—æ—¶: {qwenvl_time:.2f}ç§’\")\n",
    "print(f\"é€Ÿåº¦æ¯”: {internvl_time/qwenvl_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 ä¸­æ–‡åœºæ™¯å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸­æ–‡é—®é¢˜\n",
    "chinese_question = \"è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹\"\n",
    "\n",
    "print(\"ğŸ‡¨ğŸ‡³ ä¸­æ–‡åœºæ™¯å¯¹æ¯”\\n\")\n",
    "print(f\"é—®é¢˜: {chinese_question}\\n\")\n",
    "\n",
    "# InternVL\n",
    "print(\"ğŸ“ InternVL:\")\n",
    "internvl_answer_cn, internvl_time_cn = internvl_inference(test_image, chinese_question)\n",
    "print(f\"å›ç­”: {internvl_answer_cn}\")\n",
    "print(f\"è€—æ—¶: {internvl_time_cn:.2f}ç§’\\n\")\n",
    "\n",
    "# Qwen-VL\n",
    "print(\"ğŸ“ Qwen-VL:\")\n",
    "qwenvl_answer_cn, qwenvl_time_cn = qwenvl_inference(test_image, chinese_question)\n",
    "print(f\"å›ç­”: {qwenvl_answer_cn}\")\n",
    "print(f\"è€—æ—¶: {qwenvl_time_cn:.2f}ç§’\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 OCRè¯†åˆ«å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCRé—®é¢˜\n",
    "ocr_question = \"Please extract all text from this image.\"\n",
    "\n",
    "print(\"ğŸ” OCRè¯†åˆ«å¯¹æ¯”\\n\")\n",
    "print(f\"é—®é¢˜: {ocr_question}\\n\")\n",
    "\n",
    "# InternVL\n",
    "print(\"ğŸ“ InternVL:\")\n",
    "internvl_ocr, internvl_time_ocr = internvl_inference(test_image, ocr_question)\n",
    "print(f\"è¯†åˆ«ç»“æœ: {internvl_ocr}\")\n",
    "print(f\"è€—æ—¶: {internvl_time_ocr:.2f}ç§’\\n\")\n",
    "\n",
    "# Qwen-VL\n",
    "print(\"ğŸ“ Qwen-VL:\")\n",
    "qwenvl_ocr, qwenvl_time_ocr = qwenvl_inference(test_image, ocr_question)\n",
    "print(f\"è¯†åˆ«ç»“æœ: {qwenvl_ocr}\")\n",
    "print(f\"è€—æ—¶: {qwenvl_time_ocr:.2f}ç§’\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ç¬¬äº”éƒ¨åˆ†ï¼šæ€§èƒ½ç»Ÿè®¡\n",
    "\n",
    "## 5.1 æ¨ç†æ—¶é—´å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ç»Ÿè®¡æ•°æ®\n",
    "tasks = ['Image Caption', 'Chinese', 'OCR']\n",
    "internvl_times = [internvl_time, internvl_time_cn, internvl_time_ocr]\n",
    "qwenvl_times = [qwenvl_time, qwenvl_time_cn, qwenvl_time_ocr]\n",
    "\n",
    "# ç»˜å›¾\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, internvl_times, width, label='InternVL', color='#4CAF50')\n",
    "bars2 = ax.bar(x + width/2, qwenvl_times, width, label='Qwen-VL', color='#2196F3')\n",
    "\n",
    "ax.set_xlabel('Tasks', fontsize=12)\n",
    "ax.set_ylabel('Inference Time (seconds)', fontsize=12)\n",
    "ax.set_title('Inference Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tasks)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}s',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ‰“å°ç»Ÿè®¡\n",
    "print(\"\\nğŸ“Š å¹³å‡æ¨ç†æ—¶é—´:\")\n",
    "print(f\"InternVL: {np.mean(internvl_times):.2f}ç§’\")\n",
    "print(f\"Qwen-VL: {np.mean(qwenvl_times):.2f}ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ æ€»ç»“\n",
    "\n",
    "## âœ… å®Œæˆçš„å†…å®¹\n",
    "\n",
    "1. âœ… åŠ è½½InternVLå’ŒQwen-VLæ¨¡å‹\n",
    "2. âœ… å¯¹æ¯”å›¾åƒæè¿°ç”Ÿæˆèƒ½åŠ›\n",
    "3. âœ… å¯¹æ¯”ä¸­æ–‡ç†è§£èƒ½åŠ›\n",
    "4. âœ… å¯¹æ¯”OCRè¯†åˆ«èƒ½åŠ›\n",
    "5. âœ… åˆ†ææ¨ç†æ€§èƒ½\n",
    "\n",
    "## ğŸ”‘ å…³é”®å‘ç°\n",
    "\n",
    "### InternVLä¼˜åŠ¿\n",
    "- âœ… è‹±æ–‡ä»»åŠ¡è¡¨ç°ä¼˜ç§€\n",
    "- âœ… é«˜åˆ†è¾¨ç‡å›¾åƒæ”¯æŒ\n",
    "- âœ… OCRå‡†ç¡®ç‡æ›´é«˜\n",
    "- âœ… æ¥è¿‘GPT-4Væ€§èƒ½\n",
    "\n",
    "### Qwen-VLä¼˜åŠ¿\n",
    "- âœ… ä¸­æ–‡åœºæ™¯ä¼˜åŒ–\n",
    "- âœ… æ¨ç†é€Ÿåº¦è¾ƒå¿«\n",
    "- âœ… æ¨¡å‹ä½“ç§¯æ›´å°\n",
    "- âœ… éƒ¨ç½²æ›´ä¾¿æ·\n",
    "\n",
    "## ğŸ¯ é€‰å‹å»ºè®®\n",
    "\n",
    "- **è‹±æ–‡ä¸ºä¸»**: é€‰æ‹©InternVL\n",
    "- **ä¸­æ–‡ä¸ºä¸»**: é€‰æ‹©Qwen-VL\n",
    "- **é«˜æ€§èƒ½è¦æ±‚**: é€‰æ‹©InternVL\n",
    "- **èµ„æºå—é™**: é€‰æ‹©Qwen-VL\n",
    "\n",
    "## ğŸ“š å‚è€ƒèµ„æº\n",
    "\n",
    "- [InternVLè¯¦è§£æ–‡æ¡£](../docs/01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/08-InternVLæ¨¡å‹è¯¦è§£.md)\n",
    "- [Qwen-VLè¯¦è§£æ–‡æ¡£](../docs/01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/07-Qwen-VLæ¨¡å‹è¯¦è§£.md)\n",
    "- [æ€§èƒ½å¯¹æ¯”æ–‡æ¡£](../docs/01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/09-InternVLä¸Qwen-VLå¯¹æ¯”.md)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆæœ¬æ•™ç¨‹ï¼**\n",
    "\n",
    "å¦‚æœ‰é—®é¢˜,æ¬¢è¿åœ¨GitHubä¸ŠæIssueã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}