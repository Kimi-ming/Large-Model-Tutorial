{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# BLIP-2è§†è§‰é—®ç­”ä¸å›¾åƒæè¿°æ•™ç¨‹\n\n> å®Œæ•´æ¼”ç¤ºBLIP-2æ¨¡å‹çš„å„ç§ä½¿ç”¨æ–¹å¼\n\n**å­¦ä¹ ç›®æ ‡**ï¼š\n- æŒæ¡BLIP-2çš„å›¾åƒæè¿°ç”Ÿæˆ\n- å­¦ä¼šä½¿ç”¨BLIP-2è¿›è¡Œè§†è§‰é—®ç­”\n- äº†è§£å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—\n- æ¢ç´¢BLIP-2çš„å®é™…åº”ç”¨\n\n**é¢„è®¡æ—¶é—´**: 40-50åˆ†é’Ÿ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–\ntry:\n    from transformers import Blip2Processor, Blip2ForConditionalGeneration\n    print(\"âœ… transformerså·²å®‰è£…\")\nexcept ImportError:\n    print(\"æ­£åœ¨å®‰è£…transformers...\")\n    !pip install transformers\n    print(\"âœ… å®‰è£…å®Œæˆ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¯¼å…¥å¿…è¦çš„åº“\nimport torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport urllib.request\nimport os\n\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprint(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\nprint(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. åŠ è½½BLIP-2æ¨¡å‹\n\nBLIP-2æœ‰å¤šç§é…ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨`opt-2.7b`ç‰ˆæœ¬è¿›è¡Œæ¼”ç¤ºã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨\nmodel_name = \"Salesforce/blip2-opt-2.7b\"\n\nprint(f\"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}\")\nprint(\"   (é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œå¤§çº¦5.5GBï¼Œè¯·è€å¿ƒç­‰å¾…...)\")\n\nprocessor = Blip2Processor.from_pretrained(model_name)\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16  # ä½¿ç”¨FP16èŠ‚çœæ˜¾å­˜\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nmodel.eval()\n\nprint(f\"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. å‡†å¤‡ç¤ºä¾‹å›¾åƒ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å‡†å¤‡ç¤ºä¾‹å›¾åƒ\nimage_path = \"sample_image.jpg\"\n\n# å°è¯•ä¸‹è½½ç¤ºä¾‹å›¾åƒ\ntry:\n    if not os.path.exists(image_path):\n        print(\"ğŸ“¥ ä¸‹è½½ç¤ºä¾‹å›¾åƒ...\")\n        image_url = \"https://images.unsplash.com/photo-1574158622682-e40e69881006?w=400\"\n        urllib.request.urlretrieve(image_url, image_path)\n        print(\"âœ… ä¸‹è½½æˆåŠŸ\")\nexcept Exception as e:\n    print(f\"âš ï¸ ä¸‹è½½å¤±è´¥: {e}\")\n    print(\"ç”Ÿæˆæµ‹è¯•å›¾åƒ...\")\n    # ç”Ÿæˆä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒï¼ˆä¸ä¾èµ–OpenCVï¼‰\n    from PIL import ImageDraw, ImageFont\n    \n    # åˆ›å»ºæ¸å˜èƒŒæ™¯å›¾åƒ\n    test_image = Image.new('RGB', (600, 400))\n    pixels = test_image.load()\n    \n    for i in range(400):\n        for j in range(600):\n            # åˆ›å»ºæ¸å˜æ•ˆæœ\n            r = int(100 + 155 * (j / 600))\n            g = int(150 + 105 * (i / 400))\n            b = int(200 - 100 * ((i + j) / 1000))\n            pixels[j, i] = (r, g, b)\n    \n    # ç»˜åˆ¶å½¢çŠ¶å’Œæ–‡æœ¬\n    draw = ImageDraw.Draw(test_image)\n    \n    # ç»˜åˆ¶çŸ©å½¢\n    draw.rectangle([100, 100, 500, 300], outline='blue', width=3)\n    \n    # ç»˜åˆ¶åœ†å½¢\n    draw.ellipse([250, 150, 350, 250], outline='red', width=3)\n    \n    # æ·»åŠ æ–‡æœ¬\n    try:\n        # å°è¯•ä½¿ç”¨é»˜è®¤å­—ä½“\n        draw.text((250, 320), \"Test Image\", fill='white')\n    except:\n        # å¦‚æœæ²¡æœ‰å­—ä½“ä¹Ÿæ²¡å…³ç³»\n        pass\n    \n    test_image.save(image_path)\n    print(\"âœ… ç”Ÿæˆæµ‹è¯•å›¾åƒï¼ˆæ— éœ€OpenCVï¼‰\")\n\n# åŠ è½½å¹¶æ˜¾ç¤ºå›¾åƒ\nimage = Image.open(image_path).convert(\"RGB\")\n\nplt.figure(figsize=(8, 6))\nplt.imshow(image)\nplt.title(\"ç¤ºä¾‹å›¾åƒ\")\nplt.axis('off')\nplt.show()\n\nprint(f\"å›¾åƒå°ºå¯¸: {image.size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. å›¾åƒæè¿°ç”Ÿæˆ (Image Captioning)\n\nBLIP-2å¯ä»¥è‡ªåŠ¨ç”Ÿæˆå›¾åƒçš„æè¿°ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_caption(image, prompt=None, max_new_tokens=50):\n    \"\"\"ç”Ÿæˆå›¾åƒæè¿°\"\"\"\n    if prompt:\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n    else:\n        inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n    \n    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    \n    return caption\n\n# æ–¹å¼1ï¼šæ— æç¤ºï¼ˆè‡ªåŠ¨æè¿°ï¼‰\ncaption = generate_caption(image)\nprint(f\"ğŸ“ è‡ªåŠ¨ç”Ÿæˆçš„æè¿°:\")\nprint(f\"   {caption}\")\n\n# æ–¹å¼2ï¼šå¸¦æç¤º\ncaption_detailed = generate_caption(image, prompt=\"A detailed description:\")\nprint(f\"\\nğŸ“ è¯¦ç»†æè¿°:\")\nprint(f\"   {caption_detailed}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. è§†è§‰é—®ç­” (Visual Question Answering)\n\nBLIP-2å¯ä»¥å›ç­”å…³äºå›¾åƒçš„é—®é¢˜ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visual_qa(image, question):\n    \"\"\"è§†è§‰é—®ç­”\"\"\"\n    prompt = f\"Question: {question} Answer:\"\n    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n    \n    generated_ids = model.generate(**inputs, max_new_tokens=20)\n    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    \n    # æ¸…ç†ç­”æ¡ˆ\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):].strip()\n    \n    return answer\n\n# æµ‹è¯•å¤šä¸ªé—®é¢˜\nquestions = [\n    \"What is the main subject of this image?\",\n    \"What color is prominent in the image?\",\n    \"Is this taken indoors or outdoors?\",\n    \"What is the mood of the image?\"\n]\n\nprint(\"â“ è§†è§‰é—®ç­”ç¤ºä¾‹:\\n\")\nfor q in questions:\n    answer = visual_qa(image, q)\n    print(f\"Q: {q}\")\n    print(f\"A: {answer}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. å¤šè½®å¯¹è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ç®¡ç†ï¼‰\n\næ¼”ç¤ºçœŸæ­£çš„å¤šè½®å¯¹è¯ï¼šåç»­é—®é¢˜å¯ä»¥æŒ‡ä»£ä¹‹å‰çš„å›ç­”ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def multi_turn_conversation(image, questions):\n    \"\"\"å¤šè½®å¯¹è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ç´¯ç§¯ï¼‰\"\"\"\n    conversation = []\n    context = \"\"  # ç´¯ç§¯ä¸Šä¸‹æ–‡\n    \n    for question in questions:\n        # æ„å»ºåŒ…å«å†å²çš„æç¤º\n        if context:\n            # åŒ…å«ä¹‹å‰çš„å¯¹è¯å†å²\n            prompt = f\"{context}\\nQuestion: {question} Answer:\"\n        else:\n            prompt = f\"Question: {question} Answer:\"\n        \n        # ä½¿ç”¨å¸¦ä¸Šä¸‹æ–‡çš„æç¤ºç”Ÿæˆç­”æ¡ˆ\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n        generated_ids = model.generate(**inputs, max_new_tokens=30)\n        full_response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        \n        # æå–ç­”æ¡ˆéƒ¨åˆ†\n        if \"Answer:\" in full_response:\n            answer = full_response.split(\"Answer:\")[-1].strip()\n        else:\n            answer = full_response\n        \n        # è®°å½•å¯¹è¯\n        conversation.append((question, answer))\n        \n        # æ›´æ–°ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™æœ€è¿‘3è½®å¯¹è¯ï¼‰\n        context_entry = f\"Q: {question}\\nA: {answer}\"\n        if context:\n            context_parts = context.split(\"\\n\\n\")\n            context_parts.append(context_entry)\n            # åªä¿ç•™æœ€è¿‘3è½®\n            context = \"\\n\\n\".join(context_parts[-3:])\n        else:\n            context = context_entry\n    \n    return conversation\n\n# ç¤ºä¾‹å¯¹è¯ï¼ˆé—®é¢˜æœ‰è¿è´¯æ€§ï¼‰\nconversation_questions = [\n    \"What is the main subject in this image?\",\n    \"What color is it?\",  # æŒ‡ä»£å‰ä¸€ä¸ªé—®é¢˜çš„ä¸»ä½“\n    \"Based on what we discussed, what might this scene represent?\"\n]\n\nprint(\"ğŸ’¬ å¤šè½®å¯¹è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰:\\n\")\nconversation = multi_turn_conversation(image, conversation_questions)\n\nfor i, (q, a) in enumerate(conversation, 1):\n    print(f\"å›åˆ {i}:\")\n    print(f\"  äººç±»: {q}\")\n    print(f\"  BLIP-2: {a}\")\n    print()\n\nprint(\"\\nğŸ’¡ æ³¨æ„ï¼šå¤šè½®å¯¹è¯é€šè¿‡åœ¨æç¤ºä¸­ç´¯ç§¯å†å²Q&Aæ¥å®ç°ä¸Šä¸‹æ–‡\")\nprint(\"   è¿™æ ·åç»­é—®é¢˜å¯ä»¥æŒ‡ä»£ä¹‹å‰è®¨è®ºçš„å†…å®¹\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. æ‰¹é‡å¤„ç†\n\næ¼”ç¤ºå¦‚ä½•æ‰¹é‡å¤„ç†å¤šå¼ å›¾åƒã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åˆ›å»ºå¤šå¼ æµ‹è¯•å›¾åƒï¼ˆä¸ä¾èµ–OpenCVï¼‰\nfrom PIL import ImageDraw\n\ntest_images = []\nfor i in range(3):\n    # ç”Ÿæˆä¸åŒé¢œè‰²çš„æµ‹è¯•å›¾åƒ\n    base_color = (\n        100 + i * 50,  # R\n        150 + i * 30,  # G\n        200 - i * 40   # B\n    )\n    \n    img = Image.new('RGB', (400, 300), base_color)\n    draw = ImageDraw.Draw(img)\n    \n    # ç»˜åˆ¶ä¸åŒçš„å½¢çŠ¶\n    if i == 0:\n        draw.rectangle([50, 50, 350, 250], outline='white', width=5)\n    elif i == 1:\n        draw.ellipse([50, 50, 350, 250], outline='white', width=5)\n    else:\n        draw.polygon([(200, 50), (350, 150), (275, 250), (125, 250), (50, 150)], outline='white', width=5)\n    \n    # æ·»åŠ æ ‡è®°æ–‡æœ¬\n    draw.text((150, 270), f\"Test Image {i+1}\", fill='white')\n    \n    test_images.append(img)\n\n# æ˜¾ç¤ºæµ‹è¯•å›¾åƒ\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, img in enumerate(test_images):\n    axes[i].imshow(img)\n    axes[i].set_title(f\"Test Image {i+1}\")\n    axes[i].axis('off')\nplt.tight_layout()\nplt.show()\n\n# æ‰¹é‡ç”Ÿæˆæè¿°\nprint(\"\\nğŸ“ æ‰¹é‡ç”Ÿæˆæè¿°:\\n\")\nfor i, img in enumerate(test_images, 1):\n    caption = generate_caption(img)\n    print(f\"Image {i}: {caption}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. ç”Ÿæˆå‚æ•°è°ƒä¼˜\n\næ¢ç´¢ä¸åŒçš„ç”Ÿæˆå‚æ•°å¦‚ä½•å½±å“è¾“å‡ºã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_with_params(image, **kwargs):\n    \"\"\"ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ç”Ÿæˆ\"\"\"\n    inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = model.generate(**inputs, **kwargs)\n    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    return caption\n\nprint(\"ğŸ›ï¸ ä¸åŒç”Ÿæˆå‚æ•°çš„æ•ˆæœ:\\n\")\n\n# 1. é»˜è®¤å‚æ•°\nprint(\"1. é»˜è®¤å‚æ•°:\")\ncaption1 = generate_with_params(image, max_new_tokens=50)\nprint(f\"   {caption1}\\n\")\n\n# 2. æŸæœç´¢\nprint(\"2. æŸæœç´¢ (num_beams=5):\")\ncaption2 = generate_with_params(image, max_new_tokens=50, num_beams=5)\nprint(f\"   {caption2}\\n\")\n\n# 3. é‡‡æ ·\nprint(\"3. éšæœºé‡‡æ · (do_sample=True, temperature=0.7):\")\ncaption3 = generate_with_params(image, max_new_tokens=50, do_sample=True, temperature=0.7)\nprint(f\"   {caption3}\\n\")\n\n# 4. Top-pé‡‡æ ·\nprint(\"4. Top-pé‡‡æ · (top_p=0.9):\")\ncaption4 = generate_with_params(image, max_new_tokens=50, do_sample=True, top_p=0.9)\nprint(f\"   {caption4}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. å®é™…åº”ç”¨ç¤ºä¾‹\n\n### 8.1 è¾…åŠ©è§†éšœäººå£«"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def describe_scene_for_accessibility(image):\n    \"\"\"ä¸ºè§†éšœäººå£«æè¿°åœºæ™¯\"\"\"\n    # ç”Ÿæˆè¯¦ç»†æè¿°\n    description = generate_caption(image, prompt=\"Describe this image in detail for a blind person:\")\n    \n    # å›ç­”å…³é”®é—®é¢˜\n    safety_check = visual_qa(image, \"Are there any safety hazards visible?\")\n    location = visual_qa(image, \"What kind of place is this?\")\n    \n    return {\n        'description': description,\n        'safety': safety_check,\n        'location': location\n    }\n\nprint(\"â™¿ æ— éšœç¢æè¿°:\\n\")\nresult = describe_scene_for_accessibility(image)\n\nprint(f\"åœºæ™¯æè¿°: {result['description']}\")\nprint(f\"åœ°ç‚¹ç±»å‹: {result['location']}\")\nprint(f\"å®‰å…¨æ£€æŸ¥: {result['safety']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 8.2 ç¤¾äº¤åª’ä½“è‡ªåŠ¨æ ‡é¢˜"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_social_media_caption(image):\n    \"\"\"ç”Ÿæˆç¤¾äº¤åª’ä½“æ ‡é¢˜\"\"\"\n    # ç”Ÿæˆåˆ›æ„æè¿°\n    caption = generate_caption(image, prompt=\"A creative and engaging social media caption:\")\n    \n    # ç”Ÿæˆæ ‡ç­¾å»ºè®®\n    tags_prompt = \"Question: What are 3 relevant hashtags for this image? Answer:\"\n    inputs = processor(images=image, text=tags_prompt, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = model.generate(**inputs, max_new_tokens=30)\n    tags = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    \n    return caption, tags\n\nprint(\"ğŸ“± ç¤¾äº¤åª’ä½“æ ‡é¢˜ç”Ÿæˆ:\\n\")\ncaption, tags = generate_social_media_caption(image)\n\nprint(f\"æ ‡é¢˜: {caption}\")\nprint(f\"å»ºè®®æ ‡ç­¾: {tags}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## æ€»ç»“\n\næœ¬æ•™ç¨‹æ¼”ç¤ºäº†BLIP-2çš„æ ¸å¿ƒåŠŸèƒ½ï¼š\n\n1. **å›¾åƒæè¿°ç”Ÿæˆ** - è‡ªåŠ¨ç”Ÿæˆå‡†ç¡®çš„å›¾åƒæè¿°\n2. **è§†è§‰é—®ç­”** - å›ç­”å…³äºå›¾åƒçš„å„ç§é—®é¢˜\n3. **å¤šè½®å¯¹è¯** - æ”¯æŒè¿ç»­çš„äº¤äº’å¼é—®ç­”\n4. **æ‰¹é‡å¤„ç†** - é«˜æ•ˆå¤„ç†å¤šå¼ å›¾åƒ\n5. **å‚æ•°è°ƒä¼˜** - é€šè¿‡è°ƒæ•´ç”Ÿæˆå‚æ•°ä¼˜åŒ–è¾“å‡º\n6. **å®é™…åº”ç”¨** - æ— éšœç¢è¾…åŠ©ã€ç¤¾äº¤åª’ä½“ç­‰åœºæ™¯\n\n### ç»ƒä¹ ä»»åŠ¡\n\n1. ä½¿ç”¨è‡ªå·±çš„å›¾åƒæµ‹è¯•BLIP-2\n2. å°è¯•ä¸åŒçš„æç¤ºæ¨¡æ¿\n3. æ¯”è¾ƒä¸åŒç”Ÿæˆå‚æ•°çš„æ•ˆæœ\n4. æ¢ç´¢BLIP-2çš„å…¶ä»–åº”ç”¨åœºæ™¯\n\n### å‚è€ƒèµ„æº\n\n- [BLIP-2è®ºæ–‡](https://arxiv.org/abs/2301.12597)\n- [BLIP-2 GitHub](https://github.com/salesforce/LAVIS)\n- [BLIP-2è¯¦è§£æ–‡æ¡£](../docs/01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/06-BLIP2æ¨¡å‹è¯¦è§£.md)\n\nğŸ‰ æ­å–œå®Œæˆæœ¬æ•™ç¨‹ï¼"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}