"""
ç”ŸæˆBLIP-2 Notebookæ•™ç¨‹
"""
import json

# åˆ›å»ºnotebookç»“æ„
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# BLIP-2è§†è§‰é—®ç­”ä¸å›¾åƒæè¿°æ•™ç¨‹\n\n> å®Œæ•´æ¼”ç¤ºBLIP-2æ¨¡å‹çš„å„ç§ä½¿ç”¨æ–¹å¼\n\n**å­¦ä¹ ç›®æ ‡**ï¼š\n- æŒæ¡BLIP-2çš„å›¾åƒæè¿°ç”Ÿæˆ\n- å­¦ä¼šä½¿ç”¨BLIP-2è¿›è¡Œè§†è§‰é—®ç­”\n- äº†è§£å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—\n- æ¢ç´¢BLIP-2çš„å®é™…åº”ç”¨\n\n**é¢„è®¡æ—¶é—´**: 40-50åˆ†é’Ÿ"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–\ntry:\n    from transformers import Blip2Processor, Blip2ForConditionalGeneration\n    print(\"âœ… transformerså·²å®‰è£…\")\nexcept ImportError:\n    print(\"æ­£åœ¨å®‰è£…transformers...\")\n    !pip install transformers\n    print(\"âœ… å®‰è£…å®Œæˆ\")"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "# å¯¼å…¥å¿…è¦çš„åº“\nimport torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport urllib.request\nimport os\n\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprint(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\nprint(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1. åŠ è½½BLIP-2æ¨¡å‹\n\nBLIP-2æœ‰å¤šç§é…ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨`opt-2.7b`ç‰ˆæœ¬è¿›è¡Œæ¼”ç¤ºã€‚"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨\nmodel_name = \"Salesforce/blip2-opt-2.7b\"\n\nprint(f\"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}\")\nprint(\"   (é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œå¤§çº¦5.5GBï¼Œè¯·è€å¿ƒç­‰å¾…...)\")\n\nprocessor = Blip2Processor.from_pretrained(model_name)\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16  # ä½¿ç”¨FP16èŠ‚çœæ˜¾å­˜\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nmodel.eval()\n\nprint(f\"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. å‡†å¤‡ç¤ºä¾‹å›¾åƒ"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "# å‡†å¤‡ç¤ºä¾‹å›¾åƒ\nimage_path = \"sample_image.jpg\"\n\n# å°è¯•ä¸‹è½½ç¤ºä¾‹å›¾åƒ\ntry:\n    if not os.path.exists(image_path):\n        print(\"ğŸ“¥ ä¸‹è½½ç¤ºä¾‹å›¾åƒ...\")\n        image_url = \"https://images.unsplash.com/photo-1574158622682-e40e69881006?w=400\"\n        urllib.request.urlretrieve(image_url, image_path)\n        print(\"âœ… ä¸‹è½½æˆåŠŸ\")\nexcept Exception as e:\n    print(f\"âš ï¸ ä¸‹è½½å¤±è´¥: {e}\")\n    print(\"ç”Ÿæˆæµ‹è¯•å›¾åƒ...\")\n    # ç”Ÿæˆä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ\n    test_image = np.random.randint(128, 255, (400, 600, 3), dtype=np.uint8)\n    # ç»˜åˆ¶ä¸€äº›å½¢çŠ¶\n    import cv2\n    cv2.putText(test_image, \"Cat\", (250, 200), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 3)\n    Image.fromarray(test_image).save(image_path)\n    print(\"âœ… ç”Ÿæˆæµ‹è¯•å›¾åƒ\")\n\n# åŠ è½½å¹¶æ˜¾ç¤ºå›¾åƒ\nimage = Image.open(image_path).convert(\"RGB\")\n\nplt.figure(figsize=(8, 6))\nplt.imshow(image)\nplt.title(\"ç¤ºä¾‹å›¾åƒ\")\nplt.axis('off')\nplt.show()\n\nprint(f\"å›¾åƒå°ºå¯¸: {image.size}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 3. å›¾åƒæè¿°ç”Ÿæˆ (Image Captioning)\n\nBLIP-2å¯ä»¥è‡ªåŠ¨ç”Ÿæˆå›¾åƒçš„æè¿°ã€‚"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "def generate_caption(image, prompt=None, max_new_tokens=50):\n    \"\"\"ç”Ÿæˆå›¾åƒæè¿°\"\"\"\n    if prompt:\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n    else:\n        inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n    \n    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    \n    return caption\n\n# æ–¹å¼1ï¼šæ— æç¤ºï¼ˆè‡ªåŠ¨æè¿°ï¼‰\ncaption = generate_caption(image)\nprint(f\"ğŸ“ è‡ªåŠ¨ç”Ÿæˆçš„æè¿°:\")\nprint(f\"   {caption}\")\n\n# æ–¹å¼2ï¼šå¸¦æç¤º\ncaption_detailed = generate_caption(image, prompt=\"A detailed description:\")\nprint(f\"\\nğŸ“ è¯¦ç»†æè¿°:\")\nprint(f\"   {caption_detailed}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 4. è§†è§‰é—®ç­” (Visual Question Answering)\n\nBLIP-2å¯ä»¥å›ç­”å…³äºå›¾åƒçš„é—®é¢˜ã€‚"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "def visual_qa(image, question):\n    \"\"\"è§†è§‰é—®ç­”\"\"\"\n    prompt = f\"Question: {question} Answer:\"\n    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n    \n    generated_ids = model.generate(**inputs, max_new_tokens=20)\n    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    \n    # æ¸…ç†ç­”æ¡ˆ\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):].strip()\n    \n    return answer\n\n# æµ‹è¯•å¤šä¸ªé—®é¢˜\nquestions = [\n    \"What is the main subject of this image?\",\n    \"What color is prominent in the image?\",\n    \"Is this taken indoors or outdoors?\",\n    \"What is the mood of the image?\"\n]\n\nprint(\"â“ è§†è§‰é—®ç­”ç¤ºä¾‹:\\n\")\nfor q in questions:\n    answer = visual_qa(image, q)\n    print(f\"Q: {q}\")\n    print(f\"A: {answer}\")\n    print()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 5. å¤šè½®å¯¹è¯\n\næ¨¡æ‹Ÿä¸å›¾åƒç›¸å…³çš„å¤šè½®é—®ç­”ã€‚"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "def multi_turn_conversation(image, questions):\n    \"\"\"å¤šè½®å¯¹è¯\"\"\"\n    conversation = []\n    \n    for question in questions:\n        answer = visual_qa(image, question)\n        conversation.append((question, answer))\n    \n    return conversation\n\n# ç¤ºä¾‹å¯¹è¯\nconversation_questions = [\n    \"What do you see in this image?\",\n    \"Can you describe it in more detail?\",\n    \"What time of day might this be?\"\n]\n\nprint(\"ğŸ’¬ å¤šè½®å¯¹è¯:\\n\")\nconversation = multi_turn_conversation(image, conversation_questions)\n\nfor i, (q, a) in enumerate(conversation, 1):\n    print(f\"å›åˆ {i}:\")\n    print(f\"  äººç±»: {q}\")\n    print(f\"  BLIP-2: {a}\")\n    print()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 6. æ‰¹é‡å¤„ç†\n\næ¼”ç¤ºå¦‚ä½•æ‰¹é‡å¤„ç†å¤šå¼ å›¾åƒã€‚"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "# åˆ›å»ºå¤šå¼ æµ‹è¯•å›¾åƒ\ntest_images = []\nfor i in range(3):\n    # ç”Ÿæˆä¸åŒçš„æµ‹è¯•å›¾åƒ\n    img_array = np.random.randint(100, 200, (300, 400, 3), dtype=np.uint8)\n    # æ·»åŠ ä¸åŒçš„æ ‡è®°\n    import cv2\n    cv2.putText(img_array, f\"Image {i+1}\", (150, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    test_images.append(Image.fromarray(img_array))\n\n# æ˜¾ç¤ºæµ‹è¯•å›¾åƒ\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, img in enumerate(test_images):\n    axes[i].imshow(img)\n    axes[i].set_title(f\"Image {i+1}\")\n    axes[i].axis('off')\nplt.tight_layout()\nplt.show()\n\n# æ‰¹é‡ç”Ÿæˆæè¿°\nprint(\"\\nğŸ“ æ‰¹é‡ç”Ÿæˆæè¿°:\\n\")\nfor i, img in enumerate(test_images, 1):\n    caption = generate_caption(img)\n    print(f\"Image {i}: {caption}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 7. ç”Ÿæˆå‚æ•°è°ƒä¼˜\n\næ¢ç´¢ä¸åŒçš„ç”Ÿæˆå‚æ•°å¦‚ä½•å½±å“è¾“å‡ºã€‚"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "def generate_with_params(image, **kwargs):\n    \"\"\"ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ç”Ÿæˆ\"\"\"\n    inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = model.generate(**inputs, **kwargs)\n    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    return caption\n\nprint(\"ğŸ›ï¸ ä¸åŒç”Ÿæˆå‚æ•°çš„æ•ˆæœ:\\n\")\n\n# 1. é»˜è®¤å‚æ•°\nprint(\"1. é»˜è®¤å‚æ•°:\")\ncaption1 = generate_with_params(image, max_new_tokens=50)\nprint(f\"   {caption1}\\n\")\n\n# 2. æŸæœç´¢\nprint(\"2. æŸæœç´¢ (num_beams=5):\")\ncaption2 = generate_with_params(image, max_new_tokens=50, num_beams=5)\nprint(f\"   {caption2}\\n\")\n\n# 3. é‡‡æ ·\nprint(\"3. éšæœºé‡‡æ · (do_sample=True, temperature=0.7):\")\ncaption3 = generate_with_params(image, max_new_tokens=50, do_sample=True, temperature=0.7)\nprint(f\"   {caption3}\\n\")\n\n# 4. Top-pé‡‡æ ·\nprint(\"4. Top-pé‡‡æ · (top_p=0.9):\")\ncaption4 = generate_with_params(image, max_new_tokens=50, do_sample=True, top_p=0.9)\nprint(f\"   {caption4}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 8. å®é™…åº”ç”¨ç¤ºä¾‹\n\n### 8.1 è¾…åŠ©è§†éšœäººå£«"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "def describe_scene_for_accessibility(image):\n    \"\"\"ä¸ºè§†éšœäººå£«æè¿°åœºæ™¯\"\"\"\n    # ç”Ÿæˆè¯¦ç»†æè¿°\n    description = generate_caption(image, prompt=\"Describe this image in detail for a blind person:\")\n    \n    # å›ç­”å…³é”®é—®é¢˜\n    safety_check = visual_qa(image, \"Are there any safety hazards visible?\")\n    location = visual_qa(image, \"What kind of place is this?\")\n    \n    return {\n        'description': description,\n        'safety': safety_check,\n        'location': location\n    }\n\nprint(\"â™¿ æ— éšœç¢æè¿°:\\n\")\nresult = describe_scene_for_accessibility(image)\n\nprint(f\"åœºæ™¯æè¿°: {result['description']}\")\nprint(f\"åœ°ç‚¹ç±»å‹: {result['location']}\")\nprint(f\"å®‰å…¨æ£€æŸ¥: {result['safety']}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 8.2 ç¤¾äº¤åª’ä½“è‡ªåŠ¨æ ‡é¢˜"
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": "def generate_social_media_caption(image):\n    \"\"\"ç”Ÿæˆç¤¾äº¤åª’ä½“æ ‡é¢˜\"\"\"\n    # ç”Ÿæˆåˆ›æ„æè¿°\n    caption = generate_caption(image, prompt=\"A creative and engaging social media caption:\")\n    \n    # ç”Ÿæˆæ ‡ç­¾å»ºè®®\n    tags_prompt = \"Question: What are 3 relevant hashtags for this image? Answer:\"\n    inputs = processor(images=image, text=tags_prompt, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = model.generate(**inputs, max_new_tokens=30)\n    tags = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    \n    return caption, tags\n\nprint(\"ğŸ“± ç¤¾äº¤åª’ä½“æ ‡é¢˜ç”Ÿæˆ:\\n\")\ncaption, tags = generate_social_media_caption(image)\n\nprint(f\"æ ‡é¢˜: {caption}\")\nprint(f\"å»ºè®®æ ‡ç­¾: {tags}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## æ€»ç»“\n\næœ¬æ•™ç¨‹æ¼”ç¤ºäº†BLIP-2çš„æ ¸å¿ƒåŠŸèƒ½ï¼š\n\n1. **å›¾åƒæè¿°ç”Ÿæˆ** - è‡ªåŠ¨ç”Ÿæˆå‡†ç¡®çš„å›¾åƒæè¿°\n2. **è§†è§‰é—®ç­”** - å›ç­”å…³äºå›¾åƒçš„å„ç§é—®é¢˜\n3. **å¤šè½®å¯¹è¯** - æ”¯æŒè¿ç»­çš„äº¤äº’å¼é—®ç­”\n4. **æ‰¹é‡å¤„ç†** - é«˜æ•ˆå¤„ç†å¤šå¼ å›¾åƒ\n5. **å‚æ•°è°ƒä¼˜** - é€šè¿‡è°ƒæ•´ç”Ÿæˆå‚æ•°ä¼˜åŒ–è¾“å‡º\n6. **å®é™…åº”ç”¨** - æ— éšœç¢è¾…åŠ©ã€ç¤¾äº¤åª’ä½“ç­‰åœºæ™¯\n\n### ç»ƒä¹ ä»»åŠ¡\n\n1. ä½¿ç”¨è‡ªå·±çš„å›¾åƒæµ‹è¯•BLIP-2\n2. å°è¯•ä¸åŒçš„æç¤ºæ¨¡æ¿\n3. æ¯”è¾ƒä¸åŒç”Ÿæˆå‚æ•°çš„æ•ˆæœ\n4. æ¢ç´¢BLIP-2çš„å…¶ä»–åº”ç”¨åœºæ™¯\n\n### å‚è€ƒèµ„æº\n\n- [BLIP-2è®ºæ–‡](https://arxiv.org/abs/2301.12597)\n- [BLIP-2 GitHub](https://github.com/salesforce/LAVIS)\n- [BLIP-2è¯¦è§£æ–‡æ¡£](../docs/01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/06-BLIP2æ¨¡å‹è¯¦è§£.md)\n\nğŸ‰ æ­å–œå®Œæˆæœ¬æ•™ç¨‹ï¼"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# ä¿å­˜notebook
output_path = "notebooks/04_blip2_vqa_tutorial.ipynb"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print(f"âœ… BLIP-2 Notebookå·²åˆ›å»º: {output_path}")
print(f"   åŒ…å« {len(notebook['cells'])} ä¸ªcells")
print(f"   æ¶µç›–å›¾åƒæè¿°ã€VQAã€å¤šè½®å¯¹è¯ç­‰åŠŸèƒ½")

