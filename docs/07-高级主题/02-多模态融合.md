# å¤šæ¨¡æ€èåˆæŠ€æœ¯

æœ¬æ–‡æ¡£æ·±å…¥æ¢è®¨è§†è§‰-è¯­è¨€å¤šæ¨¡æ€èåˆæŠ€æœ¯ï¼Œä»‹ç»å¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆå›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€åº”ç”¨ã€‚

---

## ğŸ“‹ ç›®å½•

1. [å¤šæ¨¡æ€èåˆæ¦‚è¿°](#å¤šæ¨¡æ€èåˆæ¦‚è¿°)
2. [æ¨¡æ€å¯¹é½æŠ€æœ¯](#æ¨¡æ€å¯¹é½æŠ€æœ¯)
3. [è·¨æ¨¡æ€æ£€ç´¢](#è·¨æ¨¡æ€æ£€ç´¢)
4. [å¤šæ¨¡æ€èåˆæ¶æ„](#å¤šæ¨¡æ€èåˆæ¶æ„)
5. [å®é™…åº”ç”¨åœºæ™¯](#å®é™…åº”ç”¨åœºæ™¯)
6. [é«˜çº§æŠ€å·§](#é«˜çº§æŠ€å·§)

---

## å¤šæ¨¡æ€èåˆæ¦‚è¿°

### ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€èåˆ

å¤šæ¨¡æ€èåˆæ˜¯æŒ‡å°†æ¥è‡ªä¸åŒæ¨¡æ€ï¼ˆè§†è§‰ã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰ï¼‰çš„ä¿¡æ¯æ•´åˆåœ¨ä¸€èµ·ï¼Œä»¥å®ç°æ›´å…¨é¢çš„ç†è§£å’Œæ›´å¼ºçš„èƒ½åŠ›ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€èåˆ

**å•æ¨¡æ€çš„å±€é™æ€§**:
- å›¾åƒ: ç¼ºä¹è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
- æ–‡æœ¬: ç¼ºä¹è§†è§‰ç»†èŠ‚å’Œç©ºé—´ä¿¡æ¯
- éŸ³é¢‘: ç¼ºä¹è§†è§‰å’Œæ–‡å­—æè¿°

**å¤šæ¨¡æ€çš„ä¼˜åŠ¿**:
- âœ… ä¿¡æ¯äº’è¡¥
- âœ… æ›´é²æ£’çš„è¡¨ç¤º
- âœ… æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
- âœ… æ”¯æŒå¤æ‚ä»»åŠ¡

### å¤šæ¨¡æ€èåˆçš„æŒ‘æˆ˜

| æŒ‘æˆ˜ | æè¿° | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| **å¼‚æ„æ€§** | ä¸åŒæ¨¡æ€çš„æ•°æ®ç»“æ„å·®å¼‚å¤§ | ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ |
| **å¯¹é½** | ä¸åŒæ¨¡æ€çš„ä¿¡æ¯å¯¹åº”å…³ç³» | å¯¹é½å­¦ä¹  |
| **èåˆç­–ç•¥** | å¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆä¿¡æ¯ | æ³¨æ„åŠ›æœºåˆ¶ |
| **æ•°æ®ä¸å¹³è¡¡** | æŸäº›æ¨¡æ€çš„æ•°æ®æ›´ä¸°å¯Œ | å¹³è¡¡é‡‡æ · |

---

## æ¨¡æ€å¯¹é½æŠ€æœ¯

### å¯¹æ¯”å­¦ä¹ å¯¹é½ï¼ˆCLIPæ–¹æ³•ï¼‰

CLIPä½¿ç”¨å¯¹æ¯”å­¦ä¹ å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ç‰¹å¾ç©ºé—´ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import clip
from PIL import Image

class CLIPAlignment:
    """CLIPé£æ ¼çš„æ¨¡æ€å¯¹é½"""
    
    def __init__(self, model_name="ViT-B/32", device="cuda"):
        self.device = device
        self.model, self.preprocess = clip.load(model_name, device=device)
        self.model.eval()
    
    def encode_image(self, image_path):
        """ç¼–ç å›¾åƒ"""
        image = Image.open(image_path)
        image = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            image_features = self.model.encode_image(image)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        return image_features
    
    def encode_text(self, text_list):
        """ç¼–ç æ–‡æœ¬"""
        text = clip.tokenize(text_list).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        return text_features
    
    def compute_similarity(self, image_features, text_features):
        """è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦"""
        # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå·²å½’ä¸€åŒ–ï¼Œç›´æ¥ç‚¹ç§¯ï¼‰
        similarity = (image_features @ text_features.T) * 100
        return similarity.softmax(dim=-1)
    
    def align_image_text(self, image_path, text_list):
        """
        å¯¹é½å›¾åƒå’Œæ–‡æœ¬
        
        Returns:
            similarity: ç›¸ä¼¼åº¦åˆ†æ•°
            best_match: æœ€åŒ¹é…çš„æ–‡æœ¬
        """
        image_features = self.encode_image(image_path)
        text_features = self.encode_text(text_list)
        
        similarity = self.compute_similarity(image_features, text_features)
        
        best_idx = similarity.argmax().item()
        best_match = text_list[best_idx]
        best_score = similarity[0, best_idx].item()
        
        return {
            'similarity': similarity.cpu().numpy(),
            'best_match': best_match,
            'best_score': best_score,
            'all_scores': {text: similarity[0, i].item() 
                          for i, text in enumerate(text_list)}
        }

# ä½¿ç”¨ç¤ºä¾‹
aligner = CLIPAlignment()

# å¯¹é½å›¾åƒå’Œæ–‡æœ¬
result = aligner.align_image_text(
    "dog.jpg",
    ["a photo of a dog", "a photo of a cat", "a photo of a bird"]
)

print(f"æœ€ä½³åŒ¹é…: {result['best_match']}")
print(f"åŒ¹é…åˆ†æ•°: {result['best_score']:.4f}")
print(f"æ‰€æœ‰åˆ†æ•°: {result['all_scores']}")
```

### æ³¨æ„åŠ›æœºåˆ¶å¯¹é½

ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å»ºç«‹å¯¹åº”å…³ç³»ï¼š

```python
import torch
import torch.nn as nn

class CrossModalAttention(nn.Module):
    """è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, dim, num_heads=8, dropout=0.1):
        super().__init__()
        self.num_heads = num_heads
        self.dim = dim
        self.head_dim = dim // num_heads
        
        # æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±
        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        
        self.out_proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query_modality, key_value_modality, mask=None):
        """
        è·¨æ¨¡æ€æ³¨æ„åŠ›
        
        Args:
            query_modality: æŸ¥è¯¢æ¨¡æ€ [B, L1, D]
            key_value_modality: é”®å€¼æ¨¡æ€ [B, L2, D]
            mask: æ³¨æ„åŠ›æ©ç 
        
        Returns:
            output: èåˆåçš„ç‰¹å¾ [B, L1, D]
            attention_weights: æ³¨æ„åŠ›æƒé‡ [B, H, L1, L2]
        """
        B, L1, D = query_modality.shape
        L2 = key_value_modality.shape[1]
        
        # æŠ•å½±
        Q = self.q_proj(query_modality)  # [B, L1, D]
        K = self.k_proj(key_value_modality)  # [B, L2, D]
        V = self.v_proj(key_value_modality)  # [B, L2, D]
        
        # é‡å¡‘ä¸ºå¤šå¤´
        Q = Q.view(B, L1, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(B, L2, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(B, L2, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # æ³¨æ„åŠ›æƒé‡
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)  # [B, H, L1, head_dim]
        
        # åˆå¹¶å¤šå¤´
        output = output.transpose(1, 2).contiguous().view(B, L1, D)
        output = self.out_proj(output)
        
        return output, attention_weights

# ä½¿ç”¨ç¤ºä¾‹
class ImageTextFusion(nn.Module):
    """å›¾æ–‡èåˆæ¨¡å—"""
    
    def __init__(self, dim=512, num_heads=8):
        super().__init__()
        # å›¾åƒ â†’ æ–‡æœ¬ æ³¨æ„åŠ›
        self.image_to_text = CrossModalAttention(dim, num_heads)
        # æ–‡æœ¬ â†’ å›¾åƒ æ³¨æ„åŠ›
        self.text_to_image = CrossModalAttention(dim, num_heads)
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(dim, dim)
        )
    
    def forward(self, image_features, text_features):
        """
        åŒå‘è·¨æ¨¡æ€èåˆ
        
        Args:
            image_features: [B, L_img, D]
            text_features: [B, L_text, D]
        
        Returns:
            fused_features: [B, D]
        """
        # å›¾åƒå…³æ³¨æ–‡æœ¬
        img_attended, _ = self.image_to_text(image_features, text_features)
        
        # æ–‡æœ¬å…³æ³¨å›¾åƒ
        text_attended, _ = self.text_to_image(text_features, image_features)
        
        # æ± åŒ–
        img_pooled = img_attended.mean(dim=1)  # [B, D]
        text_pooled = text_attended.mean(dim=1)  # [B, D]
        
        # æ‹¼æ¥å’Œèåˆ
        combined = torch.cat([img_pooled, text_pooled], dim=-1)
        fused = self.fusion(combined)
        
        return fused

# æµ‹è¯•
fusion_model = ImageTextFusion(dim=512)
image_feat = torch.randn(2, 49, 512)  # batch=2, 49ä¸ªpatch
text_feat = torch.randn(2, 77, 512)   # batch=2, 77ä¸ªtoken

fused = fusion_model(image_feat, text_feat)
print(f"èåˆåç‰¹å¾: {fused.shape}")  # [2, 512]
```

### æœ€ä¼˜ä¼ è¾“å¯¹é½

ä½¿ç”¨æœ€ä¼˜ä¼ è¾“ç†è®ºå¯¹é½ä¸åŒæ¨¡æ€ï¼š

```python
import torch
import torch.nn as nn
import ot  # pip install POT

class OptimalTransportAlignment:
    """æœ€ä¼˜ä¼ è¾“æ¨¡æ€å¯¹é½"""
    
    def __init__(self, reg=0.1):
        """
        Args:
            reg: ç†µæ­£åˆ™åŒ–ç³»æ•°
        """
        self.reg = reg
    
    def compute_cost_matrix(self, features_a, features_b):
        """
        è®¡ç®—ä»£ä»·çŸ©é˜µï¼ˆæ¬§æ°è·ç¦»ï¼‰
        
        Args:
            features_a: [N, D] æ¨¡æ€Açš„ç‰¹å¾
            features_b: [M, D] æ¨¡æ€Bçš„ç‰¹å¾
        
        Returns:
            cost: [N, M] ä»£ä»·çŸ©é˜µ
        """
        # è®¡ç®—L2è·ç¦»
        cost = torch.cdist(features_a, features_b, p=2)
        return cost
    
    def align(self, features_a, features_b):
        """
        ä½¿ç”¨æœ€ä¼˜ä¼ è¾“å¯¹é½ä¸¤ä¸ªæ¨¡æ€
        
        Returns:
            transport_plan: [N, M] ä¼ è¾“è®¡åˆ’
            aligned_b: [N, D] å¯¹é½åçš„æ¨¡æ€Bç‰¹å¾
        """
        N, D = features_a.shape
        M = features_b.shape[0]
        
        # è®¡ç®—ä»£ä»·çŸ©é˜µ
        cost = self.compute_cost_matrix(features_a, features_b)
        cost_np = cost.cpu().numpy()
        
        # å‡åŒ€åˆ†å¸ƒ
        a = torch.ones(N) / N
        b = torch.ones(M) / M
        
        # æ±‚è§£æœ€ä¼˜ä¼ è¾“ï¼ˆSinkhornç®—æ³•ï¼‰
        transport_plan = ot.sinkhorn(
            a.numpy(),
            b.numpy(),
            cost_np,
            reg=self.reg
        )
        
        transport_plan = torch.from_numpy(transport_plan).to(features_a.device)
        
        # æ ¹æ®ä¼ è¾“è®¡åˆ’å¯¹é½ç‰¹å¾
        aligned_b = torch.matmul(transport_plan, features_b) * N
        
        return transport_plan, aligned_b
    
    def compute_ot_loss(self, features_a, features_b):
        """è®¡ç®—æœ€ä¼˜ä¼ è¾“æŸå¤±"""
        transport_plan, _ = self.align(features_a, features_b)
        cost = self.compute_cost_matrix(features_a, features_b)
        
        # æœ€ä¼˜ä¼ è¾“ä»£ä»·
        ot_loss = (transport_plan * cost).sum()
        
        return ot_loss

# ä½¿ç”¨ç¤ºä¾‹
ot_aligner = OptimalTransportAlignment(reg=0.1)

# å‡è®¾æœ‰å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
image_features = torch.randn(50, 512)  # 50ä¸ªå›¾åƒç‰¹å¾
text_features = torch.randn(77, 512)   # 77ä¸ªæ–‡æœ¬ç‰¹å¾

# å¯¹é½
transport_plan, aligned_text = ot_aligner.align(image_features, text_features)

print(f"ä¼ è¾“è®¡åˆ’å½¢çŠ¶: {transport_plan.shape}")
print(f"å¯¹é½åæ–‡æœ¬ç‰¹å¾: {aligned_text.shape}")

# å¯è§†åŒ–ä¼ è¾“è®¡åˆ’
import matplotlib.pyplot as plt
plt.imshow(transport_plan.cpu().numpy(), cmap='viridis')
plt.colorbar()
plt.xlabel("Text Tokens")
plt.ylabel("Image Patches")
plt.title("Optimal Transport Plan")
plt.savefig("ot_plan.png")
```

---

## è·¨æ¨¡æ€æ£€ç´¢

### å›¾åƒæ£€ç´¢æ–‡æœ¬

```python
import torch
import clip
from PIL import Image
import os

class ImageTextRetrieval:
    """å›¾æ–‡æ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self, model_name="ViT-B/32", device="cuda"):
        self.device = device
        self.model, self.preprocess = clip.load(model_name, device=device)
        self.model.eval()
        
        # ç‰¹å¾æ•°æ®åº“
        self.image_features_db = []
        self.text_features_db = []
        self.image_paths = []
        self.texts = []
    
    def index_images(self, image_dir):
        """ç´¢å¼•å›¾åƒåº“"""
        print("æ­£åœ¨ç´¢å¼•å›¾åƒ...")
        
        for img_file in os.listdir(image_dir):
            if not img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue
            
            img_path = os.path.join(image_dir, img_file)
            
            try:
                image = Image.open(img_path)
                image = self.preprocess(image).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    features = self.model.encode_image(image)
                    features /= features.norm(dim=-1, keepdim=True)
                
                self.image_features_db.append(features.cpu())
                self.image_paths.append(img_path)
            except Exception as e:
                print(f"å¤„ç† {img_file} æ—¶å‡ºé”™: {e}")
        
        if self.image_features_db:
            self.image_features_db = torch.cat(self.image_features_db, dim=0)
        
        print(f"å·²ç´¢å¼• {len(self.image_paths)} å¼ å›¾åƒ")
    
    def index_texts(self, text_list):
        """ç´¢å¼•æ–‡æœ¬åº“"""
        print("æ­£åœ¨ç´¢å¼•æ–‡æœ¬...")
        
        batch_size = 100
        for i in range(0, len(text_list), batch_size):
            batch = text_list[i:i+batch_size]
            text_tokens = clip.tokenize(batch).to(self.device)
            
            with torch.no_grad():
                features = self.model.encode_text(text_tokens)
                features /= features.norm(dim=-1, keepdim=True)
            
            self.text_features_db.append(features.cpu())
            self.texts.extend(batch)
        
        if self.text_features_db:
            self.text_features_db = torch.cat(self.text_features_db, dim=0)
        
        print(f"å·²ç´¢å¼• {len(self.texts)} æ¡æ–‡æœ¬")
    
    def search_images_by_text(self, query_text, top_k=5):
        """ç”¨æ–‡æœ¬æ£€ç´¢å›¾åƒ"""
        if not self.image_features_db:
            print("å›¾åƒåº“ä¸ºç©ºï¼Œè¯·å…ˆç´¢å¼•å›¾åƒ")
            return []
        
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        text_token = clip.tokenize([query_text]).to(self.device)
        with torch.no_grad():
            text_features = self.model.encode_text(text_token)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = (self.image_features_db @ text_features.T).squeeze()
        
        # è·å–Top-K
        top_k_indices = similarities.argsort(descending=True)[:top_k]
        
        results = []
        for idx in top_k_indices:
            results.append({
                'image_path': self.image_paths[idx],
                'similarity': similarities[idx].item()
            })
        
        return results
    
    def search_texts_by_image(self, query_image_path, top_k=5):
        """ç”¨å›¾åƒæ£€ç´¢æ–‡æœ¬"""
        if not self.text_features_db:
            print("æ–‡æœ¬åº“ä¸ºç©ºï¼Œè¯·å…ˆç´¢å¼•æ–‡æœ¬")
            return []
        
        # ç¼–ç æŸ¥è¯¢å›¾åƒ
        image = Image.open(query_image_path)
        image = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            image_features = self.model.encode_image(image)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = (self.text_features_db @ image_features.T).squeeze()
        
        # è·å–Top-K
        top_k_indices = similarities.argsort(descending=True)[:top_k]
        
        results = []
        for idx in top_k_indices:
            results.append({
                'text': self.texts[idx],
                'similarity': similarities[idx].item()
            })
        
        return results
    
    def search_images_by_image(self, query_image_path, top_k=5):
        """ç”¨å›¾åƒæ£€ç´¢ç›¸ä¼¼å›¾åƒ"""
        if not self.image_features_db:
            return []
        
        # ç¼–ç æŸ¥è¯¢å›¾åƒ
        image = Image.open(query_image_path)
        image = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            image_features = self.model.encode_image(image)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = (self.image_features_db @ image_features.T).squeeze()
        
        # è·å–Top-Kï¼ˆæ’é™¤è‡ªå·±ï¼‰
        top_k_indices = similarities.argsort(descending=True)[:top_k+1]
        
        results = []
        for idx in top_k_indices:
            if self.image_paths[idx] != query_image_path:
                results.append({
                    'image_path': self.image_paths[idx],
                    'similarity': similarities[idx].item()
                })
                if len(results) >= top_k:
                    break
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
retrieval = ImageTextRetrieval()

# ç´¢å¼•å›¾åƒåº“
retrieval.index_images("path/to/image/dataset")

# ç´¢å¼•æ–‡æœ¬åº“
texts = [
    "a dog playing in the park",
    "a cat sleeping on a sofa",
    "a bird flying in the sky",
    # ... æ›´å¤šæ–‡æœ¬
]
retrieval.index_texts(texts)

# æ–‡æœ¬æ£€ç´¢å›¾åƒ
results = retrieval.search_images_by_text("cute puppy", top_k=5)
print("\næ–‡æœ¬æ£€ç´¢å›¾åƒç»“æœ:")
for i, result in enumerate(results, 1):
    print(f"{i}. {result['image_path']}: {result['similarity']:.4f}")

# å›¾åƒæ£€ç´¢æ–‡æœ¬
results = retrieval.search_texts_by_image("query_image.jpg", top_k=5)
print("\nå›¾åƒæ£€ç´¢æ–‡æœ¬ç»“æœ:")
for i, result in enumerate(results, 1):
    print(f"{i}. {result['text']}: {result['similarity']:.4f}")

# ä»¥å›¾æœå›¾
results = retrieval.search_images_by_image("query_image.jpg", top_k=5)
print("\nä»¥å›¾æœå›¾ç»“æœ:")
for i, result in enumerate(results, 1):
    print(f"{i}. {result['image_path']}: {result['similarity']:.4f}")
```

### æ„å»ºé«˜æ•ˆæ£€ç´¢ç³»ç»Ÿ

ä½¿ç”¨FAISSåŠ é€Ÿå¤§è§„æ¨¡æ£€ç´¢ï¼š

```python
import faiss
import numpy as np

class ScalableRetrieval:
    """å¯æ‰©å±•çš„æ£€ç´¢ç³»ç»Ÿï¼ˆæ”¯æŒç™¾ä¸‡çº§ï¼‰"""
    
    def __init__(self, dimension=512, index_type='Flat'):
        """
        Args:
            dimension: ç‰¹å¾ç»´åº¦
            index_type: ç´¢å¼•ç±»å‹ ('Flat', 'IVF', 'HNSW')
        """
        self.dimension = dimension
        self.index_type = index_type
        
        # åˆ›å»ºFAISSç´¢å¼•
        if index_type == 'Flat':
            # ç²¾ç¡®æœç´¢
            self.index = faiss.IndexFlatIP(dimension)
        elif index_type == 'IVF':
            # å€’æ’æ–‡ä»¶ç´¢å¼•ï¼ˆå¿«é€Ÿä½†è¿‘ä¼¼ï¼‰
            quantizer = faiss.IndexFlatIP(dimension)
            self.index = faiss.IndexIVFFlat(quantizer, dimension, 100)
        elif index_type == 'HNSW':
            # åˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œå›¾ï¼ˆå¿«é€Ÿä¸”å‡†ç¡®ï¼‰
            self.index = faiss.IndexHNSWFlat(dimension, 32)
        
        self.metadata = []
    
    def add(self, features, metadata=None):
        """
        æ·»åŠ ç‰¹å¾åˆ°ç´¢å¼•
        
        Args:
            features: [N, D] numpy array
            metadata: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¦‚æ–‡ä»¶è·¯å¾„ï¼‰
        """
        # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(features)
        
        # è®­ç»ƒç´¢å¼•ï¼ˆIVFéœ€è¦ï¼‰
        if self.index_type == 'IVF' and not self.index.is_trained:
            self.index.train(features)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add(features)
        
        if metadata:
            self.metadata.extend(metadata)
    
    def search(self, query_features, top_k=10):
        """
        æœç´¢æœ€ç›¸ä¼¼çš„ç‰¹å¾
        
        Args:
            query_features: [M, D] numpy array
            top_k: è¿”å›Top-Kç»“æœ
        
        Returns:
            distances: [M, top_k] ç›¸ä¼¼åº¦åˆ†æ•°
            indices: [M, top_k] ç´¢å¼•
        """
        # å½’ä¸€åŒ–
        faiss.normalize_L2(query_features)
        
        # æœç´¢
        distances, indices = self.index.search(query_features, top_k)
        
        return distances, indices
    
    def save(self, filepath):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(self.index, filepath)
    
    def load(self, filepath):
        """åŠ è½½ç´¢å¼•"""
        self.index = faiss.read_index(filepath)

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆå¤„ç†ç™¾ä¸‡çº§æ•°æ®ï¼‰
scalable_retrieval = ScalableRetrieval(dimension=512, index_type='HNSW')

# æ‰¹é‡æ·»åŠ ç‰¹å¾
batch_size = 10000
for batch_features, batch_metadata in large_dataset:
    scalable_retrieval.add(
        batch_features.numpy(),
        metadata=batch_metadata
    )

# æœç´¢
query = get_query_features().numpy()  # [1, 512]
distances, indices = scalable_retrieval.search(query, top_k=10)

# ä¿å­˜ç´¢å¼•
scalable_retrieval.save("image_index.faiss")
```

---

## å¤šæ¨¡æ€èåˆæ¶æ„

### Early Fusionï¼ˆæ—©æœŸèåˆï¼‰

åœ¨ç‰¹å¾æå–å‰èåˆåŸå§‹è¾“å…¥ï¼š

```python
class EarlyFusion(nn.Module):
    """æ—©æœŸèåˆ - åœ¨è¾“å…¥å±‚èåˆ"""
    
    def __init__(self, image_channels=3, text_vocab_size=50000, 
                 embed_dim=512):
        super().__init__()
        
        # å›¾åƒç¼–ç å™¨
        self.image_encoder = nn.Sequential(
            nn.Conv2d(image_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            # ... æ›´å¤šå±‚
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(64, embed_dim)
        )
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_embedding = nn.Embedding(text_vocab_size, embed_dim)
        self.text_encoder = nn.LSTM(embed_dim, embed_dim, batch_first=True)
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(embed_dim * 2, embed_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(embed_dim, num_classes)
    
    def forward(self, images, texts):
        # æå–ç‰¹å¾
        image_features = self.image_encoder(images)
        
        text_embeds = self.text_embedding(texts)
        _, (text_features, _) = self.text_encoder(text_embeds)
        text_features = text_features.squeeze(0)
        
        # æ—©æœŸèåˆï¼ˆæ‹¼æ¥ï¼‰
        fused = torch.cat([image_features, text_features], dim=-1)
        fused = self.fusion(fused)
        
        # é¢„æµ‹
        output = self.classifier(fused)
        return output
```

### Late Fusionï¼ˆåæœŸèåˆï¼‰

åœ¨å†³ç­–å±‚èåˆï¼š

```python
class LateFusion(nn.Module):
    """åæœŸèåˆ - åœ¨å†³ç­–å±‚èåˆ"""
    
    def __init__(self, num_classes=1000):
        super().__init__()
        
        # ç‹¬ç«‹çš„æ¨¡æ€ç¼–ç å™¨
        self.image_model = ImageEncoder(num_classes)
        self.text_model = TextEncoder(num_classes)
        
        # èåˆæƒé‡ï¼ˆå¯å­¦ä¹ ï¼‰
        self.fusion_weights = nn.Parameter(torch.tensor([0.5, 0.5]))
    
    def forward(self, images, texts):
        # å„æ¨¡æ€ç‹¬ç«‹é¢„æµ‹
        image_logits = self.image_model(images)
        text_logits = self.text_model(texts)
        
        # åŠ æƒèåˆ
        weights = F.softmax(self.fusion_weights, dim=0)
        fused_logits = weights[0] * image_logits + weights[1] * text_logits
        
        return fused_logits
```

### Intermediate Fusionï¼ˆä¸­é—´èåˆï¼‰

åœ¨ç‰¹å¾å±‚èåˆï¼š

```python
class IntermediateFusion(nn.Module):
    """ä¸­é—´èåˆ - åœ¨ç‰¹å¾å±‚èåˆ"""
    
    def __init__(self, dim=512, num_classes=1000):
        super().__init__()
        
        # æ¨¡æ€ç¼–ç å™¨
        self.image_encoder = ImageBackbone()
        self.text_encoder = TextBackbone()
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attention = CrossModalAttention(dim)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(dim, num_classes)
        )
    
    def forward(self, images, texts):
        # æå–ä¸­é—´ç‰¹å¾
        image_features = self.image_encoder(images)  # [B, L_img, D]
        text_features = self.text_encoder(texts)     # [B, L_text, D]
        
        # è·¨æ¨¡æ€äº¤äº’
        fused_features, _ = self.cross_attention(
            image_features, text_features
        )
        
        # æ± åŒ–å’Œåˆ†ç±»
        pooled = fused_features.mean(dim=1)
        output = self.classifier(pooled)
        
        return output
```

---

## å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯1: å›¾æ–‡åŒ¹é…

åˆ¤æ–­å›¾åƒå’Œæ–‡æœ¬æ˜¯å¦åŒ¹é…ï¼š

```python
class ImageTextMatching:
    """å›¾æ–‡åŒ¹é…ç³»ç»Ÿ"""
    
    def __init__(self, model, threshold=0.3):
        self.model = model
        self.threshold = threshold
    
    def is_match(self, image_path, text):
        """
        åˆ¤æ–­å›¾åƒå’Œæ–‡æœ¬æ˜¯å¦åŒ¹é…
        
        Returns:
            is_match: bool
            confidence: float
        """
        # ç¼–ç 
        image_features = self.model.encode_image(image_path)
        text_features = self.model.encode_text([text])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = (image_features @ text_features.T).item()
        
        is_match = similarity > self.threshold
        
        return is_match, similarity

# ä½¿ç”¨ç¤ºä¾‹
matcher = ImageTextMatching(model)

image = "dog.jpg"
text = "a photo of a dog"

is_match, confidence = matcher.is_match(image, text)
print(f"åŒ¹é…: {is_match}, ç½®ä¿¡åº¦: {confidence:.4f}")
```

### åœºæ™¯2: è§†è§‰é—®ç­”ï¼ˆVQAï¼‰

```python
from transformers import BlipProcessor, BlipForQuestionAnswering
from PIL import Image

class VisualQuestionAnswering:
    """è§†è§‰é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, model_name="Salesforce/blip-vqa-base"):
        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForQuestionAnswering.from_pretrained(model_name)
        self.model.eval()
    
    def answer_question(self, image_path, question):
        """
        å›ç­”å…³äºå›¾åƒçš„é—®é¢˜
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            question: é—®é¢˜æ–‡æœ¬
        
        Returns:
            answer: ç­”æ¡ˆæ–‡æœ¬
        """
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(image, question, return_tensors="pt")
        
        # ç”Ÿæˆç­”æ¡ˆ
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_length=20)
        
        # è§£ç ç­”æ¡ˆ
        answer = self.processor.decode(outputs[0], skip_special_tokens=True)
        
        return answer

# ä½¿ç”¨ç¤ºä¾‹
vqa = VisualQuestionAnswering()

image = "living_room.jpg"
questions = [
    "What color is the sofa?",
    "How many people are in the image?",
    "What is on the table?"
]

for question in questions:
    answer = vqa.answer_question(image, question)
    print(f"Q: {question}")
    print(f"A: {answer}\n")
```

### åœºæ™¯3: å›¾åƒæè¿°ç”Ÿæˆ

```python
from transformers import BlipProcessor, BlipForConditionalGeneration

class ImageCaptioning:
    """å›¾åƒæè¿°ç”Ÿæˆ"""
    
    def __init__(self, model_name="Salesforce/blip-image-captioning-base"):
        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(model_name)
        self.model.eval()
    
    def generate_caption(self, image_path, max_length=50, num_beams=5):
        """
        ç”Ÿæˆå›¾åƒæè¿°
        
        Returns:
            caption: æè¿°æ–‡æœ¬
        """
        image = Image.open(image_path).convert('RGB')
        
        inputs = self.processor(image, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                num_beams=num_beams
            )
        
        caption = self.processor.decode(outputs[0], skip_special_tokens=True)
        
        return caption
    
    def generate_diverse_captions(self, image_path, num_captions=3):
        """ç”Ÿæˆå¤šæ ·åŒ–çš„æè¿°"""
        image = Image.open(image_path).convert('RGB')
        inputs = self.processor(image, return_tensors="pt")
        
        captions = []
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=50,
                num_return_sequences=num_captions,
                num_beams=num_captions * 2,
                temperature=0.7,
                do_sample=True
            )
        
        for output in outputs:
            caption = self.processor.decode(output, skip_special_tokens=True)
            captions.append(caption)
        
        return captions

# ä½¿ç”¨ç¤ºä¾‹
captioner = ImageCaptioning()

image = "beach.jpg"

# ç”Ÿæˆå•ä¸ªæè¿°
caption = captioner.generate_caption(image)
print(f"æè¿°: {caption}")

# ç”Ÿæˆå¤šæ ·åŒ–æè¿°
captions = captioner.generate_diverse_captions(image, num_captions=3)
print("\nå¤šæ ·åŒ–æè¿°:")
for i, cap in enumerate(captions, 1):
    print(f"{i}. {cap}")
```

---

## é«˜çº§æŠ€å·§

### 1. æ¨¡æ€ç‰¹å®šçš„å½’ä¸€åŒ–

```python
class ModalityNormalization(nn.Module):
    """æ¨¡æ€ç‰¹å®šçš„å½’ä¸€åŒ–"""
    
    def __init__(self, num_modalities=2, dim=512):
        super().__init__()
        self.modality_norms = nn.ModuleList([
            nn.LayerNorm(dim) for _ in range(num_modalities)
        ])
    
    def forward(self, features, modality_id):
        """
        Args:
            features: [B, D]
            modality_id: 0 for image, 1 for text
        """
        return self.modality_norms[modality_id](features)
```

### 2. åŠ¨æ€æ¨¡æ€æƒé‡

```python
class DynamicModalityWeighting(nn.Module):
    """åŠ¨æ€æ¨¡æ€æƒé‡"""
    
    def __init__(self, dim=512):
        super().__init__()
        self.weight_net = nn.Sequential(
            nn.Linear(dim * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 2),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, image_features, text_features):
        # æ‹¼æ¥ç‰¹å¾
        combined = torch.cat([image_features, text_features], dim=-1)
        
        # é¢„æµ‹æƒé‡
        weights = self.weight_net(combined)  # [B, 2]
        
        # åŠ æƒèåˆ
        fused = (weights[:, 0:1] * image_features + 
                weights[:, 1:2] * text_features)
        
        return fused, weights
```

### 3. å¯¹æ¯”å­¦ä¹ æŸå¤±

```python
def contrastive_loss(image_features, text_features, temperature=0.07):
    """
    CLIPé£æ ¼çš„å¯¹æ¯”å­¦ä¹ æŸå¤±
    
    Args:
        image_features: [B, D]
        text_features: [B, D]
        temperature: æ¸©åº¦å‚æ•°
    
    Returns:
        loss: å¯¹æ¯”æŸå¤±
    """
    # å½’ä¸€åŒ–
    image_features = F.normalize(image_features, dim=-1)
    text_features = F.normalize(text_features, dim=-1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    logits = (image_features @ text_features.T) / temperature  # [B, B]
    
    # æ ‡ç­¾ï¼ˆå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬ï¼‰
    labels = torch.arange(len(logits)).to(logits.device)
    
    # åŒå‘æŸå¤±
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.T, labels)
    
    loss = (loss_i2t + loss_t2i) / 2
    
    return loss
```

---

## ğŸ“ æ€»ç»“

å¤šæ¨¡æ€èåˆæŠ€æœ¯è¦ç‚¹ï¼š

1. **å¯¹é½æ–¹æ³•**: å¯¹æ¯”å­¦ä¹ ã€æ³¨æ„åŠ›æœºåˆ¶ã€æœ€ä¼˜ä¼ è¾“
2. **èåˆç­–ç•¥**: æ—©æœŸã€ä¸­æœŸã€æ™šæœŸèåˆ
3. **å®é™…åº”ç”¨**: æ£€ç´¢ã€VQAã€æè¿°ç”Ÿæˆ
4. **é«˜çº§æŠ€å·§**: æ¨¡æ€ç‰¹å®šå¤„ç†ã€åŠ¨æ€æƒé‡

---

## ğŸ”— å‚è€ƒèµ„æº

### è®ºæ–‡
- [CLIP: Learning Transferable Visual Models](https://arxiv.org/abs/2103.00020)
- [BLIP: Bootstrapping Language-Image Pre-training](https://arxiv.org/abs/2201.12086)
- [Optimal Transport for Multi-Modal Alignment](https://arxiv.org/abs/2006.14222)

### ç›¸å…³æ–‡æ¡£
- [BLIP-2æ¨¡å‹è¯¦è§£](../01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/06-BLIP2æ¨¡å‹è¯¦è§£.md)
- [APIéƒ¨ç½²](../04-å¤šå¹³å°éƒ¨ç½²/02-æ¨¡å‹æœåŠ¡åŒ–.md)

---

**ä¸‹ä¸€æ­¥**: [æŒç»­å­¦ä¹ ä¸å¢é‡è®­ç»ƒ](./03-æŒç»­å­¦ä¹ ä¸å¢é‡è®­ç»ƒ.md)

