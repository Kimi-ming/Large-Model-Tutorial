# 模型压缩与加速

本文档介绍视觉大模型的压缩和加速技术，帮助你在保持模型性能的同时，显著降低计算成本和推理延迟。

---

## 📋 目录

1. [为什么需要模型压缩与加速](#为什么需要模型压缩与加速)
2. [模型量化](#模型量化)
3. [模型剪枝](#模型剪枝)
4. [知识蒸馏](#知识蒸馏)
5. [推理引擎优化](#推理引擎优化)
6. [综合实践](#综合实践)
7. [性能对比](#性能对比)

---

## 为什么需要模型压缩与加速

### 面临的挑战

视觉大模型通常具有以下特点：
- **参数量大**: CLIP-ViT-L 有 300M+ 参数
- **计算量大**: 推理需要大量浮点运算
- **内存占用高**: 模型加载需要数GB显存
- **延迟高**: 单张图片推理可能需要 100ms+

### 实际应用需求

| 场景 | 需求 | 挑战 |
|------|------|------|
| 边缘设备 | 低功耗、小内存 | 资源受限 |
| 实时应用 | 低延迟 (< 50ms) | 速度要求高 |
| 大规模服务 | 高吞吐、低成本 | 硬件成本 |
| 移动端应用 | 小体积、快响应 | 网络和存储限制 |

### 优化目标

- **减小模型体积**: 便于部署和分发
- **降低内存占用**: 支持更多并发请求
- **提高推理速度**: 改善用户体验
- **保持模型精度**: 精度损失 < 1-2%

---

## 模型量化

### 什么是量化

量化是将模型参数从高精度（FP32）转换为低精度（INT8/FP16）的过程。

**精度对比**:
- **FP32**: 32位浮点，默认精度
- **FP16**: 16位浮点，半精度
- **INT8**: 8位整数，低精度

**理论收益**:
- FP16: 模型体积 ↓50%，速度 ↑1.5-2x
- INT8: 模型体积 ↓75%，速度 ↑2-4x

### FP16混合精度训练

使用PyTorch的Automatic Mixed Precision (AMP):

```python
import torch
from torch.cuda.amp import autocast, GradScaler
import clip

# 加载模型
device = "cuda"
model, preprocess = clip.load("ViT-B/32", device=device)

# 创建梯度缩放器
scaler = GradScaler()

# 训练循环
for images, texts in dataloader:
    images = images.to(device)
    texts = texts.to(device)
    
    optimizer.zero_grad()
    
    # 使用混合精度
    with autocast():
        image_features = model.encode_image(images)
        text_features = model.encode_text(texts)
        
        # 计算损失
        logits = image_features @ text_features.T
        loss = loss_fn(logits, targets)
    
    # 梯度缩放和更新
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

print("FP16混合精度训练完成")
```

**优势**:
- ✅ 训练速度提升 1.5-2x
- ✅ 显存占用减少 40-50%
- ✅ 几乎无精度损失

### FP16推理

转换模型为FP16进行推理:

```python
import torch
import clip
from PIL import Image

device = "cuda"
model, preprocess = clip.load("ViT-B/32", device=device)

# 转换为FP16
model = model.half()

# 准备输入
image = preprocess(Image.open("test.jpg")).unsqueeze(0).half().to(device)
text = clip.tokenize(["a dog", "a cat"]).to(device)

# 推理
with torch.no_grad(), torch.cuda.amp.autocast():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    
    # 计算相似度
    similarity = (image_features @ text_features.T).softmax(dim=-1)
    
print(f"预测结果: {similarity}")
```

### INT8量化（PyTorch）

使用PyTorch的量化工具:

```python
import torch
import torch.quantization as quantization
import clip

# 1. 加载模型
device = "cpu"  # 量化通常在CPU上进行
model, preprocess = clip.load("ViT-B/32", device=device)
model.eval()

# 2. 准备量化配置
model.qconfig = quantization.get_default_qconfig('fbgemm')

# 3. 准备量化（插入观察器）
quantization.prepare(model, inplace=True)

# 4. 校准（使用代表性数据）
print("开始校准...")
with torch.no_grad():
    for images in calibration_dataloader:
        model.encode_image(images)

# 5. 转换为量化模型
quantization.convert(model, inplace=True)

# 6. 保存量化模型
torch.save(model.state_dict(), "clip_int8.pth")

print("INT8量化完成")

# 7. 测试量化模型
with torch.no_grad():
    output = model.encode_image(test_image)
    print(f"量化模型输出: {output.shape}")
```

### 动态量化（快速方法）

不需要校准数据的量化方法:

```python
import torch
import torch.quantization as quantization

# 动态量化（仅量化权重，激活值保持FP32）
model_dynamic = quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # 指定要量化的层类型
    dtype=torch.qint8
)

# 保存
torch.save(model_dynamic.state_dict(), "clip_dynamic_int8.pth")

print("动态量化完成")
```

### ONNX量化

使用ONNX Runtime进行量化:

```python
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

# 1. 转换为ONNX（如果还没有）
# 参考: code/04-deployment/nvidia/onnx/convert_to_onnx.py

# 2. 量化ONNX模型
model_fp32 = "clip_vit_b32.onnx"
model_int8 = "clip_vit_b32_int8.onnx"

quantize_dynamic(
    model_input=model_fp32,
    model_output=model_int8,
    weight_type=QuantType.QUInt8,
    optimize_model=True
)

print(f"ONNX量化完成: {model_int8}")

# 3. 加载和使用量化模型
import onnxruntime as ort

session = ort.InferenceSession(model_int8)

# 推理
outputs = session.run(
    None,
    {"input": input_data.numpy()}
)
```

---

## 模型剪枝

### 什么是剪枝

剪枝是移除模型中不重要的参数或连接，减少计算量。

**剪枝类型**:
1. **非结构化剪枝**: 移除单个权重（不规则）
2. **结构化剪枝**: 移除整个通道或层（规则）

### 权重剪枝

使用PyTorch的剪枝功能:

```python
import torch
import torch.nn.utils.prune as prune
import clip

# 加载模型
device = "cuda"
model, _ = clip.load("ViT-B/32", device=device)

# 对指定层进行剪枝
def prune_model(model, amount=0.3):
    """
    剪枝模型
    
    Args:
        model: 要剪枝的模型
        amount: 剪枝比例（0-1）
    """
    # 收集所有Linear层
    modules_to_prune = []
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            modules_to_prune.append((module, 'weight'))
    
    # 全局剪枝（保留最重要的权重）
    prune.global_unstructured(
        modules_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=amount,
    )
    
    return model

# 剪枝30%的权重
model = prune_model(model, amount=0.3)

# 使剪枝永久化
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.remove(module, 'weight')

print("模型剪枝完成")

# 微调剪枝后的模型
# ... 训练代码 ...
```

### 结构化剪枝

移除整个通道或注意力头:

```python
import torch
import torch.nn.utils.prune as prune

def structured_prune(model, amount=0.2):
    """
    结构化剪枝 - 移除整个通道
    
    Args:
        model: 要剪枝的模型
        amount: 剪枝比例
    """
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Conv2d):
            # 按L2范数剪枝通道
            prune.ln_structured(
                module,
                name='weight',
                amount=amount,
                n=2,
                dim=0  # 输出通道维度
            )
    
    return model

model = structured_prune(model, amount=0.2)
print("结构化剪枝完成")
```

### 渐进式剪枝

在训练过程中逐步增加剪枝比例:

```python
import torch.nn.utils.prune as prune

class ProgressivePruning:
    def __init__(self, model, initial_sparsity=0.0, final_sparsity=0.5, 
                 pruning_steps=10):
        self.model = model
        self.initial_sparsity = initial_sparsity
        self.final_sparsity = final_sparsity
        self.pruning_steps = pruning_steps
        self.current_step = 0
    
    def step(self):
        """执行一步剪枝"""
        self.current_step += 1
        
        # 计算当前稀疏度
        current_sparsity = self.initial_sparsity + \
            (self.final_sparsity - self.initial_sparsity) * \
            (self.current_step / self.pruning_steps)
        
        # 剪枝
        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.Linear):
                prune.l1_unstructured(
                    module,
                    name='weight',
                    amount=current_sparsity
                )
        
        print(f"Step {self.current_step}: 稀疏度 = {current_sparsity:.2%}")

# 使用示例
pruner = ProgressivePruning(model, final_sparsity=0.5, pruning_steps=10)

for epoch in range(num_epochs):
    # 训练
    train_one_epoch()
    
    # 每个epoch后剪枝
    if epoch % (num_epochs // 10) == 0:
        pruner.step()
```

---

## 知识蒸馏

### 什么是知识蒸馏

知识蒸馏是让小模型（学生）学习大模型（教师）的知识。

**优势**:
- 小模型速度快、体积小
- 保持接近大模型的性能
- 适合边缘设备部署

### CLIP模型蒸馏

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import clip

class KnowledgeDistillation:
    def __init__(self, teacher_model, student_model, temperature=4.0, alpha=0.5):
        """
        知识蒸馏
        
        Args:
            teacher_model: 教师模型（大模型）
            student_model: 学生模型（小模型）
            temperature: 温度参数（软化概率分布）
            alpha: 蒸馏损失权重
        """
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature
        self.alpha = alpha
        
        # 教师模型设为评估模式
        self.teacher.eval()
        for param in self.teacher.parameters():
            param.requires_grad = False
    
    def distillation_loss(self, student_logits, teacher_logits, labels):
        """
        计算蒸馏损失
        
        Returns:
            total_loss: 总损失
            hard_loss: 硬标签损失
            soft_loss: 软标签损失
        """
        # 硬标签损失（使用真实标签）
        hard_loss = F.cross_entropy(student_logits, labels)
        
        # 软标签损失（使用教师模型输出）
        soft_loss = F.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=1),
            F.softmax(teacher_logits / self.temperature, dim=1),
            reduction='batchmean'
        ) * (self.temperature ** 2)
        
        # 总损失
        total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss
        
        return total_loss, hard_loss, soft_loss

# 使用示例
device = "cuda"

# 教师模型: CLIP-ViT-L/14 (大模型)
teacher, _ = clip.load("ViT-L/14", device=device)
teacher.eval()

# 学生模型: CLIP-ViT-B/32 (小模型)
student, preprocess = clip.load("ViT-B/32", device=device)
student.train()

# 创建蒸馏训练器
distiller = KnowledgeDistillation(
    teacher_model=teacher,
    student_model=student,
    temperature=4.0,
    alpha=0.7  # 更重视蒸馏损失
)

# 训练循环
optimizer = torch.optim.Adam(student.parameters(), lr=1e-5)

for epoch in range(num_epochs):
    for images, texts, labels in dataloader:
        images = images.to(device)
        texts = texts.to(device)
        labels = labels.to(device)
        
        # 教师模型推理
        with torch.no_grad():
            teacher_image_features = teacher.encode_image(images)
            teacher_text_features = teacher.encode_text(texts)
            teacher_logits = teacher_image_features @ teacher_text_features.T
        
        # 学生模型推理
        student_image_features = student.encode_image(images)
        student_text_features = student.encode_text(texts)
        student_logits = student_image_features @ student_text_features.T
        
        # 计算蒸馏损失
        loss, hard_loss, soft_loss = distiller.distillation_loss(
            student_logits, teacher_logits, labels
        )
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if step % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}, "
                  f"Hard: {hard_loss:.4f}, Soft: {soft_loss:.4f}")

print("知识蒸馏完成")
```

### 特征蒸馏

不仅蒸馏输出，还蒸馏中间特征:

```python
class FeatureDistillation:
    def __init__(self, teacher, student, feature_layers):
        """
        特征蒸馏
        
        Args:
            teacher: 教师模型
            student: 学生模型
            feature_layers: 要蒸馏的层名称列表
        """
        self.teacher = teacher
        self.student = student
        self.feature_layers = feature_layers
        
        # 注册钩子提取中间特征
        self.teacher_features = {}
        self.student_features = {}
        
        for name in feature_layers:
            # 教师模型钩子
            getattr(teacher, name).register_forward_hook(
                lambda m, i, o, name=name: self.teacher_features.update({name: o})
            )
            # 学生模型钩子
            getattr(student, name).register_forward_hook(
                lambda m, i, o, name=name: self.student_features.update({name: o})
            )
    
    def feature_loss(self):
        """计算特征蒸馏损失"""
        loss = 0
        for name in self.feature_layers:
            teacher_feat = self.teacher_features[name]
            student_feat = self.student_features[name]
            
            # L2损失
            loss += F.mse_loss(student_feat, teacher_feat)
        
        return loss / len(self.feature_layers)
```

---

## 推理引擎优化

### TensorRT加速

使用NVIDIA TensorRT加速推理:

```python
import torch
import torch_tensorrt

# 加载PyTorch模型
model = load_model()
model.eval()

# 准备示例输入
example_input = torch.randn(1, 3, 224, 224).cuda()

# 编译为TensorRT
trt_model = torch_tensorrt.compile(
    model,
    inputs=[torch_tensorrt.Input(
        min_shape=[1, 3, 224, 224],
        opt_shape=[4, 3, 224, 224],
        max_shape=[8, 3, 224, 224]
    )],
    enabled_precisions={torch.float16},  # FP16精度
    workspace_size=1 << 30  # 1GB workspace
)

# 保存TensorRT模型
torch.jit.save(trt_model, "model_trt.ts")

# 推理
with torch.no_grad():
    output = trt_model(example_input)

print("TensorRT加速完成")
```

### ONNX Runtime优化

```python
import onnxruntime as ort
import numpy as np

# 创建推理会话（启用优化）
session_options = ort.SessionOptions()
session_options.graph_optimization_level = \
    ort.GraphOptimizationLevel.ORT_ENABLE_ALL

# 使用CUDA执行提供器
providers = [
    ('CUDAExecutionProvider', {
        'device_id': 0,
        'arena_extend_strategy': 'kNextPowerOfTwo',
        'gpu_mem_limit': 2 * 1024 * 1024 * 1024,  # 2GB
        'cudnn_conv_algo_search': 'EXHAUSTIVE',
    }),
    'CPUExecutionProvider'
]

session = ort.InferenceSession(
    "model.onnx",
    sess_options=session_options,
    providers=providers
)

# 推理
input_name = session.get_inputs()[0].name
output = session.run(None, {input_name: input_data})

print("ONNX Runtime优化完成")
```

### 批处理优化

```python
import torch
from typing import List

class BatchInference:
    def __init__(self, model, batch_size=32, max_wait_time=0.1):
        """
        批处理推理
        
        Args:
            model: 推理模型
            batch_size: 批大小
            max_wait_time: 最大等待时间（秒）
        """
        self.model = model
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.queue = []
    
    def predict(self, input_data):
        """添加到队列并返回预测"""
        self.queue.append(input_data)
        
        # 如果队列满或超时，执行批处理
        if len(self.queue) >= self.batch_size:
            return self._process_batch()
    
    def _process_batch(self):
        """处理一个批次"""
        if not self.queue:
            return []
        
        # 合并输入
        batch = torch.stack(self.queue[:self.batch_size])
        
        # 批量推理
        with torch.no_grad():
            outputs = self.model(batch)
        
        # 清空已处理的数据
        self.queue = self.queue[self.batch_size:]
        
        return outputs

# 使用示例
batch_inference = BatchInference(model, batch_size=32)

# 推理
results = []
for image in images:
    result = batch_inference.predict(image)
    if result is not None:
        results.extend(result)
```

---

## 综合实践

### 完整优化流程

```python
import torch
import clip
from torch.quantization import quantize_dynamic

class ModelOptimizer:
    """模型优化器 - 集成多种优化技术"""
    
    def __init__(self, model_name="ViT-B/32"):
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def optimize(self, techniques=['fp16', 'prune', 'quantize']):
        """
        应用优化技术
        
        Args:
            techniques: 优化技术列表
        
        Returns:
            optimized_model: 优化后的模型
        """
        # 1. 加载原始模型
        model, preprocess = clip.load(self.model_name, device=self.device)
        model.eval()
        
        print(f"原始模型大小: {self.get_model_size(model):.2f} MB")
        
        # 2. FP16转换
        if 'fp16' in techniques:
            model = model.half()
            print(f"FP16后大小: {self.get_model_size(model):.2f} MB")
        
        # 3. 模型剪枝
        if 'prune' in techniques:
            model = self.prune_model(model, amount=0.3)
            print(f"剪枝后稀疏度: {self.get_sparsity(model):.2%}")
        
        # 4. 量化
        if 'quantize' in techniques and self.device == 'cpu':
            model = quantize_dynamic(
                model,
                {torch.nn.Linear},
                dtype=torch.qint8
            )
            print(f"量化后大小: {self.get_model_size(model):.2f} MB")
        
        return model, preprocess
    
    @staticmethod
    def get_model_size(model):
        """计算模型大小（MB）"""
        param_size = sum(p.numel() * p.element_size() 
                        for p in model.parameters())
        buffer_size = sum(b.numel() * b.element_size() 
                         for b in model.buffers())
        return (param_size + buffer_size) / (1024 ** 2)
    
    @staticmethod
    def get_sparsity(model):
        """计算模型稀疏度"""
        total = 0
        zeros = 0
        for param in model.parameters():
            total += param.numel()
            zeros += (param == 0).sum().item()
        return zeros / total if total > 0 else 0
    
    @staticmethod
    def prune_model(model, amount=0.3):
        """剪枝模型"""
        import torch.nn.utils.prune as prune
        
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=amount)
                prune.remove(module, 'weight')
        
        return model

# 使用示例
optimizer = ModelOptimizer("ViT-B/32")

# 应用所有优化
optimized_model, preprocess = optimizer.optimize(
    techniques=['fp16', 'prune']
)

# 测试性能
import time
test_image = torch.randn(1, 3, 224, 224).to(optimizer.device)

start = time.time()
with torch.no_grad():
    _ = optimized_model.encode_image(test_image.half())
torch.cuda.synchronize()
end = time.time()

print(f"推理时间: {(end - start) * 1000:.2f} ms")
```

---

## 性能对比

### 不同优化技术的效果

| 优化技术 | 模型大小 | 推理速度 | 精度损失 | 适用场景 |
|---------|---------|---------|---------|---------|
| **原始FP32** | 100% | 1.0x | 0% | 基准 |
| **FP16** | 50% | 1.5-2.0x | < 0.1% | GPU推理 |
| **INT8量化** | 25% | 2-4x | 0.5-2% | CPU/边缘设备 |
| **剪枝30%** | 70% | 1.2-1.5x | 0.5-1% | 通用 |
| **蒸馏** | 30-50% | 2-3x | 1-3% | 小模型部署 |
| **TensorRT** | 100% | 2-5x | < 0.1% | NVIDIA GPU |
| **综合优化** | 15-30% | 3-8x | 1-3% | 生产环境 |

### CLIP模型优化示例

**测试环境**: NVIDIA RTX 3090, Batch Size = 1

| 配置 | 大小 | 延迟 | 吞吐量 | Top-1准确率 |
|------|------|------|--------|------------|
| ViT-B/32 FP32 | 338 MB | 45 ms | 22 img/s | 63.2% |
| ViT-B/32 FP16 | 169 MB | 28 ms | 36 img/s | 63.1% |
| ViT-B/32 INT8 | 85 MB | 18 ms | 56 img/s | 62.5% |
| ViT-B/32 TensorRT | 169 MB | 15 ms | 67 img/s | 63.1% |
| ViT-B/16 → B/32蒸馏 | 169 MB | 28 ms | 36 img/s | 64.8% |

---

## 📝 实践建议

### 1. 选择合适的优化技术

**云端GPU服务**:
- FP16 + TensorRT
- 追求极致速度和吞吐量

**边缘设备**:
- INT8量化 + 模型剪枝
- 平衡精度和资源消耗

**移动端**:
- 知识蒸馏 + INT8量化
- 使用专门的轻量级模型

### 2. 优化流程建议

1. **基准测试**: 先测量原始性能
2. **单一优化**: 逐个应用优化技术
3. **评估影响**: 测试精度和速度变化
4. **组合优化**: 结合多种技术
5. **生产验证**: 在真实场景中测试

### 3. 注意事项

- ⚠️ 量化可能导致精度损失，需要在代表性数据上校准
- ⚠️ 剪枝后需要微调以恢复精度
- ⚠️ 不同硬件平台的优化效果差异大
- ⚠️ 优化后要在真实场景中全面测试

---

## 🔗 参考资源

### 工具和库
- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)
- [TensorRT](https://developer.nvidia.com/tensorrt)
- [ONNX Runtime](https://onnxruntime.ai/)
- [Neural Network Intelligence (NNI)](https://github.com/microsoft/nni)

### 相关文档
- [NVIDIA部署文档](../04-多平台部署/01-NVIDIA部署基础.md)
- [模型服务化](../04-多平台部署/02-模型服务化.md)

### 论文
- [Quantization and Training of Neural Networks](https://arxiv.org/abs/1712.05877)
- [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

---

## 🎯 实践任务

### 任务1: FP16优化
1. 加载CLIP模型
2. 转换为FP16
3. 对比推理速度和精度

### 任务2: 模型量化
1. 准备校准数据集
2. 量化模型为INT8
3. 评估量化前后的性能差异

### 任务3: 知识蒸馏
1. 选择教师模型（大模型）和学生模型（小模型）
2. 实现蒸馏训练
3. 对比学生模型和教师模型的性能

### 任务4: 综合优化
1. 结合多种优化技术
2. 在真实场景中测试
3. 记录优化前后的各项指标

---

**下一步**: [多模态融合](./02-多模态融合.md)

