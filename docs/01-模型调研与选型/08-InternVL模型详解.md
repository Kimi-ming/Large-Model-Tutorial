# InternVLæ¨¡å‹è¯¦è§£

## ğŸ’¡ å­¦ä¹ è€…æç¤º

**å­¦ä¹ ç›®æ ‡**:
- æ·±å…¥ç†è§£InternVLæ¨¡å‹æ¶æ„å’ŒåŸç†
- æŒæ¡InternVLåœ¨å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨
- å­¦ä¼šä½¿ç”¨InternVLè¿›è¡Œè§†è§‰-è¯­è¨€ä»»åŠ¡

**å…ˆä¿®è¦æ±‚**:
- äº†è§£Transformeræ¶æ„åŸºç¡€
- é˜…è¯»è¿‡[ä¸»æµè§†è§‰å¤§æ¨¡å‹æ¦‚è¿°](./01-ä¸»æµè§†è§‰å¤§æ¨¡å‹æ¦‚è¿°.md)
- ç†Ÿæ‚‰Pythonå’ŒPyTorch

**éš¾åº¦**: â­â­â­â˜†â˜†(ä¸­ç­‰)
**é¢„è®¡æ—¶é—´**: 60-90åˆ†é’Ÿ

---

## ğŸ“š æ¨¡å‹æ¦‚è¿°

### ä»€ä¹ˆæ˜¯InternVL?

**InternVL**æ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤(Shanghai AI Lab)å’Œå•†æ±¤ç§‘æŠ€è”åˆå¼€å‘çš„å¼€æºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹,å…¶æ€§èƒ½æ¥è¿‘GPT-4V,æ˜¯å½“å‰å¼€æºé¢†åŸŸæœ€å¼ºçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¹‹ä¸€ã€‚

**å¼€å‘å›¢é˜Ÿ**: ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ & å•†æ±¤ç§‘æŠ€
**å‘å¸ƒæ—¶é—´**: 2023å¹´12æœˆ(v1.0), 2024å¹´7æœˆ(v2.0), 2025å¹´1æœˆ(v3.0)
**å¼€æºåœ°å€**: [GitHub](https://github.com/OpenGVLab/InternVL)
**è®ºæ–‡**: [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://arxiv.org/abs/2312.14238)

### æ ¸å¿ƒç‰¹ç‚¹

1. **GPT-4Vçº§åˆ«æ€§èƒ½** ğŸš€
   - åœ¨å¤šä¸ªè§†è§‰-è¯­è¨€åŸºå‡†ä¸Šæ¥è¿‘æˆ–è¶…è¶ŠGPT-4V
   - å¤šé¡¹ä»»åŠ¡è¾¾åˆ°å¼€æºæ¨¡å‹SOTA
   - å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›

2. **å¤šè¯­è¨€æ”¯æŒ** ğŸŒ
   - æ”¯æŒä¸­è‹±æ–‡ç­‰å¤šç§è¯­è¨€
   - ä¼˜ç§€çš„ä¸­æ–‡ç†è§£èƒ½åŠ›
   - è·¨è¯­è¨€è§†è§‰æ¨ç†

3. **é«˜åˆ†è¾¨ç‡ç†è§£** ğŸ”
   - æ”¯æŒåŠ¨æ€é«˜åˆ†è¾¨ç‡(æœ€é«˜4K+)
   - ç»†ç²’åº¦è§†è§‰ç†è§£
   - ç²¾ç¡®çš„OCRå’Œæ£€æµ‹èƒ½åŠ›

4. **çµæ´»çš„æ¨¡å‹è§„æ¨¡** ğŸ“
   - InternVL3-1B: è½»é‡çº§(1Bå‚æ•°)
   - InternVL2-8B: å¹³è¡¡ç‰ˆ(8Bå‚æ•°)
   - InternVL3-8B: é«˜æ€§èƒ½ç‰ˆ(8Bå‚æ•°)
   - InternVL3-78B: æ——èˆ°ç‰ˆ(78Bå‚æ•°)

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### æ•´ä½“æ¶æ„

```
è¾“å…¥å›¾åƒ â”€â”€â”
          â”œâ”€â”€â–º InternViT-6B â”€â”€â–º Vision Adapter â”€â”€â”
è¾“å…¥æ–‡æœ¬ â”€â”€â”˜                                    â”œâ”€â”€â–º LLM Backbone â”€â”€â–º è¾“å‡ºæ–‡æœ¬
                                                â”‚
                                         Cross-Attention
```

### ä¸»è¦ç»„ä»¶

#### 1. è§†è§‰ç¼–ç å™¨(InternViT-6B)

```python
# InternViTæ¶æ„ç¤ºæ„
InternViT-6B (çº¦6Bå‚æ•°)
â”œâ”€â”€ Patch Embedding (14Ã—14 æˆ– dynamic)
â”œâ”€â”€ 48å±‚ Vision Transformer Blocks
â”‚   â”œâ”€â”€ Multi-Head Self-Attention
â”‚   â”œâ”€â”€ Feed-Forward Network
â”‚   â”œâ”€â”€ Layer Normalization
â”‚   â””â”€â”€ Residual Connection
â””â”€â”€ è¾“å‡º: é«˜è´¨é‡è§†è§‰ç‰¹å¾
```

**ç‰¹ç‚¹**:
- åŸºäºViT-6Bæ¶æ„
- æ”¯æŒåŠ¨æ€åˆ†è¾¨ç‡(336px-4K+)
- åœ¨è¶…å¤§è§„æ¨¡æ•°æ®ä¸Šé¢„è®­ç»ƒ
- å¼ºå¤§çš„è§†è§‰è¡¨å¾èƒ½åŠ›

#### 2. è§†è§‰-è¯­è¨€é€‚é…å™¨(Vision-Language Adapter)

```python
# é€‚é…å™¨è®¾è®¡
MLP-based Projector
â”œâ”€â”€ Linear Layer 1 (Vision Dim â†’ Hidden Dim)
â”œâ”€â”€ GELU Activation
â”œâ”€â”€ Linear Layer 2 (Hidden Dim â†’ LLM Dim)
â””â”€â”€ LayerNorm
```

**ä½œç”¨**:
- å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€æ¨¡å‹ç©ºé—´
- ä¿æŒè§†è§‰ä¿¡æ¯çš„å®Œæ•´æ€§
- é«˜æ•ˆçš„æ¨¡æ€å¯¹é½

#### 3. å¤§è¯­è¨€æ¨¡å‹éª¨å¹²ç½‘ç»œ

**InternVL2/3æ”¯æŒå¤šç§LLM**:

```python
# LLMé€‰é¡¹(ä»¥InternLM2ä¸ºä¾‹)
InternLM2-Chat-7B (7Bå‚æ•°)
â”œâ”€â”€ 32å±‚ Decoder Transformer
â”‚   â”œâ”€â”€ Grouped Query Attention (GQA)
â”‚   â”œâ”€â”€ SwiGLU Activation
â”‚   â”œâ”€â”€ RMSNorm
â”‚   â””â”€â”€ Rotary Position Embedding
â”œâ”€â”€ è¯è¡¨: 92,544 tokens
â””â”€â”€ ä¸Šä¸‹æ–‡é•¿åº¦: 32K tokens

# å…¶ä»–æ”¯æŒçš„LLM
- Vicuna-7B/13B
- Nous-Hermes-2-Yi-34B
- Qwen2-7B
ç­‰...
```

---

## ğŸ¯ æ ¸å¿ƒèƒ½åŠ›

### 1. å›¾åƒæè¿°ç”Ÿæˆ(Image Captioning)

**èƒ½åŠ›æè¿°**: ç”Ÿæˆå‡†ç¡®ã€è¯¦ç»†çš„å›¾åƒæè¿°(æ”¯æŒå¤šè¯­è¨€)

**ç¤ºä¾‹ä»£ç **:
```python
from transformers import AutoModel, AutoProcessor
from PIL import Image

# åŠ è½½æ¨¡å‹
model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
).eval()

processor = AutoProcessor.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    trust_remote_code=True
)

# æ¨ç†
image = Image.open("image.jpg")
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Please describe this image in detail."}
        ]
    }
]

prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[image], return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=512)
response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
print(response)
```

**å‚è€ƒè¾“å‡º**:
```
This image shows a bustling city street during daytime.
The scene features modern high-rise buildings with glass facades
reflecting the sunlight. Several cars and pedestrians can be seen
on the wide avenue. The clear blue sky and bright atmosphere
suggest it's a sunny day in an urban metropolitan area.
```

### 2. è§†è§‰é—®ç­”(Visual Question Answering)

**èƒ½åŠ›æè¿°**: åŸºäºå›¾åƒå†…å®¹å›ç­”å¤æ‚é—®é¢˜

**æ”¯æŒçš„é—®é¢˜ç±»å‹**:
- **è®¡æ•°é—®é¢˜**: "How many people are in the image?"
- **è¯†åˆ«é—®é¢˜**: "What breed is this dog?"
- **å…³ç³»é—®é¢˜**: "What is the person doing?"
- **å±æ€§é—®é¢˜**: "What color is the car?"
- **æ¨ç†é—®é¢˜**: "Where was this photo likely taken?"
- **æ¯”è¾ƒé—®é¢˜**: "Which object is larger?"

**ç¤ºä¾‹ä»£ç **:
```python
# VQAç¤ºä¾‹
questions = [
    "How many people are in this image?",
    "What are they doing?",
    "Is this during day or night?",
    "What is the weather like?"
]

for question in questions:
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": question}
            ]
        }
    ]

    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=128)
    answer = processor.batch_decode(outputs, skip_special_tokens=True)[0]

    print(f"Q: {question}")
    print(f"A: {answer}\n")
```

### 3. OCRæ–‡å­—è¯†åˆ«

**èƒ½åŠ›æè¿°**: é«˜ç²¾åº¦çš„æ–‡å­—è¯†åˆ«å’Œç†è§£

**æ”¯æŒåœºæ™¯**:
- æ–‡æ¡£æ‰«æå›¾
- è‡ªç„¶åœºæ™¯æ–‡å­—
- æ‰‹å†™æ–‡å­—
- å¤šè¯­è¨€æ–‡æœ¬
- è¡¨æ ¼è¯†åˆ«

**ç¤ºä¾‹ä»£ç **:
```python
# OCRè¯†åˆ«
ocr_prompt = "Please extract all text from this image and organize it."

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": ocr_prompt}
        ]
    }
]

prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[document_image], return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=1024)
text = processor.batch_decode(outputs, skip_special_tokens=True)[0]
print(text)
```

**æ€§èƒ½æŒ‡æ ‡**:
- è‹±æ–‡OCRå‡†ç¡®ç‡: 94.5% F1-score
- ä¸­æ–‡OCRå‡†ç¡®ç‡: 91.2% F1-score
- åœºæ™¯æ–‡å­—è¯†åˆ«: 88.7% F1-score

### 4. å¤šå›¾ç†è§£

**èƒ½åŠ›æè¿°**: åŒæ—¶å¤„ç†å¤šå¼ å›¾ç‰‡å¹¶ç†è§£å®ƒä»¬çš„å…³ç³»

**ç¤ºä¾‹ä»£ç **:
```python
# å¤šå›¾ç†è§£
image1 = Image.open("image1.jpg")
image2 = Image.open("image2.jpg")

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "image"},
            {"type": "text", "text": "Compare these two images. What are the similarities and differences?"}
        ]
    }
]

prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[image1, image2], return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=512)
response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
print(response)
```

**åº”ç”¨åœºæ™¯**:
- å›¾ç‰‡å¯¹æ¯”åˆ†æ
- æ—¶é—´åºåˆ—ç†è§£
- å¤šè§†è§’åœºæ™¯é‡å»º
- è§†é¢‘å¸§ç†è§£

### 5. å¤šè½®å¯¹è¯

**èƒ½åŠ›æè¿°**: åŸºäºå›¾åƒçš„ä¸Šä¸‹æ–‡è¿è´¯å¯¹è¯

**ç¤ºä¾‹ä»£ç **:
```python
# å¤šè½®å¯¹è¯
image = Image.open("image.jpg")

# æ„å»ºå¯¹è¯å†å²
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What do you see in this image?"}
        ]
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "I see a red car parked on the street."}
        ]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "What brand is it?"}
        ]
    }
]

prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
inputs = processor(text=prompt, images=[image], return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=128)
response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
print(response)
```

---

## ğŸ“Š æ€§èƒ½è¯„æµ‹

### åŸºå‡†æµ‹è¯•ç»“æœ

#### é€šç”¨VQAä»»åŠ¡

| æ•°æ®é›† | InternVL2-8B | InternVL3-8B | GPT-4V | è¯´æ˜ |
|--------|--------------|--------------|--------|------|
| **VQAv2** | 82.3% | 84.1% | 80.6% | é€šç”¨VQAåŸºå‡† |
| **GQA** | 64.2% | 66.5% | 63.8% | è§†è§‰æ¨ç† |
| **TextVQA** | 73.4% | 75.6% | 78.0% | æ–‡å­—VQA |
| **DocVQA** | 90.9% | 92.1% | 88.4% | æ–‡æ¡£VQA |

#### OCRå’Œæ–‡æ¡£ç†è§£

| ä»»åŠ¡ç±»å‹ | InternVL2-8B | InternVL3-8B | è¯´æ˜ |
|---------|--------------|--------------|------|
| **OCRBench** | 794 | 822 | OCRç»¼åˆè¯„æµ‹ |
| **ChartQA** | 83.3% | 86.2% | å›¾è¡¨ç†è§£ |
| **InfoVQA** | 70.9% | 73.5% | ä¿¡æ¯å›¾ç†è§£ |

#### å¤šæ¨¡æ€åŸºå‡†

| åŸºå‡† | InternVL2-8B | InternVL3-8B | GPT-4V | è¯´æ˜ |
|------|--------------|--------------|--------|------|
| **MMBench** | 83.6 | 85.7 | 83.0 | å¤šæ¨¡æ€ç»¼åˆ |
| **MMMU** | 51.2 | 54.0 | 56.8 | å¤šå­¦ç§‘ç†è§£ |
| **MathVista** | 58.3 | 61.2 | 63.8% | æ•°å­¦æ¨ç† |

### æ¨ç†æ€§èƒ½

**æµ‹è¯•ç¯å¢ƒ**: NVIDIA A100 (40GB)

| æ¨¡å‹ç‰ˆæœ¬ | å‚æ•°é‡ | ååé‡ | å¹³å‡å»¶è¿Ÿ | æ˜¾å­˜å ç”¨ |
|---------|--------|--------|---------|---------|
| **InternVL3-1B** | 1B | 8.5 samples/s | 235ms | 4.2GB |
| **InternVL2-8B** | 8B | 3.2 samples/s | 625ms | 18.5GB |
| **InternVL3-8B** | 8B | 3.8 samples/s | 526ms | 19.2GB |
| **InternVL3-78B** | 78B | 0.4 samples/s | 2.5s | 156GB |

**æ€§èƒ½ç‰¹ç‚¹**:
- âœ… InternVL3-1Bå¯åœ¨æ¶ˆè´¹çº§GPUè¿è¡Œ
- âœ… InternVL2/3-8Bå•å¡å¯éƒ¨ç½²(16GB+æ˜¾å­˜)
- âœ… æ”¯æŒBFloat16å’Œé‡åŒ–
- âš ï¸ InternVL3-78Béœ€è¦å¤šå¡éƒ¨ç½²

---

## ğŸ› ï¸ ä½¿ç”¨æŒ‡å—

### ç¯å¢ƒé…ç½®

#### 1. åŸºç¡€ä¾èµ–

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch>=2.0.0 torchvision
pip install transformers>=4.37.2
pip install accelerate
pip install pillow

# å¯é€‰:åŠ é€Ÿæ¨ç†
pip install flash-attn  # Flash Attention 2(éœ€è¦CUDA 11.8+)
```

#### 2. æ¨¡å‹ä¸‹è½½

```python
from transformers import AutoModelForImageTextToText, AutoProcessor

# æ–¹å¼1: è‡ªåŠ¨ä¸‹è½½(éœ€è¦ç½‘ç»œ)
model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# æ–¹å¼2: ä»æœ¬åœ°åŠ è½½
model = AutoModelForImageTextToText.from_pretrained(
    "/path/to/local/model",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)
```

**æ¨¡å‹å¤§å°**:
- InternVL3-1B: ~2GB
- InternVL2-8B: ~16GB
- InternVL3-8B: ~18GB
- InternVL3-78B: ~150GB

### åŸºç¡€ä½¿ç”¨

#### å®Œæ•´ç¤ºä¾‹

```python
#!/usr/bin/env python3
"""InternVLåŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""

from transformers import AutoModelForImageTextToText, AutoProcessor
import torch
from PIL import Image

# 1. åŠ è½½æ¨¡å‹
print("åŠ è½½æ¨¡å‹...")
model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True
).eval()

processor = AutoProcessor.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    trust_remote_code=True
)

# 2. å•å›¾æ¨ç†
def single_image_inference(image_path, question):
    image = Image.open(image_path).convert('RGB')

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": question}
            ]
        }
    ]

    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=512)

    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return response

# 3. å¤šè½®å¯¹è¯
def multi_turn_chat(image_path):
    image = Image.open(image_path).convert('RGB')

    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "What is in this image?"}
            ]
        }
    ]

    # ç¬¬ä¸€è½®
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    outputs = model.generate(**inputs, max_new_tokens=256)
    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    print(f"Round 1: {response}")

    # æ·»åŠ åŠ©æ‰‹å›å¤
    conversation.append({
        "role": "assistant",
        "content": [{"type": "text", "text": response}]
    })

    # ç¬¬äºŒè½®
    conversation.append({
        "role": "user",
        "content": [{"type": "text", "text": "What color is it?"}]
    })

    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    outputs = model.generate(**inputs, max_new_tokens=256)
    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    print(f"Round 2: {response}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å•å›¾æ¨ç†
    response = single_image_inference(
        "test.jpg",
        "Describe this image in detail."
    )
    print(response)

    # å¤šè½®å¯¹è¯
    multi_turn_chat("test.jpg")
```

### é«˜çº§åŠŸèƒ½

#### 1. è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°

```python
# è°ƒæ•´ç”Ÿæˆå‚æ•°ä»¥è·å¾—æ›´å¥½çš„è¾“å‡º
generation_config = {
    "max_new_tokens": 1024,     # æœ€å¤§ç”Ÿæˆé•¿åº¦
    "temperature": 0.7,          # æ¸©åº¦å‚æ•°(è¶Šé«˜è¶Šéšæœº)
    "top_p": 0.9,                # nucleus sampling
    "do_sample": True,           # å¯ç”¨é‡‡æ ·
    "repetition_penalty": 1.1,   # é‡å¤æƒ©ç½š
    "num_beams": 1,              # beam search
}

outputs = model.generate(**inputs, **generation_config)
```

#### 2. æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªå›¾ç‰‡
@torch.no_grad()
def batch_inference(image_paths, questions, batch_size=4):
    results = []

    for i in range(0, len(image_paths), batch_size):
        batch_images = [Image.open(p).convert('RGB')
                       for p in image_paths[i:i+batch_size]]
        batch_questions = questions[i:i+batch_size]

        # æ„å»ºæ¶ˆæ¯
        batch_messages = [
            [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": q}
                    ]
                }
            ]
            for q in batch_questions
        ]

        # æ‰¹é‡å¤„ç†
        for msgs, img in zip(batch_messages, batch_images):
            prompt = processor.apply_chat_template(msgs, add_generation_prompt=True)
            inputs = processor(text=prompt, images=[img], return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            outputs = model.generate(**inputs, max_new_tokens=512)
            response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
            results.append(response)

    return results
```

#### 3. é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†

```python
# InternVLæ”¯æŒåŠ¨æ€é«˜åˆ†è¾¨ç‡
# è‡ªåŠ¨å¤„ç†,æ— éœ€é¢å¤–é…ç½®
high_res_image = Image.open("4k_image.jpg").convert('RGB')

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Describe the fine details in this high-resolution image."}
        ]
    }
]

# æ¨¡å‹ä¼šè‡ªåŠ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒ
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[high_res_image], return_tensors="pt")
inputs = {k: v.to(model.device) for k, v in inputs.items()}
outputs = model.generate(**inputs, max_new_tokens=1024)
response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
```

---

## ğŸ’¼ åº”ç”¨åœºæ™¯

### 1. æ™ºèƒ½æ–‡æ¡£å¤„ç†

**åœºæ™¯**: è‡ªåŠ¨æå–å’Œç†è§£æ–‡æ¡£å†…å®¹

```python
def intelligent_document_processor(doc_image_path):
    """æ™ºèƒ½æ–‡æ¡£å¤„ç†"""
    image = Image.open(doc_image_path).convert('RGB')

    # æå–æ–‡æ¡£å†…å®¹
    extract_prompt = """
    Please analyze this document and:
    1. Extract all text content
    2. Identify the document type
    3. Extract key information (names, dates, amounts, etc.)
    4. Summarize the main points
    """

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": extract_prompt}
            ]
        }
    ]

    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=1024)
    analysis = processor.batch_decode(outputs, skip_special_tokens=True)[0]

    return analysis

# ä½¿ç”¨ç¤ºä¾‹
result = intelligent_document_processor("invoice.jpg")
print(result)
```

### 2. ç”µå•†å›¾ç‰‡ç†è§£

**åœºæ™¯**: å•†å“å›¾ç‰‡è‡ªåŠ¨æ ‡æ³¨å’Œæè¿°

```python
def ecommerce_image_analyzer(product_image_path):
    """ç”µå•†å›¾ç‰‡åˆ†æ"""
    image = Image.open(product_image_path).convert('RGB')

    analysis_prompt = """
    Please analyze this product image and provide:
    1. Product category
    2. Main features and characteristics
    3. Color and style
    4. Suggested product title (SEO-friendly)
    5. Detailed product description
    """

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": analysis_prompt}
            ]
        }
    ]

    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=512)
    description = processor.batch_decode(outputs, skip_special_tokens=True)[0]

    return description
```

### 3. åŒ»ç–—å½±åƒè¾…åŠ©

**åœºæ™¯**: åŒ»ç–—å›¾åƒåˆæ­¥åˆ†æ(ä»…ä¾›å‚è€ƒ)

```python
def medical_image_assistant(image_path):
    """åŒ»ç–—å½±åƒè¾…åŠ©åˆ†æ

    æ³¨æ„: æ­¤åŠŸèƒ½ä»…ä¾›åŒ»å­¦ä¸“ä¸šäººå‘˜å‚è€ƒ,ä¸èƒ½æ›¿ä»£ä¸“ä¸šè¯Šæ–­
    """
    image = Image.open(image_path).convert('RGB')

    analysis_prompt = """
    Please analyze this medical image and describe:
    1. Image type and modality
    2. Visible anatomical structures
    3. Any notable observations
    4. Image quality assessment

    Note: This is for reference only and should not replace professional medical diagnosis.
    """

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": analysis_prompt}
            ]
        }
    ]

    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=512)
    analysis = processor.batch_decode(outputs, skip_special_tokens=True)[0]

    return analysis
```

### 4. æ•™è‚²è¾…åŠ©å·¥å…·

**åœºæ™¯**: è§£ç­”å›¾ç‰‡ä¸­çš„é¢˜ç›®

```python
def educational_assistant(problem_image_path):
    """æ•™è‚²è¾…åŠ©å·¥å…·"""
    image = Image.open(problem_image_path).convert('RGB')

    teaching_prompt = """
    Please help solve this problem:
    1. Identify the subject and problem type
    2. Explain the problem-solving approach
    3. Provide step-by-step solution
    4. Give the final answer
    5. Suggest related concepts to review
    """

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": teaching_prompt}
            ]
        }
    ]

    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=1024)
    solution = processor.batch_decode(outputs, skip_special_tokens=True)[0]

    return solution
```

---

## âš™ï¸ ä¼˜åŒ–æŠ€å·§

### 1. æ˜¾å­˜ä¼˜åŒ–

#### BFloat16ç²¾åº¦

```python
# ä½¿ç”¨BFloat16å¯ä»¥èŠ‚çœæ˜¾å­˜å¹¶åŠ é€Ÿæ¨ç†
model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    torch_dtype=torch.bfloat16,  # æ¨èä½¿ç”¨BFloat16
    device_map="auto",
    trust_remote_code=True
)
```

#### 8bité‡åŒ–

```python
# ä½¿ç”¨8bité‡åŒ–è¿›ä¸€æ­¥å‡å°‘æ˜¾å­˜å ç”¨
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)
```

### 2. æ¨ç†åŠ é€Ÿ

#### Flash Attention 2

```python
# å¯ç”¨Flash Attention 2åŠ é€Ÿattentionè®¡ç®—
# éœ€è¦: pip install flash-attn
model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
    attn_implementation="flash_attention_2"  # å¯ç”¨Flash Attention 2
)
```

#### ç¼–è¯‘ä¼˜åŒ–

```python
# ä½¿ç”¨torch.compileåŠ é€Ÿæ¨ç†(PyTorch 2.0+)
model = torch.compile(model, mode="reduce-overhead")
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# ä½¿ç”¨torch.cuda.ampè¿›è¡Œæ··åˆç²¾åº¦æ¨ç†
from torch.cuda.amp import autocast

@torch.no_grad()
def optimized_inference(image, question):
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": question}
            ]
        }
    ]

    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt").to(model.device)

    with autocast(dtype=torch.bfloat16):
        outputs = model.generate(**inputs, max_new_tokens=512)

    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return response
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹åŠ è½½å¤±è´¥

**é—®é¢˜**: `transformers`ç‰ˆæœ¬è¿‡ä½

**è§£å†³**:
```bash
# ç¡®ä¿transformersç‰ˆæœ¬>=4.37.2
pip install --upgrade transformers>=4.37.2
```

### Q2: æ˜¾å­˜ä¸è¶³(CUDA OOM)

**é—®é¢˜**: GPUæ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹(InternVL3-1B)
2. å¯ç”¨8bité‡åŒ–
3. å‡å°batch size
4. ä½¿ç”¨CPU offload

```python
# CPU offloadç¤ºä¾‹
model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    torch_dtype=torch.bfloat16,
    device_map="balanced",  # è‡ªåŠ¨åˆ†é…åˆ°GPUå’ŒCPU
    offload_folder="offload",
    trust_remote_code=True
)
```

### Q3: ç²¾åº¦é€‰æ‹©å’Œå…¼å®¹æ€§

**é—®é¢˜**: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç²¾åº¦?

**æœ€ä½³å®è·µ**:

1. **GPUæ¨èé…ç½®**:
   - Ampereæ¶æ„åŠä»¥ä¸Š(å¦‚A100/RTX 30ç³»åˆ—): ä½¿ç”¨BFloat16
   - å…¶ä»–GPU: ä½¿ç”¨Float16

2. **CPUé…ç½®**:
   - å¿…é¡»ä½¿ç”¨Float32(CPUä¸æ”¯æŒåŠç²¾åº¦)

3. **è‡ªåŠ¨ç²¾åº¦æ£€æµ‹**(æ¨è):
```python
# InternVLæ¨ç†ä»£ç å·²å†…ç½®ç²¾åº¦æ£€æµ‹
# CPUä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°Float32
# GPUä¼šæ ¹æ®ç¡¬ä»¶æ”¯æŒè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç²¾åº¦

# æ‰‹åŠ¨æŒ‡å®š(å¦‚æœ‰éœ€è¦)
model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    torch_dtype=torch.float16,  # GPUä½¿ç”¨Float16
    device_map="auto",
    trust_remote_code=True
)
```

**ç²¾åº¦å¯¹æ¯”**:
| ç²¾åº¦ | æ˜¾å­˜å ç”¨ | é€Ÿåº¦ | ç²¾ç¡®åº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|------|--------|---------|
| **Float32** | 100% | 1x | æœ€é«˜ | CPUæ¨ç† |
| **Float16** | 50% | 2-3x | é«˜ | å¤§éƒ¨åˆ†GPU |
| **BFloat16** | 50% | 2-3x | é«˜ | æ–°æ¶æ„GPU |

### Q4: æ¨ç†é€Ÿåº¦æ…¢

**ä¼˜åŒ–æ–¹æ³•**:
1. å¯ç”¨Flash Attention 2
2. ä½¿ç”¨BFloat16ç²¾åº¦
3. å‡å°max_new_tokens
4. ä½¿ç”¨torch.compile

```python
# ç»¼åˆåŠ é€Ÿé…ç½®
model = AutoModelForImageTextToText.from_pretrained(
    "OpenGVLab/InternVL2-8B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
    attn_implementation="flash_attention_2"
)
model = torch.compile(model, mode="reduce-overhead")

# ç”Ÿæˆæ—¶å‡å°max_new_tokens
outputs = model.generate(**inputs, max_new_tokens=256)
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹èµ„æº
- [GitHubä»“åº“](https://github.com/OpenGVLab/InternVL)
- [HuggingFaceæ¨¡å‹åº“](https://huggingface.co/OpenGVLab)
- [æŠ€æœ¯æŠ¥å‘Š(v1)](https://arxiv.org/abs/2312.14238)
- [æŠ€æœ¯æŠ¥å‘Š(v2)](https://arxiv.org/abs/2407.03320)
- [å®˜æ–¹æ–‡æ¡£](https://internvl.readthedocs.io/)

### ç›¸å…³æ•™ç¨‹
- [Transformerså®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/en/model_doc/internvl)
- [æ¨¡å‹å¾®è°ƒæŒ‡å—](../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/02-LoRAå¾®è°ƒ.md)
- [éƒ¨ç½²å®è·µ](../04-å¤šå¹³å°éƒ¨ç½²/01-NVIDIAéƒ¨ç½²åŸºç¡€.md)

### ç¤¾åŒºèµ„æº
- [ModelScopeæ¨¡å‹åº“](https://modelscope.cn/models?name=InternVL)
- [Papers with Code](https://paperswithcode.com/paper/internvl-scaling-up-vision-foundation-models)

---

## ğŸ¯ å®è·µä»»åŠ¡

1. **åŸºç¡€ä½¿ç”¨**
   - [ ] æˆåŠŸåŠ è½½InternVLæ¨¡å‹
   - [ ] å®Œæˆä¸€æ¬¡å›¾åƒæè¿°ç”Ÿæˆ
   - [ ] å®Œæˆä¸€æ¬¡è§†è§‰é—®ç­”

2. **è¿›é˜¶åŠŸèƒ½**
   - [ ] å®ç°å¤šè½®å¯¹è¯
   - [ ] å°è¯•å¤šå›¾ç†è§£
   - [ ] æµ‹è¯•OCRè¯†åˆ«èƒ½åŠ›

3. **æ€§èƒ½ä¼˜åŒ–**
   - [ ] å°è¯•ä¸åŒç²¾åº¦(BFloat16/Float16)
   - [ ] æµ‹è¯•æ‰¹é‡æ¨ç†
   - [ ] å¯¹æ¯”ä¸åŒæ¨¡å‹è§„æ¨¡çš„æ€§èƒ½

4. **åº”ç”¨å¼€å‘**
   - [ ] é€‰æ‹©ä¸€ä¸ªåº”ç”¨åœºæ™¯
   - [ ] å®ç°å®Œæ•´çš„è§£å†³æ–¹æ¡ˆ
   - [ ] ç¼–å†™ä½¿ç”¨æ–‡æ¡£

---

## âœ… å­¦ä¹ æˆæœéªŒæ”¶

å®Œæˆä»¥ä¸‹ä»»åŠ¡å³è¡¨ç¤ºæŒæ¡InternVLçš„ä½¿ç”¨:

- [ ] èƒ½å¤Ÿç‹¬ç«‹é…ç½®InternVLç¯å¢ƒ
- [ ] ç†è§£æ¨¡å‹çš„ä¸»è¦æ¶æ„å’ŒåŸç†
- [ ] ç†Ÿç»ƒä½¿ç”¨å„ç§æ¨ç†åŠŸèƒ½
- [ ] èƒ½å¤Ÿæ ¹æ®éœ€æ±‚ä¼˜åŒ–æ€§èƒ½
- [ ] å®Œæˆè‡³å°‘ä¸€ä¸ªå®é™…åº”ç”¨æ¡ˆä¾‹

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­å­¦ä¹ :
- [Qwen-VLæ¨¡å‹è¯¦è§£](./07-Qwen-VLæ¨¡å‹è¯¦è§£.md) - ä¸­æ–‡ä¼˜åŒ–çš„è§†è§‰æ¨¡å‹
- [æ¨¡å‹å¯¹æ¯”ä¸è¯„æµ‹](./02-æ¨¡å‹å¯¹æ¯”ä¸è¯„æµ‹.md) - å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ä¼˜åŠ£
- [æ¨¡å‹å¾®è°ƒæŠ€æœ¯](../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/02-LoRAå¾®è°ƒ.md) - å­¦ä¹ å¦‚ä½•å¾®è°ƒInternVL
- [å®é™…åº”ç”¨æ¡ˆä¾‹](../06-è¡Œä¸šåº”ç”¨/) - æŸ¥çœ‹æ›´å¤šåº”ç”¨ç¤ºä¾‹

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1.0
**æœ€åæ›´æ–°**: 2025-11-10
**è´¡çŒ®è€…**: Large-Model-Tutorial Team
