# BLIP-2æ¨¡å‹è¯¦è§£

> **BLIP-2 (Bootstrapping Language-Image Pre-training 2)**: Salesforceåœ¨2023å¹´æå‡ºçš„é«˜æ•ˆè§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡Q-Formeræ¶æ„å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ã€‚

---

## ğŸ“‹ ç›®å½•

1. [æ¨¡å‹æ¦‚è¿°](#1-æ¨¡å‹æ¦‚è¿°)
2. [æ ¸å¿ƒåˆ›æ–°](#2-æ ¸å¿ƒåˆ›æ–°)
3. [æ¶æ„è¯¦è§£](#3-æ¶æ„è¯¦è§£)
4. [Q-Formeræœºåˆ¶](#4-q-formeræœºåˆ¶)
5. [è®­ç»ƒç­–ç•¥](#5-è®­ç»ƒç­–ç•¥)
6. [ä½¿ç”¨æ–¹æ³•](#6-ä½¿ç”¨æ–¹æ³•)
7. [æ€§èƒ½åˆ†æ](#7-æ€§èƒ½åˆ†æ)
8. [åº”ç”¨åœºæ™¯](#8-åº”ç”¨åœºæ™¯)
9. [ä¼˜ç¼ºç‚¹åˆ†æ](#9-ä¼˜ç¼ºç‚¹åˆ†æ)
10. [å®è·µå»ºè®®](#10-å®è·µå»ºè®®)

---

## 1. æ¨¡å‹æ¦‚è¿°

### 1.1 åŸºæœ¬ä¿¡æ¯

| å±æ€§ | æè¿° |
|------|------|
| **å‘å¸ƒæ—¶é—´** | 2023å¹´1æœˆ |
| **å‘å¸ƒæœºæ„** | Salesforce Research |
| **è®ºæ–‡** | [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) |
| **å¼€æºåœ°å€** | [https://github.com/salesforce/LAVIS](https://github.com/salesforce/LAVIS) |
| **æ¨¡å‹ç±»å‹** | è§†è§‰-è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰ |
| **æ ¸å¿ƒæŠ€æœ¯** | Q-Former + å†»ç»“ç¼–ç å™¨ |

### 1.2 è®¾è®¡åŠ¨æœº

**ä¼ ç»ŸVLPæ¨¡å‹çš„ç—›ç‚¹**ï¼š
1. **ç«¯åˆ°ç«¯è®­ç»ƒæˆæœ¬é«˜**ï¼šéœ€è¦åŒæ—¶è®­ç»ƒå›¾åƒç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹
2. **ç¾éš¾æ€§é—å¿˜**ï¼šå¾®è°ƒæ—¶ä¼šæŸå¤±é¢„è®­ç»ƒçŸ¥è¯†
3. **è®¡ç®—èµ„æºæµªè´¹**ï¼šé‡æ–°è®­ç»ƒå·²æœ‰çš„å¼ºå¤§æ¨¡å‹

**BLIP-2çš„è§£å†³æ–¹æ¡ˆ**ï¼š
- å†»ç»“é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹
- ä»…è®­ç»ƒè½»é‡çº§çš„Q-Formeræ¡¥æ¥æ¨¡å—
- åˆ©ç”¨ç°æœ‰æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¤§å¹…é™ä½æˆæœ¬

### 1.3 å…³é”®ç‰¹æ€§

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BLIP-2 å…³é”®ç‰¹æ€§                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… å‚æ•°æ•ˆç‡é«˜ï¼ˆä»…è®­ç»ƒQ-Formerï¼‰         â”‚
â”‚ âœ… è®­ç»ƒæˆæœ¬ä½ï¼ˆç›¸æ¯”ç«¯åˆ°ç«¯é™ä½90%+ï¼‰     â”‚
â”‚ âœ… æ€§èƒ½å¼ºå¤§ï¼ˆå¤šé¡¹VLä»»åŠ¡SOTAï¼‰           â”‚
â”‚ âœ… çµæ´»ç»„åˆï¼ˆä»»æ„å›¾åƒ+è¯­è¨€æ¨¡å‹ï¼‰        â”‚
â”‚ âœ… ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆæ— ç¾éš¾æ€§é—å¿˜ï¼‰       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. æ ¸å¿ƒåˆ›æ–°

### 2.1 Q-Formeræ¶æ„

**Querying Transformer (Q-Former)** æ˜¯BLIP-2çš„æ ¸å¿ƒåˆ›æ–°ï¼š

```
è¾“å…¥å›¾åƒ
   â†“
[å†»ç»“çš„å›¾åƒç¼–ç å™¨] (å¦‚ViT)
   â†“
è§†è§‰ç‰¹å¾ (256ä¸ªpatches)
   â†“
[Q-Former] â† 32ä¸ªå¯å­¦ä¹ æŸ¥è¯¢å‘é‡ (Learnable Queries)
   â†“
   â”œâ”€ è‡ªæ³¨æ„åŠ›å±‚ (Queryé—´äº¤äº’)
   â”œâ”€ äº¤å‰æ³¨æ„åŠ›å±‚ (Queryä¸è§†è§‰ç‰¹å¾äº¤äº’)
   â””â”€ å‰é¦ˆç½‘ç»œ
   â†“
32ä¸ªè¾“å‡ºå‘é‡ (å›ºå®šé•¿åº¦çš„è§†è§‰æ‘˜è¦)
   â†“
[çº¿æ€§æŠ•å½±å±‚]
   â†“
[å†»ç»“çš„LLM] (å¦‚OPT/FlanT5)
   â†“
è¾“å‡ºæ–‡æœ¬
```

**Q-Formerçš„ä½œç”¨**ï¼š
1. **ä¿¡æ¯ç“¶é¢ˆ**ï¼šå°†256ä¸ªè§†è§‰ç‰¹å¾å‹ç¼©ä¸º32ä¸ªæŸ¥è¯¢å‘é‡
2. **è¯­ä¹‰æå–**ï¼šé€šè¿‡å­¦ä¹ æå–æœ€ç›¸å…³çš„è§†è§‰ä¿¡æ¯
3. **æ¨¡æ€å¯¹é½**ï¼šå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç©ºé—´

### 2.2 ä¸¤é˜¶æ®µè®­ç»ƒ

**é˜¶æ®µ1ï¼šè§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹  (Vision-Language Representation Learning)**

å†»ç»“å›¾åƒç¼–ç å™¨ï¼Œè®­ç»ƒQ-Formerï¼Œä½¿ç”¨ä¸‰ä¸ªç›®æ ‡ï¼š

1. **å›¾åƒ-æ–‡æœ¬å¯¹æ¯”å­¦ä¹  (ITC)**ï¼š
   ```python
   # å¯¹æ¯”Q-Formerè¾“å‡ºä¸æ–‡æœ¬è¡¨ç¤º
   loss_itc = contrastive_loss(query_output, text_embedding)
   ```

2. **å›¾åƒ-æ–‡æœ¬åŒ¹é… (ITM)**ï¼š
   ```python
   # äºŒåˆ†ç±»ï¼šå›¾åƒå’Œæ–‡æœ¬æ˜¯å¦åŒ¹é…
   loss_itm = binary_cross_entropy(match_score, label)
   ```

3. **å›¾åƒæ¡ä»¶çš„æ–‡æœ¬ç”Ÿæˆ (ITG)**ï¼š
   ```python
   # ç”Ÿæˆæè¿°å›¾åƒçš„æ–‡æœ¬
   loss_itg = language_modeling_loss(generated_text, ground_truth)
   ```

**é˜¶æ®µ2ï¼šè§†è§‰åˆ°è¯­è¨€ç”Ÿæˆå­¦ä¹  (Vision-to-Language Generative Learning)**

å†»ç»“å›¾åƒç¼–ç å™¨å’ŒLLMï¼Œä»…è®­ç»ƒQ-Formerå’Œçº¿æ€§æŠ•å½±å±‚ï¼š

```python
# ä½¿ç”¨LLMçš„è¯­è¨€å»ºæ¨¡æŸå¤±
loss = language_modeling_loss(llm_output, target_text)
```

### 2.3 ä¸BLIP-1å¯¹æ¯”

| ç‰¹æ€§ | BLIP-1 | BLIP-2 |
|------|--------|--------|
| **å›¾åƒç¼–ç å™¨** | ç«¯åˆ°ç«¯è®­ç»ƒ | **å†»ç»“** |
| **è¯­è¨€æ¨¡å‹** | ç«¯åˆ°ç«¯è®­ç»ƒ | **å†»ç»“** |
| **æ¡¥æ¥æ¨¡å—** | æ—  | **Q-Former** |
| **è®­ç»ƒå‚æ•°** | ~200M | **~180M (ä»…Q-Former)** |
| **è®­ç»ƒæˆæœ¬** | é«˜ | **ä½90%+** |
| **é›¶æ ·æœ¬æ€§èƒ½** | è‰¯å¥½ | **æ›´ä¼˜** |

---

## 3. æ¶æ„è¯¦è§£

### 3.1 å®Œæ•´æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BLIP-2 å®Œæ•´æ¶æ„                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥: å›¾åƒ + æ–‡æœ¬æç¤º

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Image Encoder â”‚  â† å†»ç»“ï¼ˆå¦‚ViT-L/14, ViT-g/14ï¼‰
â”‚   (Frozen)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ è¾“å‡º: [B, 256, D_v]
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Q-Former Module                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  32ä¸ªLearnable Queries [B, 32, D]â”‚â†â”€åˆå§‹åŒ–   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                 â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Self-Attention Layers       â”‚               â”‚
â”‚  â”‚  (Queryé—´äº¤äº’)                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                 â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Cross-Attention Layers      â”‚â†â”€è§†è§‰ç‰¹å¾    â”‚
â”‚  â”‚  (Queryä¸Imageäº¤äº’)          â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                 â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Feed-Forward Network        â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                 â”‚                                â”‚
â”‚  è¾“å‡º: [B, 32, D] (è§†è§‰æ‘˜è¦)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Linear Projection Layer       â”‚ â† å¯è®­ç»ƒ
â”‚   [B, 32, D] â†’ [B, 32, D_llm]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Large Language Model          â”‚ â† å†»ç»“ï¼ˆå¦‚OPT-2.7B, FlanT5-XXLï¼‰
â”‚   (Frozen)                      â”‚
â”‚                                 â”‚
â”‚   è¾“å…¥: è§†è§‰token + æ–‡æœ¬æç¤º    â”‚
â”‚   è¾“å‡º: ç”Ÿæˆçš„æ–‡æœ¬              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Q-Formerè¯¦ç»†ç»“æ„

```python
class QFormer(nn.Module):
    def __init__(self, 
                 num_queries=32,          # æŸ¥è¯¢å‘é‡æ•°é‡
                 hidden_dim=768,          # éšè—å±‚ç»´åº¦
                 num_layers=12,           # Transformerå±‚æ•°
                 num_heads=12):           # æ³¨æ„åŠ›å¤´æ•°
        super().__init__()
        
        # å¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡
        self.queries = nn.Parameter(torch.randn(num_queries, hidden_dim))
        
        # Transformerå±‚
        self.layers = nn.ModuleList([
            QFormerLayer(hidden_dim, num_heads)
            for _ in range(num_layers)
        ])
    
    def forward(self, image_features):
        """
        Args:
            image_features: [B, 256, D_v] å›¾åƒç‰¹å¾
        Returns:
            query_output: [B, 32, D] Q-Formerè¾“å‡º
        """
        B = image_features.size(0)
        
        # æ‰©å±•æŸ¥è¯¢å‘é‡åˆ°batch
        queries = self.queries.unsqueeze(0).expand(B, -1, -1)  # [B, 32, D]
        
        # é€šè¿‡Transformerå±‚
        for layer in self.layers:
            queries = layer(queries, image_features)
        
        return queries


class QFormerLayer(nn.Module):
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        
        # Self-Attentionï¼ˆQueryé—´äº¤äº’ï¼‰
        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads)
        self.norm1 = nn.LayerNorm(hidden_dim)
        
        # Cross-Attentionï¼ˆQueryä¸Imageäº¤äº’ï¼‰
        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads)
        self.norm2 = nn.LayerNorm(hidden_dim)
        
        # Feed-Forward
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        self.norm3 = nn.LayerNorm(hidden_dim)
    
    def forward(self, queries, image_features):
        """
        Args:
            queries: [B, 32, D]
            image_features: [B, 256, D_v]
        """
        # Self-Attention
        q = queries
        q2, _ = self.self_attn(q, q, q)
        q = self.norm1(q + q2)
        
        # Cross-Attention
        q2, _ = self.cross_attn(q, image_features, image_features)
        q = self.norm2(q + q2)
        
        # Feed-Forward
        q2 = self.ffn(q)
        q = self.norm3(q + q2)
        
        return q
```

### 3.3 æ¨¡å‹å˜ä½“

BLIP-2æä¾›å¤šç§é…ç½®ï¼š

| é…ç½® | å›¾åƒç¼–ç å™¨ | LLM | å‚æ•°é‡ | æ€§èƒ½ |
|------|------------|-----|--------|------|
| **BLIP2-OPT-2.7B** | ViT-L/14 | OPT-2.7B | ~3.0B | ä¼˜ç§€ |
| **BLIP2-OPT-6.7B** | ViT-L/14 | OPT-6.7B | ~7.0B | æ›´ä¼˜ |
| **BLIP2-FlanT5-XL** | ViT-g/14 | FlanT5-XL (3B) | ~3.4B | æœ€ä½³ |
| **BLIP2-FlanT5-XXL** | ViT-g/14 | FlanT5-XXL (11B) | ~11.4B | SOTA |

---

## 4. Q-Formeræœºåˆ¶

### 4.1 æŸ¥è¯¢å‘é‡çš„ä½œç”¨

**å¯å­¦ä¹ æŸ¥è¯¢ (Learnable Queries)** ç±»ä¼¼äº"é—®é¢˜"ï¼š

```python
# åˆå§‹åŒ–32ä¸ªæŸ¥è¯¢å‘é‡
queries = nn.Parameter(torch.randn(32, 768))

# æ¯ä¸ªæŸ¥è¯¢å­¦ä¼šæå–ç‰¹å®šç±»å‹çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š
# Query 1: æå–ä¸»è¦ç‰©ä½“ä¿¡æ¯
# Query 2: æå–é¢œè‰²å’Œçº¹ç†
# Query 3: æå–ç©ºé—´å…³ç³»
# ...
# Query 32: æå–åœºæ™¯ä¸Šä¸‹æ–‡
```

### 4.2 ä¿¡æ¯ç“¶é¢ˆ

**ä¸ºä»€ä¹ˆæ˜¯32ä¸ªæŸ¥è¯¢ï¼Ÿ**

```
å›¾åƒç‰¹å¾: 256ä¸ªpatches Ã— 768ç»´ = 196,608ç»´ä¿¡æ¯
    â†“ (ä¿¡æ¯å‹ç¼©)
Q-Former: 32ä¸ªqueries Ã— 768ç»´ = 24,576ç»´ä¿¡æ¯
    â†“ (çº¦12.5%çš„ä¿¡æ¯é‡)
```

**å¥½å¤„**ï¼š
1. **è®¡ç®—æ•ˆç‡**ï¼šå¤§å¹…å‡å°‘LLMçš„è¾“å…¥é•¿åº¦
2. **ä¿¡æ¯èšç„¦**ï¼šå¼ºåˆ¶æå–æœ€ç›¸å…³çš„ä¿¡æ¯
3. **çµæ´»æ€§**ï¼š32ä¸ªtokené€‚åˆå¤§å¤šæ•°LLM

### 4.3 æ³¨æ„åŠ›æœºåˆ¶

**Self-Attentionï¼ˆQueryé—´äº¤äº’ï¼‰**ï¼š
```python
# Queryä¹‹é—´äº’ç›¸å…³æ³¨ï¼Œå½¢æˆå…¨å±€è§†è§’
# ä¾‹å¦‚ï¼šè¯†åˆ«"çº¢è‰²çš„è½¦"éœ€è¦ç»“åˆé¢œè‰²å’Œç‰©ä½“æŸ¥è¯¢
Q_self = SelfAttention(Q, Q, Q)
```

**Cross-Attentionï¼ˆQueryä¸Imageäº¤äº’ï¼‰**ï¼š
```python
# Queryä»å›¾åƒç‰¹å¾ä¸­æå–ä¿¡æ¯
# Queryä½œä¸º"é—®é¢˜"ï¼ŒImage Featuresä½œä¸º"ç­”æ¡ˆæº"
Q_cross = CrossAttention(Q, Image_Features, Image_Features)
```

### 4.4 ä¸Perceiverå’ŒDETRçš„å…³ç³»

Q-Formerå€Ÿé‰´äº†ï¼š

| æ¨¡å‹ | æ ¸å¿ƒæ€æƒ³ | BLIP-2çš„åº”ç”¨ |
|------|----------|--------------|
| **Perceiver** | ä½¿ç”¨latent querieså‹ç¼©è¾“å…¥ | Q-Formerçš„æŸ¥è¯¢æœºåˆ¶ |
| **DETR** | Object Querieså­¦ä¹ æ£€æµ‹ç‰©ä½“ | Learnable Queries |
| **Flamingo** | Gated cross-attention | è§†è§‰-è¯­è¨€èåˆ |

---

## 5. è®­ç»ƒç­–ç•¥

### 5.1 é˜¶æ®µ1ï¼šè§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ 

**ç›®æ ‡**ï¼šè®©Q-Formerå­¦ä¼šä»å›¾åƒä¸­æå–ä¸è¯­è¨€ç›¸å…³çš„ä¿¡æ¯

**è®­ç»ƒæ•°æ®**ï¼šå›¾åƒ-æ–‡æœ¬å¯¹ï¼ˆå¦‚COCO, VG, CC3Mç­‰ï¼‰

**ä¸‰ä¸ªæŸå¤±å‡½æ•°**ï¼š

#### 5.1.1 å›¾åƒ-æ–‡æœ¬å¯¹æ¯”å­¦ä¹  (ITC)

```python
def image_text_contrastive_loss(query_output, text_embedding, temperature=0.07):
    """
    Args:
        query_output: [B, 32, D] Q-Formerè¾“å‡º
        text_embedding: [B, D] æ–‡æœ¬åµŒå…¥
    """
    # æ± åŒ–Q-Formerè¾“å‡º
    image_embed = query_output.mean(dim=1)  # [B, D]
    
    # å½’ä¸€åŒ–
    image_embed = F.normalize(image_embed, dim=-1)
    text_embed = F.normalize(text_embedding, dim=-1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = torch.matmul(image_embed, text_embed.T) / temperature  # [B, B]
    
    # å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬ï¼‰
    labels = torch.arange(B).to(device)
    loss_i2t = F.cross_entropy(sim_matrix, labels)
    loss_t2i = F.cross_entropy(sim_matrix.T, labels)
    
    return (loss_i2t + loss_t2i) / 2
```

#### 5.1.2 å›¾åƒ-æ–‡æœ¬åŒ¹é… (ITM)

```python
def image_text_matching_loss(query_output, text_embedding, is_match):
    """
    Args:
        query_output: [B, 32, D]
        text_embedding: [B, D]
        is_match: [B] 0æˆ–1ï¼Œè¡¨ç¤ºæ˜¯å¦åŒ¹é…
    """
    # æ‹¼æ¥è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾
    combined = torch.cat([query_output, text_embedding.unsqueeze(1)], dim=1)
    
    # é€šè¿‡åˆ†ç±»å¤´é¢„æµ‹åŒ¹é…æ¦‚ç‡
    match_score = classifier(combined)  # [B, 2]
    
    # äºŒåˆ†ç±»æŸå¤±
    loss = F.cross_entropy(match_score, is_match)
    return loss
```

#### 5.1.3 å›¾åƒæ¡ä»¶çš„æ–‡æœ¬ç”Ÿæˆ (ITG)

```python
def image_grounded_text_generation_loss(query_output, caption):
    """
    Args:
        query_output: [B, 32, D] ä½œä¸ºdecoderçš„prefix
        caption: [B, L] ç›®æ ‡æ–‡æœ¬
    """
    # ä½¿ç”¨Q-Formerçš„decoderæ¨¡å¼ç”Ÿæˆæ–‡æœ¬
    logits = qformer_decoder(query_output, caption[:, :-1])
    
    # è¯­è¨€å»ºæ¨¡æŸå¤±
    loss = F.cross_entropy(
        logits.reshape(-1, vocab_size),
        caption[:, 1:].reshape(-1)
    )
    return loss
```

**æ€»æŸå¤±**ï¼š
```python
loss_stage1 = loss_itc + loss_itm + loss_itg
```

### 5.2 é˜¶æ®µ2ï¼šè§†è§‰åˆ°è¯­è¨€ç”Ÿæˆå­¦ä¹ 

**ç›®æ ‡**ï¼šè®©Q-Formerçš„è¾“å‡ºèƒ½è¢«LLMç†è§£å¹¶ç”Ÿæˆæ­£ç¡®çš„å“åº”

**è®­ç»ƒæ•°æ®**ï¼šå›¾åƒ-æ–‡æœ¬å¯¹ + æŒ‡ä»¤æ•°æ®ï¼ˆå¦‚LLaVA-Instructï¼‰

**è®­ç»ƒæµç¨‹**ï¼š

```python
def stage2_training_step(image, prompt, target_text):
    """
    Args:
        image: è¾“å…¥å›¾åƒ
        prompt: æ–‡æœ¬æç¤ºï¼ˆå¦‚"Describe this image:"ï¼‰
        target_text: æœŸæœ›çš„è¾“å‡ºæ–‡æœ¬
    """
    # 1. æå–å›¾åƒç‰¹å¾ï¼ˆå†»ç»“ï¼‰
    with torch.no_grad():
        image_features = image_encoder(image)  # [B, 256, D_v]
    
    # 2. Q-Formerå¤„ç†ï¼ˆå¯è®­ç»ƒï¼‰
    query_output = qformer(image_features)  # [B, 32, D]
    
    # 3. çº¿æ€§æŠ•å½±åˆ°LLMç©ºé—´ï¼ˆå¯è®­ç»ƒï¼‰
    visual_tokens = projection(query_output)  # [B, 32, D_llm]
    
    # 4. æ‹¼æ¥è§†è§‰tokenå’Œæ–‡æœ¬prompt
    prompt_tokens = llm_tokenizer(prompt)  # [B, L_p]
    input_embeds = torch.cat([
        visual_tokens,              # è§†è§‰å‰ç¼€
        llm_embed(prompt_tokens)    # æ–‡æœ¬æç¤º
    ], dim=1)  # [B, 32+L_p, D_llm]
    
    # 5. LLMç”Ÿæˆï¼ˆå†»ç»“ï¼‰
    with torch.no_grad():
        logits = llm(inputs_embeds=input_embeds, ...)
    
    # 6. è®¡ç®—æŸå¤±ï¼ˆä»…å¯¹target_textéƒ¨åˆ†ï¼‰
    target_tokens = llm_tokenizer(target_text)
    loss = F.cross_entropy(logits[..., -(len(target_tokens)):, :], target_tokens)
    
    # 7. åå‘ä¼ æ’­ï¼ˆä»…æ›´æ–°Q-Formerå’ŒProjectionï¼‰
    loss.backward()
    optimizer.step()  # åªæ›´æ–°Q-Formerå’ŒProjectionçš„å‚æ•°
    
    return loss
```

### 5.3 è®­ç»ƒé…ç½®

**é˜¶æ®µ1é…ç½®**ï¼š
```yaml
# è§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ 
batch_size: 512
learning_rate: 1e-4
optimizer: AdamW
warmup_steps: 5000
max_epochs: 10
dataset: COCO + VG + CC3M + SBU (çº¦4Må›¾åƒ)
```

**é˜¶æ®µ2é…ç½®**ï¼š
```yaml
# è§†è§‰åˆ°è¯­è¨€ç”Ÿæˆå­¦ä¹ 
batch_size: 256
learning_rate: 5e-5
optimizer: AdamW
warmup_steps: 2000
max_epochs: 5
dataset: COCO + VG + CC12M (çº¦14Må›¾åƒ)
```

---

## 6. ä½¿ç”¨æ–¹æ³•

### 6.1 åŸºç¡€æ¨ç†

```python
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import torch

# 1. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# 2. å‡†å¤‡è¾“å…¥
image = Image.open("example.jpg").convert("RGB")
prompt = "Question: What is in this image? Answer:"

# 3. å¤„ç†è¾“å…¥
inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)

# 4. ç”Ÿæˆè¾“å‡º
generated_ids = model.generate(**inputs, max_new_tokens=50)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

print(generated_text)
```

### 6.2 å›¾åƒæè¿° (Image Captioning)

```python
def generate_caption(image_path):
    """ç”Ÿæˆå›¾åƒæè¿°"""
    image = Image.open(image_path).convert("RGB")
    
    # æ–¹å¼1ï¼šæ— æç¤ºï¼ˆè‡ªåŠ¨æè¿°ï¼‰
    inputs = processor(images=image, return_tensors="pt").to(device)
    generated_ids = model.generate(**inputs, max_new_tokens=50)
    caption = processor.decode(generated_ids[0], skip_special_tokens=True)
    
    return caption

# ç¤ºä¾‹
caption = generate_caption("cat.jpg")
# è¾“å‡º: "a cat sitting on a couch"
```

### 6.3 è§†è§‰é—®ç­” (Visual Question Answering)

```python
def visual_question_answering(image_path, question):
    """å›ç­”å…³äºå›¾åƒçš„é—®é¢˜"""
    image = Image.open(image_path).convert("RGB")
    
    # æ„å»ºæç¤º
    prompt = f"Question: {question} Answer:"
    
    inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)
    generated_ids = model.generate(**inputs, max_new_tokens=20)
    answer = processor.decode(generated_ids[0], skip_special_tokens=True)
    
    # ç§»é™¤promptéƒ¨åˆ†
    answer = answer.replace(prompt, "").strip()
    
    return answer

# ç¤ºä¾‹
answer = visual_question_answering("beach.jpg", "What is the weather like?")
# è¾“å‡º: "sunny"
```

### 6.4 å¤šè½®å¯¹è¯

```python
def multi_turn_conversation(image_path, questions):
    """å¤šè½®å¯¹è¯"""
    image = Image.open(image_path).convert("RGB")
    conversation_history = []
    
    for question in questions:
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n".join([f"Q: {q}\nA: {a}" for q, a in conversation_history])
        prompt = f"{context}\nQuestion: {question} Answer:"
        
        inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)
        generated_ids = model.generate(**inputs, max_new_tokens=30)
        answer = processor.decode(generated_ids[0], skip_special_tokens=True)
        answer = answer.split("Answer:")[-1].strip()
        
        conversation_history.append((question, answer))
    
    return conversation_history

# ç¤ºä¾‹
questions = [
    "What is the main object?",
    "What color is it?",
    "Where is it located?"
]
conversation = multi_turn_conversation("image.jpg", questions)
```

### 6.5 æ‰¹é‡å¤„ç†

```python
def batch_inference(image_paths, prompts, batch_size=4):
    """æ‰¹é‡æ¨ç†"""
    results = []
    
    for i in range(0, len(image_paths), batch_size):
        batch_images = [Image.open(p).convert("RGB") for p in image_paths[i:i+batch_size]]
        batch_prompts = prompts[i:i+batch_size] if isinstance(prompts, list) else [prompts] * len(batch_images)
        
        inputs = processor(images=batch_images, text=batch_prompts, return_tensors="pt", padding=True).to(device)
        
        generated_ids = model.generate(**inputs, max_new_tokens=50)
        texts = processor.batch_decode(generated_ids, skip_special_tokens=True)
        
        results.extend(texts)
    
    return results
```

---

## 7. æ€§èƒ½åˆ†æ

### 7.1 ä¸»è¦åŸºå‡†æµ‹è¯•ç»“æœ

#### VQAv2ï¼ˆè§†è§‰é—®ç­”ï¼‰

| æ¨¡å‹ | Test-Dev Accuracy | å‚æ•°é‡ |
|------|-------------------|--------|
| Flamingo-80B | 56.3 | 80B |
| PaLI-17B | 84.3 | 17B |
| **BLIP2-FlanT5-XXL** | **85.3** | 11.4B |
| **BLIP2-OPT-6.7B** | 78.9 | 7B |

#### COCO Captioningï¼ˆå›¾åƒæè¿°ï¼‰

| æ¨¡å‹ | CIDEr | BLEU-4 |
|------|-------|--------|
| BLIP | 136.7 | 40.4 |
| **BLIP2-FlanT5-XXL** | **144.5** | **42.5** |
| **BLIP2-OPT-6.7B** | 138.2 | 41.0 |

#### é›¶æ ·æœ¬å›¾åƒ-æ–‡æœ¬æ£€ç´¢

**COCO (5K test set)**:

| æ¨¡å‹ | Imageâ†’Text R@1 | Textâ†’Image R@1 |
|------|----------------|----------------|
| CLIP-ViT-L/14 | 58.4 | 37.8 |
| BLIP | 65.1 | 46.8 |
| **BLIP2-ViT-g** | **74.9** | **56.7** |

### 7.2 è®­ç»ƒæ•ˆç‡å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          è®­ç»ƒæˆæœ¬å¯¹æ¯”ï¼ˆç›¸åŒæ•°æ®é‡ï¼‰              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹          â”‚ è®­ç»ƒå‚æ•° â”‚ GPUå°æ—¶ â”‚ ç›¸å¯¹æˆæœ¬  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BLIP (ç«¯åˆ°ç«¯) â”‚ 223M     â”‚ ~100K   â”‚ 100%      â”‚
â”‚ Flamingo      â”‚ 80B      â”‚ ~500K   â”‚ 500%      â”‚
â”‚ BLIP2-Stage1  â”‚ 188M     â”‚ ~10K    â”‚ 10%       â”‚
â”‚ BLIP2-Stage2  â”‚ 188M     â”‚ ~5K     â”‚ 5%        â”‚
â”‚ **BLIP2æ€»è®¡** â”‚ **188M** â”‚ **~15K**â”‚ **15%**   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 æ¨ç†é€Ÿåº¦

**æµ‹è¯•ç¯å¢ƒ**ï¼šå•å¼ A100 GPU

| æ¨¡å‹é…ç½® | å›¾åƒå°ºå¯¸ | æ‰¹å¤§å° | ååé‡ (img/s) | å»¶è¿Ÿ (ms) |
|----------|----------|--------|----------------|-----------|
| BLIP2-OPT-2.7B | 224Ã—224 | 1 | 8.2 | 122 |
| BLIP2-OPT-2.7B | 224Ã—224 | 8 | 45.3 | 177 |
| BLIP2-FlanT5-XL | 224Ã—224 | 1 | 6.5 | 154 |
| BLIP2-FlanT5-XXL | 224Ã—224 | 1 | 2.8 | 357 |

### 7.4 å†…å­˜å ç”¨

| æ¨¡å‹ | æ¨¡å‹å¤§å° | æ¨ç†æ˜¾å­˜ (FP32) | æ¨ç†æ˜¾å­˜ (FP16) |
|------|----------|-----------------|-----------------|
| BLIP2-OPT-2.7B | ~5.5GB | ~12GB | ~8GB |
| BLIP2-OPT-6.7B | ~13GB | ~26GB | ~15GB |
| BLIP2-FlanT5-XL | ~6.8GB | ~14GB | ~9GB |
| BLIP2-FlanT5-XXL | ~22GB | ~45GB | ~24GB |

---

## 8. åº”ç”¨åœºæ™¯

### 8.1 å›¾åƒæè¿°ç”Ÿæˆ

**åœºæ™¯**ï¼šä¸ºç¤¾äº¤åª’ä½“ã€ç”µå•†ã€æ— éšœç¢æœåŠ¡ç”Ÿæˆå›¾åƒæè¿°

```python
# ç”µå•†äº§å“æè¿°
caption = generate_caption("product.jpg")
# "a red leather handbag with gold hardware"

# ç¤¾äº¤åª’ä½“è‡ªåŠ¨æ ‡é¢˜
caption = generate_caption("vacation.jpg")
# "people enjoying a sunny day at the beach"
```

### 8.2 è§†è§‰é—®ç­”ç³»ç»Ÿ

**åœºæ™¯**ï¼šå®¢æœæœºå™¨äººã€æ•™è‚²è¾…åŠ©ã€åŒ»ç–—å½±åƒåˆ†æ

```python
# åŒ»ç–—å½±åƒè¾…åŠ©
answer = vqa("xray.jpg", "Is there any abnormality?")

# æ•™è‚²åº”ç”¨
answer = vqa("math_diagram.jpg", "What geometric shape is this?")

# æ™ºèƒ½å®¶å±…
answer = vqa("fridge_interior.jpg", "What food items are running low?")
```

### 8.3 å¤šæ¨¡æ€å†…å®¹æ£€ç´¢

**åœºæ™¯**ï¼šå›¾åƒæœç´¢ã€è§†é¢‘åˆ†æã€å†…å®¹å®¡æ ¸

```python
# å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
def image_text_similarity(image_path, text):
    image = Image.open(image_path)
    inputs = processor(images=image, text=text, return_tensors="pt").to(device)
    
    # æå–ç‰¹å¾
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        image_embed = outputs.vision_model_output.last_hidden_state.mean(1)
        text_embed = outputs.language_model_output.last_hidden_state.mean(1)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = F.cosine_similarity(image_embed, text_embed)
    return similarity.item()
```

### 8.4 å›¾åƒç†è§£å¢å¼º

**åœºæ™¯**ï¼šè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ã€æ™ºèƒ½ç›‘æ§

```python
# åœºæ™¯ç†è§£
scene_description = vqa("street_view.jpg", "Describe the traffic situation")

# ç‰©ä½“è®¡æ•°
count = vqa("crowd.jpg", "How many people are in the image?")

# å…³ç³»ç†è§£
relation = vqa("family_photo.jpg", "What is the relationship between the people?")
```

### 8.5 è¾…åŠ©åˆ›ä½œ

**åœºæ™¯**ï¼šå†…å®¹åˆ›ä½œã€è‰ºæœ¯è®¾è®¡ã€æ•…äº‹ç”Ÿæˆ

```python
# åˆ›æ„æè¿°
creative_caption = vqa("artwork.jpg", "Describe this image in a poetic way")

# æ•…äº‹ç”Ÿæˆ
story = vqa("scene.jpg", "Create a short story based on this image")
```

---

## 9. ä¼˜ç¼ºç‚¹åˆ†æ

### 9.1 ä¼˜åŠ¿

#### âœ… 1. å‚æ•°æ•ˆç‡æé«˜

```
ä¼ ç»ŸVLPæ¨¡å‹è®­ç»ƒå‚æ•°: 200M+
BLIP-2è®­ç»ƒå‚æ•°: ~180M (ä»…Q-Former)
æ•ˆç‡æå‡: >10å€
```

#### âœ… 2. è®­ç»ƒæˆæœ¬ä½

- ä»…éœ€è®­ç»ƒè½»é‡çº§Q-Former
- å†»ç»“å›¾åƒç¼–ç å™¨å’ŒLLMï¼ŒèŠ‚çœ90%+è®¡ç®—
- æ›´å¿«çš„è¿­ä»£å’Œå®éªŒ

#### âœ… 3. çµæ´»çš„æ¨¡å‹ç»„åˆ

```python
# å¯ä»¥ä»»æ„ç»„åˆï¼š
BLIP2 = ä»»æ„å›¾åƒç¼–ç å™¨ + Q-Former + ä»»æ„LLM

# ä¾‹å¦‚ï¼š
- ViT-L/14 + Q-Former + OPT-2.7B
- ViT-g/14 + Q-Former + FlanT5-XXL
- EVA-CLIP + Q-Former + LLaMA-7B (ç¤¾åŒºå®ç°)
```

#### âœ… 4. ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†

- å†»ç»“çš„LLMä¿ç•™è¯­è¨€èƒ½åŠ›
- å†»ç»“çš„å›¾åƒç¼–ç å™¨ä¿ç•™è§†è§‰èƒ½åŠ›
- æ— ç¾éš¾æ€§é—å¿˜

#### âœ… 5. ä¼˜ç§€çš„é›¶æ ·æœ¬æ€§èƒ½

- å¤šé¡¹VLä»»åŠ¡SOTA
- æ³›åŒ–èƒ½åŠ›å¼º
- é€‚åº”æ–°ä»»åŠ¡å¿«

### 9.2 åŠ£åŠ¿

#### âŒ 1. å›ºå®šçš„æŸ¥è¯¢æ•°é‡

```python
# 32ä¸ªæŸ¥è¯¢å‘é‡å¯èƒ½ä¸å¤Ÿè¡¨è¾¾å¤æ‚åœºæ™¯
queries = 32  # å›ºå®šçš„ä¿¡æ¯ç“¶é¢ˆ
```

**å½±å“**ï¼š
- ç»†ç²’åº¦ä¿¡æ¯å¯èƒ½ä¸¢å¤±
- å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰æ•ˆæœå—é™

#### âŒ 2. ä¸¤é˜¶æ®µè®­ç»ƒå¤æ‚åº¦

- éœ€è¦åˆ†åˆ«è®­ç»ƒä¸¤ä¸ªé˜¶æ®µ
- è¶…å‚æ•°è°ƒä¼˜å¤æ‚
- é˜¶æ®µé—´çš„è¡”æ¥éœ€è¦ä»”ç»†è®¾è®¡

#### âŒ 3. æ¨ç†å»¶è¿Ÿ

```
BLIP2æ¨ç†æµç¨‹:
å›¾åƒç¼–ç  (ViT) â†’ Q-Former â†’ LLMç”Ÿæˆ
   ~50ms          ~20ms      ~100ms+

æ€»å»¶è¿Ÿ: ~170ms+ (batch=1, OPT-2.7B)
```

**ä¸é€‚åˆ**ï¼šå®æ—¶åº”ç”¨ï¼ˆå¦‚è§†é¢‘æµåˆ†æï¼‰

#### âŒ 4. å†…å­˜å ç”¨å¤§

- éœ€è¦åŒæ—¶åŠ è½½å›¾åƒç¼–ç å™¨ã€Q-Formerã€LLM
- FlanT5-XXLç‰ˆæœ¬éœ€è¦24GB+æ˜¾å­˜ï¼ˆFP16ï¼‰
- é™åˆ¶äº†éƒ¨ç½²åœºæ™¯

#### âŒ 5. ä¾èµ–é¢„è®­ç»ƒLLMè´¨é‡

- LLMçš„åè§ä¼šä¼ é€’åˆ°BLIP-2
- LLMçš„å±€é™æ€§å½±å“æ•´ä½“æ€§èƒ½
- éš¾ä»¥ä¿®å¤LLMä¸­çš„é—®é¢˜

### 9.3 ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”

| ç‰¹æ€§ | BLIP-2 | CLIP | LLaVA | Flamingo |
|------|--------|------|-------|----------|
| **è®­ç»ƒæˆæœ¬** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­ |
| **é›¶æ ·æœ¬VQA** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **å›¾åƒæè¿°** | â­â­â­â­â­ | âŒ | â­â­â­â­ | â­â­â­â­ |
| **æ¨ç†é€Ÿåº¦** | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­ |
| **éƒ¨ç½²éš¾åº¦** | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­ |
| **çµæ´»æ€§** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ |

---

## 10. å®è·µå»ºè®®

### 10.1 æ¨¡å‹é€‰æ‹©

```python
# æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„é…ç½®

# åœºæ™¯1ï¼šèµ„æºå—é™ï¼ˆå¦‚è¾¹ç¼˜è®¾å¤‡ã€ä¸ªäººGPUï¼‰
model = "Salesforce/blip2-opt-2.7b"  # æ¨è
# - æ˜¾å­˜éœ€æ±‚: ~8GB (FP16)
# - æ€§èƒ½: è‰¯å¥½çš„é›¶æ ·æœ¬èƒ½åŠ›

# åœºæ™¯2ï¼šé«˜æ€§èƒ½éœ€æ±‚ï¼ˆå¦‚äº‘æœåŠ¡ã€ç ”ç©¶ï¼‰
model = "Salesforce/blip2-flan-t5-xxl"  # æ¨è
# - æ˜¾å­˜éœ€æ±‚: ~24GB (FP16)
# - æ€§èƒ½: SOTA

# åœºæ™¯3ï¼šå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
model = "Salesforce/blip2-flan-t5-xl"  # æ¨è
# - æ˜¾å­˜éœ€æ±‚: ~9GB (FP16)
# - æ€§èƒ½: æ¥è¿‘SOTA
```

### 10.2 æ¨ç†ä¼˜åŒ–

#### ä½¿ç”¨åŠç²¾åº¦

```python
from transformers import Blip2ForConditionalGeneration
import torch

model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    torch_dtype=torch.float16  # ä½¿ç”¨FP16
)
model.to("cuda")
```

#### æ‰¹é‡æ¨ç†

```python
# æ‰¹é‡å¤„ç†ä»¥æé«˜ååé‡
batch_size = 8  # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´
images = [Image.open(f"img_{i}.jpg") for i in range(batch_size)]
inputs = processor(images=images, return_tensors="pt", padding=True).to(device)
outputs = model.generate(**inputs)
```

#### ç¼“å­˜KV

```python
# å¯¹äºå¤šè½®å¯¹è¯ï¼Œä½¿ç”¨past_key_valuesç¼“å­˜
generated_ids = model.generate(
    **inputs,
    use_cache=True,  # å¯ç”¨KVç¼“å­˜
    max_new_tokens=50
)
```

### 10.3 æç¤ºå·¥ç¨‹

#### æœ‰æ•ˆçš„æç¤ºæ¨¡æ¿

```python
# VQAæç¤º
prompt = "Question: {question} Answer:"

# å›¾åƒæè¿°æç¤º
prompt = "A photo of"  # ç®€æ´æç¤º
prompt = "Describe this image in detail:"  # è¯¦ç»†æç¤º

# å¤šé€‰é¢˜æç¤º
prompt = "Question: {question} Options: A) {opt_a} B) {opt_b} C) {opt_c} Answer:"

# è®¡æ•°æç¤º
prompt = "Question: How many {object} are in the image? Answer:"
```

#### æç¤ºæŠ€å·§

1. **ç®€æ´æ˜äº†**ï¼šé¿å…å†—ä½™è¯æ±‡
2. **æ˜ç¡®ä»»åŠ¡**ï¼šæ¸…æ¥šæŒ‡å®šæœŸæœ›çš„è¾“å‡º
3. **ä¸€è‡´æ ¼å¼**ï¼šä¿æŒæç¤ºæ ¼å¼ç»Ÿä¸€
4. **Few-Shot**ï¼šåœ¨æç¤ºä¸­æä¾›ç¤ºä¾‹ï¼ˆå¦‚æœLLMæ”¯æŒï¼‰

### 10.4 å¾®è°ƒå»ºè®®

#### å‚æ•°é«˜æ•ˆå¾®è°ƒ

```python
from peft import LoraConfig, get_peft_model

# å¯¹Q-Formeråº”ç”¨LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["query", "key", "value"],
    lora_dropout=0.1,
)

model.qformer = get_peft_model(model.qformer, lora_config)
```

#### æ•°æ®å‡†å¤‡

```python
# å¾®è°ƒæ•°æ®æ ¼å¼
{
    "image": "path/to/image.jpg",
    "conversations": [
        {"from": "human", "value": "Question about the image?"},
        {"from": "gpt", "value": "Answer to the question."}
    ]
}
```

### 10.5 å¸¸è§é—®é¢˜

#### Q1: å¦‚ä½•å¤„ç†å¤šä¸ªå›¾åƒï¼Ÿ

```python
# BLIP-2é»˜è®¤å¤„ç†å•å¼ å›¾åƒ
# å¤šå›¾åƒéœ€è¦åˆ†åˆ«å¤„ç†ååˆå¹¶ä¿¡æ¯
results = [vqa(img, question) for img in image_list]
```

#### Q2: å¦‚ä½•æé«˜ç”Ÿæˆè´¨é‡ï¼Ÿ

```python
# è°ƒæ•´ç”Ÿæˆå‚æ•°
output = model.generate(
    **inputs,
    max_new_tokens=100,
    num_beams=5,          # æŸæœç´¢
    temperature=0.7,      # æ§åˆ¶éšæœºæ€§
    top_p=0.9,            # æ ¸é‡‡æ ·
    repetition_penalty=1.2  # é¿å…é‡å¤
)
```

#### Q3: å¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Ÿ

```python
from datasets import load_dataset
from evaluate import load

# åŠ è½½è¯„ä¼°æŒ‡æ ‡
cider = load("cider")
bleu = load("bleu")

# åœ¨COCOä¸Šè¯„ä¼°
coco_dataset = load_dataset("coco_captions")
predictions = [generate_caption(img) for img in coco_dataset["test"]]
references = [img["captions"] for img in coco_dataset["test"]]

cider_score = cider.compute(predictions=predictions, references=references)
bleu_score = bleu.compute(predictions=predictions, references=references)
```

---

## æ€»ç»“

BLIP-2é€šè¿‡åˆ›æ–°çš„Q-Formeræ¶æ„å’Œå†»ç»“ç¼–ç å™¨ç­–ç•¥ï¼Œå®ç°äº†ï¼š

1. **è¶…é«˜å‚æ•°æ•ˆç‡**ï¼šä»…è®­ç»ƒ188Må‚æ•°
2. **è¶…ä½è®­ç»ƒæˆæœ¬**ï¼šç›¸æ¯”ç«¯åˆ°ç«¯é™ä½90%+
3. **SOTAæ€§èƒ½**ï¼šå¤šé¡¹VLä»»åŠ¡é¢†å…ˆ
4. **çµæ´»ç»„åˆ**ï¼šé€‚åº”ä¸åŒçš„ç¼–ç å™¨å’ŒLLM

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… å›¾åƒæè¿°ã€VQAã€å¤šæ¨¡æ€æ£€ç´¢
- âœ… éœ€è¦é«˜è´¨é‡é›¶æ ·æœ¬æ€§èƒ½
- âœ… èµ„æºå—é™ä½†è¿½æ±‚æ€§èƒ½

**ä¸é€‚ç”¨åœºæ™¯**ï¼š
- âŒ å®æ—¶åº”ç”¨ï¼ˆæ¨ç†å»¶è¿Ÿè¾ƒé«˜ï¼‰
- âŒ å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆä¿¡æ¯ç“¶é¢ˆï¼‰
- âŒ æåº¦èµ„æºå—é™ï¼ˆéœ€è¦åŠ è½½å¤§æ¨¡å‹ï¼‰

---

## å‚è€ƒèµ„æ–™

- **è®ºæ–‡**: [BLIP-2: Bootstrapping Language-Image Pre-training](https://arxiv.org/abs/2301.12597)
- **ä»£ç **: [GitHub - LAVIS](https://github.com/salesforce/LAVIS)
- **æ¨¡å‹**: [Hugging Face - BLIP-2](https://huggingface.co/models?search=blip2)
- **åšå®¢**: [Salesforce Research Blog](https://blog.salesforceairesearch.com/blip-2/)

---

*æœ¬æ–‡æ¡£ç”±Large-Model-Tutorialé¡¹ç›®ç»´æŠ¤ã€‚å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æIssueï¼*

