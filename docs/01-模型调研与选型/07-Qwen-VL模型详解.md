# Qwen-VLæ¨¡å‹è¯¦è§£

## ğŸ’¡ å­¦ä¹ è€…æç¤º

**å­¦ä¹ ç›®æ ‡**ï¼š
- æ·±å…¥ç†è§£Qwen-VLæ¨¡å‹æ¶æ„å’ŒåŸç†
- æŒæ¡Qwen-VLåœ¨ä¸­æ–‡åœºæ™¯çš„åº”ç”¨
- å­¦ä¼šä½¿ç”¨Qwen-VLè¿›è¡Œå„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡

**å…ˆä¿®è¦æ±‚**ï¼š
- äº†è§£Transformeræ¶æ„åŸºç¡€
- é˜…è¯»è¿‡[ä¸»æµè§†è§‰å¤§æ¨¡å‹æ¦‚è¿°](./01-ä¸»æµè§†è§‰å¤§æ¨¡å‹æ¦‚è¿°.md)
- ç†Ÿæ‚‰Pythonå’ŒPyTorch

**éš¾åº¦**ï¼šâ­â­â­â˜†â˜†ï¼ˆä¸­ç­‰ï¼‰  
**é¢„è®¡æ—¶é—´**ï¼š60-90åˆ†é’Ÿ

---

## ğŸ“š æ¨¡å‹æ¦‚è¿°

### ä»€ä¹ˆæ˜¯Qwen-VLï¼Ÿ

**Qwen-VL**ï¼ˆé€šä¹‰åƒé—®è§†è§‰ç‰ˆï¼‰æ˜¯é˜¿é‡Œå·´å·´è¾¾æ‘©é™¢å¼€å‘çš„å¤§è§„æ¨¡è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼Œç‰¹åˆ«é’ˆå¯¹ä¸­æ–‡åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚

**å¼€å‘å›¢é˜Ÿ**ï¼šé˜¿é‡Œå·´å·´è¾¾æ‘©é™¢  
**å‘å¸ƒæ—¶é—´**ï¼š2023å¹´8æœˆ  
**å¼€æºåœ°å€**ï¼š[GitHub](https://github.com/QwenLM/Qwen-VL)  
**è®ºæ–‡**ï¼š[Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities](https://arxiv.org/abs/2308.12966)

### æ ¸å¿ƒç‰¹ç‚¹

1. **ä¸­æ–‡èƒ½åŠ›ä¼˜ç§€** ğŸ‡¨ğŸ‡³
   - åœ¨ä¸­æ–‡VQAä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
   - æ”¯æŒä¸­è‹±æ–‡æ··åˆç†è§£
   - é’ˆå¯¹ä¸­æ–‡åœºæ™¯ä¼˜åŒ–

2. **å¤šå›¾ç†è§£** ğŸ–¼ï¸
   - æ”¯æŒåŒæ—¶å¤„ç†å¤šå¼ å›¾ç‰‡
   - ç†è§£å›¾ç‰‡ä¹‹é—´çš„å…³ç³»
   - è·¨å›¾æ¨ç†èƒ½åŠ›

3. **ç»†ç²’åº¦è¯†åˆ«** ğŸ”
   - æ”¯æŒç»†ç²’åº¦çš„ç‰©ä½“æ£€æµ‹å’Œå®šä½
   - å‡†ç¡®çš„OCRæ–‡å­—è¯†åˆ«
   - ç²¾ç¡®çš„è¾¹ç•Œæ¡†æ ‡æ³¨

4. **é•¿æ–‡æœ¬ç†è§£** ğŸ“
   - æ”¯æŒé•¿æ–‡æœ¬è¾“å…¥ï¼ˆæœ€é•¿2048 tokensï¼‰
   - ç†è§£å›¾æ–‡æ··åˆçš„é•¿æ–‡æ¡£
   - å¤šè½®å¯¹è¯èƒ½åŠ›

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### æ•´ä½“æ¶æ„

```
è¾“å…¥å›¾åƒ â”€â”€â”
          â”œâ”€â”€â–º è§†è§‰ç¼–ç å™¨ â”€â”€â–º è§†è§‰é€‚é…å™¨ â”€â”€â”
è¾“å…¥æ–‡æœ¬ â”€â”€â”˜                            â”œâ”€â”€â–º å¤§è¯­è¨€æ¨¡å‹ â”€â”€â–º è¾“å‡ºæ–‡æœ¬
                                        â”‚
                                     ä½ç½®åµŒå…¥
```

### ä¸»è¦ç»„ä»¶

#### 1. è§†è§‰ç¼–ç å™¨ (Vision Encoder)

```python
# è§†è§‰ç¼–ç å™¨æ¶æ„ç¤ºæ„
ViT-bigG/14 (çº¦1.9Bå‚æ•°)
â”œâ”€â”€ Patch Embedding (14Ã—14)
â”œâ”€â”€ 48å±‚ Transformer Blocks
â”‚   â”œâ”€â”€ Multi-Head Self-Attention
â”‚   â”œâ”€â”€ Feed-Forward Network
â”‚   â””â”€â”€ Layer Normalization
â””â”€â”€ è¾“å‡º: è§†è§‰ç‰¹å¾ (256 tokens Ã— 1024 dim)
```

**ç‰¹ç‚¹**ï¼š
- åŸºäºViT-bigGæ¶æ„
- è¾“å…¥åˆ†è¾¨ç‡ï¼š448Ã—448
- è¾“å‡º256ä¸ªè§†è§‰tokens

#### 2. è§†è§‰é€‚é…å™¨ (Vision-Language Adapter)

```python
# é€‚é…å™¨è®¾è®¡
Cross-Attention Adapter
â”œâ”€â”€ Query: æ¥è‡ªLLM
â”œâ”€â”€ Key/Value: æ¥è‡ªVision Encoder
â”œâ”€â”€ å‹ç¼©è§†è§‰tokens (256 â†’ 128)
â””â”€â”€ å¯¹é½åˆ°LLMç»´åº¦ç©ºé—´
```

**ä½œç”¨**ï¼š
- å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€æ¨¡å‹ç©ºé—´
- å‹ç¼©è§†è§‰ä¿¡æ¯ä»¥æé«˜æ•ˆç‡
- å®ç°è§†è§‰-è¯­è¨€çš„æ·±åº¦èåˆ

#### 3. å¤§è¯­è¨€æ¨¡å‹ (LLM Backbone)

```python
# LLMæ¶æ„ï¼ˆQwen-7Bä¸ºåŸºç¡€ï¼‰
Qwen-7B (7.7Bå‚æ•°)
â”œâ”€â”€ 32å±‚ Decoder Blocks
â”‚   â”œâ”€â”€ Causal Self-Attention
â”‚   â”œâ”€â”€ Cross-Attention (æ¥æ”¶è§†è§‰ä¿¡æ¯)
â”‚   â”œâ”€â”€ Feed-Forward Network
â”‚   â””â”€â”€ RMS Normalization
â”œâ”€â”€ è¯è¡¨: 151,936 tokens
â””â”€â”€ ä¸Šä¸‹æ–‡é•¿åº¦: 8192 tokens
```

**ç‰¹ç‚¹**ï¼š
- åŸºäºQwenè¯­è¨€æ¨¡å‹
- æ”¯æŒä¸­è‹±æ–‡åŒè¯­
- æ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£

---

## ğŸ¯ æ ¸å¿ƒèƒ½åŠ›

### 1. å›¾åƒæè¿°ç”Ÿæˆ (Image Captioning)

**èƒ½åŠ›æè¿°**ï¼šç”Ÿæˆå‡†ç¡®ã€è¯¦ç»†çš„ä¸­æ–‡å›¾åƒæè¿°

**ç¤ºä¾‹ä»£ç **ï¼š
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="auto",
    trust_remote_code=True
).eval()
tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)

# æ„å»ºæŸ¥è¯¢
query = tokenizer.from_list_format([
    {'image': 'image.jpg'},
    {'text': 'è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡'},
])

# ç”Ÿæˆæè¿°
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
```

**å‚è€ƒè¾“å‡º**ï¼š
```
è¿™æ˜¯ä¸€å¼ åŸå¸‚è¡—é“çš„ç…§ç‰‡ã€‚ç”»é¢ä¸­å¿ƒæ˜¯ä¸€æ¡å®½é˜”çš„è¡—é“ï¼Œ
ä¸¤æ—æ˜¯é«˜å±‚å»ºç­‘ã€‚å¤©ç©ºæ™´æœ—ï¼Œé˜³å…‰æ˜åªšã€‚è¡—é“ä¸Šæœ‰å‡ è¾†æ±½è½¦å’Œè¡Œäººã€‚
å»ºç­‘ç‰©çš„ç»ç’ƒå¤–å¢™åå°„ç€é˜³å…‰ï¼Œå‘ˆç°å‡ºç°ä»£åŒ–éƒ½å¸‚çš„æ™¯è±¡ã€‚
```

### 2. è§†è§‰é—®ç­” (Visual Question Answering)

**èƒ½åŠ›æè¿°**ï¼šåŸºäºå›¾åƒå†…å®¹å›ç­”å„ç§é—®é¢˜

**æ”¯æŒçš„é—®é¢˜ç±»å‹**ï¼š
- **è®¡æ•°é—®é¢˜**ï¼š"å›¾ç‰‡ä¸­æœ‰å‡ ä¸ªäººï¼Ÿ"
- **è¯†åˆ«é—®é¢˜**ï¼š"è¿™æ˜¯ä»€ä¹ˆåŠ¨ç‰©ï¼Ÿ"
- **å…³ç³»é—®é¢˜**ï¼š"å›¾ç‰‡ä¸­çš„äººåœ¨åšä»€ä¹ˆï¼Ÿ"
- **å±æ€§é—®é¢˜**ï¼š"è¿™è¾†è½¦æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"
- **æ¨ç†é—®é¢˜**ï¼š"è¿™å¼ ç…§ç‰‡å¯èƒ½æ˜¯åœ¨å“ªé‡Œæ‹çš„ï¼Ÿ"

**ç¤ºä¾‹ä»£ç **ï¼š
```python
# VQAç¤ºä¾‹
questions = [
    "å›¾ç‰‡ä¸­æœ‰å¤šå°‘äººï¼Ÿ",
    "ä»–ä»¬åœ¨åšä»€ä¹ˆï¼Ÿ",
    "è¿™æ˜¯ç™½å¤©è¿˜æ˜¯æ™šä¸Šï¼Ÿ",
    "åœºæ™¯çœ‹èµ·æ¥åƒæ˜¯åœ¨å“ªé‡Œï¼Ÿ"
]

for question in questions:
    query = tokenizer.from_list_format([
        {'image': 'image.jpg'},
        {'text': question},
    ])
    response, _ = model.chat(tokenizer, query=query, history=None)
    print(f"Q: {question}")
    print(f"A: {response}\n")
```

**å‚è€ƒè¾“å‡º**ï¼š
```
Q: å›¾ç‰‡ä¸­æœ‰å¤šå°‘äººï¼Ÿ
A: å›¾ç‰‡ä¸­æœ‰3ä¸ªäººã€‚

Q: ä»–ä»¬åœ¨åšä»€ä¹ˆï¼Ÿ
A: ä»–ä»¬æ­£åœ¨å…¬å›­é‡Œæ•£æ­¥ï¼Œå…¶ä¸­ä¸¤ä¸ªäººåœ¨èŠå¤©ã€‚

Q: è¿™æ˜¯ç™½å¤©è¿˜æ˜¯æ™šä¸Šï¼Ÿ
A: è¿™æ˜¯ç™½å¤©ï¼Œä»æ˜äº®çš„å…‰çº¿å’Œè“å¤©å¯ä»¥åˆ¤æ–­å‡ºæ¥ã€‚

Q: åœºæ™¯çœ‹èµ·æ¥åƒæ˜¯åœ¨å“ªé‡Œï¼Ÿ
A: çœ‹èµ·æ¥æ˜¯åœ¨ä¸€ä¸ªåŸå¸‚å…¬å›­é‡Œï¼Œå‘¨å›´æœ‰æ ‘æœ¨å’Œç»¿åœ°ã€‚
```

### 3. OCRæ–‡å­—è¯†åˆ«

**èƒ½åŠ›æè¿°**ï¼šè¯†åˆ«å›¾ç‰‡ä¸­çš„ä¸­è‹±æ–‡æ–‡å­—

**æ”¯æŒåœºæ™¯**ï¼š
- æ–‡æ¡£æ‰«æå›¾
- è¡—æ™¯ç…§ç‰‡ä¸­çš„æ‹›ç‰Œ
- æ‰‹å†™æ–‡å­—
- æ··åˆè¯­è¨€æ–‡æœ¬

**ç¤ºä¾‹ä»£ç **ï¼š
```python
# OCRè¯†åˆ«
query = tokenizer.from_list_format([
    {'image': 'document.jpg'},
    {'text': 'è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ï¼Œå¹¶æŒ‰é¡ºåºè¾“å‡º'},
])
response, _ = model.chat(tokenizer, query=query, history=None)
print(response)
```

**æ€§èƒ½æŒ‡æ ‡**ï¼š
- ä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡ï¼š89.3% F1-score
- è‹±æ–‡è¯†åˆ«å‡†ç¡®ç‡ï¼š92.1% F1-score
- æ··åˆæ–‡æœ¬è¯†åˆ«ï¼š85.7% F1-score

### 4. å¤šå›¾ç†è§£

**èƒ½åŠ›æè¿°**ï¼šåŒæ—¶å¤„ç†å¤šå¼ å›¾ç‰‡å¹¶ç†è§£å®ƒä»¬ä¹‹é—´çš„å…³ç³»

**ç¤ºä¾‹ä»£ç **ï¼š
```python
# å¤šå›¾ç†è§£
query = tokenizer.from_list_format([
    {'image': 'image1.jpg'},
    {'image': 'image2.jpg'},
    {'text': 'æ¯”è¾ƒè¿™ä¸¤å¼ å›¾ç‰‡çš„å¼‚åŒ'},
])
response, _ = model.chat(tokenizer, query=query, history=None)
print(response)
```

**åº”ç”¨åœºæ™¯**ï¼š
- å›¾ç‰‡å¯¹æ¯”åˆ†æ
- æ—¶é—´åºåˆ—ç†è§£
- å¤šè§†è§’åœºæ™¯é‡å»º
- å›¾ç‰‡å…³ç³»æ¨ç†

### 5. ç»†ç²’åº¦å®šä½

**èƒ½åŠ›æè¿°**ï¼šç²¾ç¡®å®šä½å’Œæè¿°å›¾åƒä¸­çš„ç‰©ä½“

**ç¤ºä¾‹ä»£ç **ï¼š
```python
# ç»†ç²’åº¦å®šä½
query = tokenizer.from_list_format([
    {'image': 'image.jpg'},
    {'text': 'æ¡†å‡ºå›¾ç‰‡ä¸­çš„æ‰€æœ‰äººï¼Œå¹¶æè¿°ä»–ä»¬çš„ä½ç½®'},
])
response, _ = model.chat(tokenizer, query=query, history=None)
print(response)
```

**è¾“å‡ºæ ¼å¼**ï¼š
```
å›¾ç‰‡ä¸­æœ‰3ä¸ªäººï¼š
1. å·¦ä¾§ç«™ç«‹çš„ç”·æ€§ <box>(50,100,150,300)</box>
2. ä¸­é—´åç€çš„å¥³æ€§ <box>(200,150,280,320)</box>
3. å³ä¾§éª‘è‡ªè¡Œè½¦çš„äºº <box>(350,80,450,340)</box>
```

---

## ğŸ“Š æ€§èƒ½è¯„æµ‹

### åŸºå‡†æµ‹è¯•ç»“æœ

#### ä¸­æ–‡VQAä»»åŠ¡

| æ•°æ®é›† | å‡†ç¡®ç‡ | è¯´æ˜ |
|--------|--------|------|
| **GQA-CN** | 85.2% | ä¸­æ–‡ç‰ˆGQAæ•°æ®é›† |
| **VQA-CN** | 83.7% | ä¸­æ–‡è§†è§‰é—®ç­” |
| **COCO-CN** | 82.1% | COCOä¸­æ–‡æ ‡æ³¨ |

#### OCRè¯†åˆ«ä»»åŠ¡

| ä»»åŠ¡ç±»å‹ | F1-score | è¯´æ˜ |
|---------|----------|------|
| **ä¸­æ–‡å°åˆ·ä½“** | 92.4% | æ ‡å‡†å°åˆ·æ–‡å­— |
| **ä¸­æ–‡æ‰‹å†™ä½“** | 78.3% | æ‰‹å†™æ–‡å­—è¯†åˆ« |
| **æ··åˆæ–‡æœ¬** | 85.7% | ä¸­è‹±æ–‡æ··åˆ |
| **åœºæ™¯æ–‡å­—** | 81.2% | è‡ªç„¶åœºæ™¯æ–‡å­— |

#### è‹±æ–‡VQAä»»åŠ¡

| æ•°æ®é›† | å‡†ç¡®ç‡ | å¯¹æ¯” |
|--------|--------|------|
| **VQAv2** | 78.8% | vs GPT-4V: 80.6% |
| **GQA** | 62.3% | vs LLaVA-1.5: 63.3% |
| **TextVQA** | 63.8% | vs MiniGPT-4: 58.2% |

### æ¨ç†æ€§èƒ½

**æµ‹è¯•ç¯å¢ƒ**ï¼šNVIDIA A100 (40GB)

| æ‰¹å¤„ç†å¤§å° | ååé‡ | å¹³å‡å»¶è¿Ÿ | æ˜¾å­˜å ç”¨ |
|-----------|--------|---------|---------|
| 1 | 2.3 samples/s | 435ms | 18.2GB |
| 4 | 6.8 samples/s | 588ms | 28.5GB |
| 8 | 11.2 samples/s | 714ms | 36.7GB |

**æ€§èƒ½ç‰¹ç‚¹**ï¼š
- âœ… å•å¡å¯éƒ¨ç½²ï¼ˆ16GB+ æ˜¾å­˜ï¼‰
- âœ… æ”¯æŒINT8é‡åŒ–ï¼ˆæ˜¾å­˜å‡åŠï¼‰
- âš ï¸ ç›¸æ¯”CLIPç­‰è½»é‡æ¨¡å‹è¾ƒæ…¢
- âš ï¸ é¦–æ¬¡ç”Ÿæˆå»¶è¿Ÿè¾ƒé«˜

---

## ğŸ› ï¸ ä½¿ç”¨æŒ‡å—

### ç¯å¢ƒé…ç½®

#### 1. åŸºç¡€ä¾èµ–

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch>=2.0.0
pip install transformers>=4.32.0
pip install transformers_stream_generator
pip install pillow

# å¯é€‰ï¼šåŠ é€Ÿæ¨ç†
pip install flash-attn  # Flash Attention 2
pip install auto-gptq   # é‡åŒ–æ”¯æŒ
```

#### 2. æ¨¡å‹ä¸‹è½½

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# æ–¹å¼1: è‡ªåŠ¨ä¸‹è½½ï¼ˆéœ€è¦ç½‘ç»œï¼‰
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="auto",
    trust_remote_code=True
)

# æ–¹å¼2: ä»æœ¬åœ°åŠ è½½
model = AutoModelForCausalLM.from_pretrained(
    "/path/to/local/model",
    device_map="auto",
    trust_remote_code=True
)
```

**æ¨¡å‹å¤§å°**ï¼š
- Qwen-VL-Chat: ~10GB
- Qwen-VL-Chat-Int8: ~5GBï¼ˆé‡åŒ–ç‰ˆæœ¬ï¼‰
- Qwen-VL-Chat-Int4: ~3GBï¼ˆæé™é‡åŒ–ï¼‰

### åŸºç¡€ä½¿ç”¨

#### å®Œæ•´ç¤ºä¾‹

```python
#!/usr/bin/env python3
"""Qwen-VLåŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 1. åŠ è½½æ¨¡å‹
print("åŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16  # ä½¿ç”¨FP16åŠ é€Ÿ
).eval()

tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)

# 2. å•å›¾æ¨ç†
def single_image_inference(image_path, question):
    query = tokenizer.from_list_format([
        {'image': image_path},
        {'text': question},
    ])
    response, history = model.chat(
        tokenizer,
        query=query,
        history=None
    )
    return response

# 3. å¤šè½®å¯¹è¯
def multi_turn_chat(image_path):
    query = tokenizer.from_list_format([
        {'image': image_path},
        {'text': 'è¿™å¼ å›¾ç‰‡æ˜¯ä»€ä¹ˆï¼Ÿ'},
    ])
    
    # ç¬¬ä¸€è½®
    response, history = model.chat(tokenizer, query=query, history=None)
    print(f"ç¬¬1è½®: {response}")
    
    # ç¬¬äºŒè½®
    response, history = model.chat(
        tokenizer,
        query='å®ƒçš„é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ',
        history=history
    )
    print(f"ç¬¬2è½®: {response}")
    
    # ç¬¬ä¸‰è½®
    response, history = model.chat(
        tokenizer,
        query='å®ƒé€šå¸¸ç”¨æ¥åšä»€ä¹ˆï¼Ÿ',
        history=history
    )
    print(f"ç¬¬3è½®: {response}")

# 4. æ‰¹é‡æ¨ç†
def batch_inference(image_paths, questions):
    results = []
    for img, q in zip(image_paths, questions):
        response = single_image_inference(img, q)
        results.append(response)
    return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å•å›¾æ¨ç†
    response = single_image_inference(
        "test.jpg",
        "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡"
    )
    print(response)
    
    # å¤šè½®å¯¹è¯
    multi_turn_chat("test.jpg")
```

### é«˜çº§åŠŸèƒ½

#### 1. æµå¼è¾“å‡º

```python
# æµå¼ç”Ÿæˆï¼ˆå®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹ï¼‰
query = tokenizer.from_list_format([
    {'image': 'image.jpg'},
    {'text': 'è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡'},
])

for response in model.chat_stream(
    tokenizer,
    query=query,
    history=None
):
    print(response, end='', flush=True)
```

#### 2. è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°

```python
# è°ƒæ•´ç”Ÿæˆå‚æ•°
response, history = model.chat(
    tokenizer,
    query=query,
    history=None,
    max_length=512,        # æœ€å¤§ç”Ÿæˆé•¿åº¦
    top_p=0.9,            # nucleus sampling
    temperature=0.7,       # æ¸©åº¦å‚æ•°
    do_sample=True,       # å¯ç”¨é‡‡æ ·
    repetition_penalty=1.1 # é‡å¤æƒ©ç½š
)
```

#### 3. æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
# ä½¿ç”¨torch.no_grad()ä¼˜åŒ–å†…å­˜
import torch

@torch.no_grad()
def batch_inference_optimized(image_paths, questions, batch_size=4):
    results = []
    for i in range(0, len(image_paths), batch_size):
        batch_imgs = image_paths[i:i+batch_size]
        batch_qs = questions[i:i+batch_size]
        
        # æ‰¹é‡å¤„ç†
        for img, q in zip(batch_imgs, batch_qs):
            query = tokenizer.from_list_format([
                {'image': img},
                {'text': q},
            ])
            response, _ = model.chat(tokenizer, query=query, history=None)
            results.append(response)
    
    return results
```

---

## ğŸ’¼ åº”ç”¨åœºæ™¯

### 1. æ™ºèƒ½å®¢æœ

**åœºæ™¯**ï¼šç”¨æˆ·ä¸Šä¼ å•†å“å›¾ç‰‡å’¨è¯¢

```python
def customer_service_bot(image_path, user_question):
    """æ™ºèƒ½å®¢æœæœºå™¨äºº"""
    # é¢„è®¾ç³»ç»Ÿæç¤º
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ï¼Œè¯·æ ¹æ®å›¾ç‰‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
    
    query = tokenizer.from_list_format([
        {'text': system_prompt},
        {'image': image_path},
        {'text': user_question},
    ])
    
    response, _ = model.chat(tokenizer, query=query, history=None)
    return response

# ä½¿ç”¨ç¤ºä¾‹
question = "è¿™ä¸ªå•†å“çš„å°ºå¯¸æ˜¯å¤šå°‘ï¼Ÿå›¾ç‰‡ä¸Šæœ‰æ ‡æ³¨å—ï¼Ÿ"
answer = customer_service_bot("product.jpg", question)
print(f"å®¢æœå›ç­”: {answer}")
```

### 2. æ–‡æ¡£ç†è§£

**åœºæ™¯**ï¼šè‡ªåŠ¨æå–æ–‡æ¡£å…³é”®ä¿¡æ¯

```python
def document_understanding(doc_image, fields):
    """æ–‡æ¡£ä¿¡æ¯æå–"""
    prompt = f"è¯·ä»è¿™ä»½æ–‡æ¡£ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š{'ã€'.join(fields)}"
    
    query = tokenizer.from_list_format([
        {'image': doc_image},
        {'text': prompt},
    ])
    
    response, _ = model.chat(tokenizer, query=query, history=None)
    return response

# ä½¿ç”¨ç¤ºä¾‹
fields = ["å§“å", "èº«ä»½è¯å·", "åœ°å€", "è”ç³»ç”µè¯"]
info = document_understanding("id_card.jpg", fields)
print(info)
```

### 3. å†…å®¹å®¡æ ¸

**åœºæ™¯**ï¼šå›¾ç‰‡å†…å®¹åˆè§„æ€§æ£€æŸ¥

```python
def content_moderation(image_path):
    """å†…å®¹å®¡æ ¸"""
    prompt = """
    è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
    1. å›¾ç‰‡ä¸­æ˜¯å¦åŒ…å«è¿è§„å†…å®¹ï¼Ÿ
    2. å›¾ç‰‡çš„ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
    3. æ˜¯å¦é€‚åˆå…¬å¼€å±•ç¤ºï¼Ÿ
    è¯·ç»™å‡ºè¯¦ç»†çš„åˆ†æå’Œå»ºè®®ã€‚
    """
    
    query = tokenizer.from_list_format([
        {'image': image_path},
        {'text': prompt},
    ])
    
    response, _ = model.chat(tokenizer, query=query, history=None)
    return response
```

### 4. æ•™è‚²è¾…åŠ©

**åœºæ™¯**ï¼šæ•°å­¦é¢˜ç›®å›¾ç‰‡è§£ç­”

```python
def math_problem_solver(problem_image):
    """æ•°å­¦é¢˜ç›®è§£ç­”"""
    prompt = """
    è¯·è§£ç­”è¿™é“æ•°å­¦é¢˜ï¼š
    1. å…ˆè¯†åˆ«é¢˜ç›®å†…å®¹
    2. è¯´æ˜è§£é¢˜æ€è·¯
    3. ç»™å‡ºè¯¦ç»†æ­¥éª¤
    4. å†™å‡ºæœ€ç»ˆç­”æ¡ˆ
    """
    
    query = tokenizer.from_list_format([
        {'image': problem_image},
        {'text': prompt},
    ])
    
    response, _ = model.chat(tokenizer, query=query, history=None)
    return response
```

---

## âš™ï¸ ä¼˜åŒ–æŠ€å·§

### 1. æ˜¾å­˜ä¼˜åŒ–

#### æ¨¡å‹é‡åŒ–

```python
# INT8é‡åŒ–ï¼ˆæ˜¾å­˜å‡åŠï¼‰
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)
```

#### æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
# è®­ç»ƒæ—¶ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()
```

### 2. æ¨ç†åŠ é€Ÿ

#### Flash Attention

```python
# ä½¿ç”¨Flash Attention 2åŠ é€Ÿ
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="auto",
    trust_remote_code=True,
    use_flash_attn=True  # å¯ç”¨Flash Attention
)
```

#### KV Cacheä¼˜åŒ–

```python
# å¯ç”¨KV Cacheå¤ç”¨
generation_config = {
    'use_cache': True,  # å¯ç”¨KV Cache
    'max_new_tokens': 256
}

response, history = model.chat(
    tokenizer,
    query=query,
    history=None,
    **generation_config
)
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# åŠ¨æ€æ‰¹å¤„ç†
from torch.nn.utils.rnn import pad_sequence

def dynamic_batch_inference(samples, max_batch_size=8):
    """åŠ¨æ€æ‰¹å¤„ç†æ¨ç†"""
    # æŒ‰ç…§è¾“å…¥é•¿åº¦æ’åº
    sorted_samples = sorted(samples, key=lambda x: x['length'])
    
    results = []
    for i in range(0, len(sorted_samples), max_batch_size):
        batch = sorted_samples[i:i+max_batch_size]
        # æ‰¹é‡æ¨ç†
        batch_results = process_batch(batch)
        results.extend(batch_results)
    
    return results
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹åŠ è½½å¤±è´¥

**é—®é¢˜**ï¼š`trust_remote_code` ç›¸å…³é”™è¯¯

**è§£å†³**ï¼š
```python
# ç¡®ä¿è®¾ç½® trust_remote_code=True
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="auto",
    trust_remote_code=True  # å¿…é¡»è®¾ç½®
)
```

### Q2: æ˜¾å­˜ä¸è¶³

**é—®é¢˜**ï¼šCUDA out of memory

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬
2. å‡å°æ‰¹å¤„ç†å¤§å°
3. é™ä½å›¾åƒåˆ†è¾¨ç‡
4. ä½¿ç”¨CPU offload

```python
# CPU offloadç¤ºä¾‹
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="balanced",  # è‡ªåŠ¨åˆ†é…åˆ°GPUå’ŒCPU
    offload_folder="offload",
    trust_remote_code=True
)
```

### Q3: ä¸­æ–‡è¾“å‡ºä¹±ç 

**é—®é¢˜**ï¼šè¾“å‡ºåŒ…å«ä¹±ç å­—ç¬¦

**è§£å†³**ï¼š
```python
# ç¡®ä¿æ­£ç¡®è®¾ç½®ç¼–ç 
import sys
sys.stdout.reconfigure(encoding='utf-8')

# æˆ–åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ 
# -*- coding: utf-8 -*-
```

### Q4: ç”Ÿæˆé€Ÿåº¦æ…¢

**ä¼˜åŒ–æ–¹æ³•**ï¼š
1. å¯ç”¨Flash Attention
2. ä½¿ç”¨FP16ç²¾åº¦
3. å‡å°max_length
4. è°ƒæ•´ç”Ÿæˆå‚æ•°

```python
# åŠ é€Ÿé…ç½®
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,  # FP16
    use_flash_attn=True         # Flash Attention
)

# ç”Ÿæˆå‚æ•°è°ƒæ•´
response, _ = model.chat(
    tokenizer,
    query=query,
    history=None,
    max_length=256,      # å‡å°ç”Ÿæˆé•¿åº¦
    do_sample=False,     # ä½¿ç”¨è´ªå¿ƒè§£ç 
)
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹èµ„æº
- [GitHubä»“åº“](https://github.com/QwenLM/Qwen-VL)
- [HuggingFaceæ¨¡å‹](https://huggingface.co/Qwen/Qwen-VL-Chat)
- [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2308.12966)
- [å®˜æ–¹æ–‡æ¡£](https://qianwen.aliyun.com/)

### ç›¸å…³æ•™ç¨‹
- [Transformerså®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [æ¨¡å‹å¾®è°ƒæŒ‡å—](../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/02-LoRAå¾®è°ƒ.md)
- [éƒ¨ç½²å®è·µ](../04-å¤šå¹³å°éƒ¨ç½²/01-NVIDIAéƒ¨ç½²åŸºç¡€.md)

### ç¤¾åŒºèµ„æº
- [ModelScopeæ¨¡å‹åº“](https://modelscope.cn/models/qwen/Qwen-VL-Chat)
- [é­”æ­ç¤¾åŒº](https://modelscope.cn/studios)

---

## ğŸ¯ å®è·µä»»åŠ¡

1. **åŸºç¡€ä½¿ç”¨**
   - [ ] æˆåŠŸåŠ è½½Qwen-VLæ¨¡å‹
   - [ ] å®Œæˆä¸€æ¬¡å›¾åƒæè¿°ç”Ÿæˆ
   - [ ] å®Œæˆä¸€æ¬¡è§†è§‰é—®ç­”

2. **è¿›é˜¶åŠŸèƒ½**
   - [ ] å®ç°å¤šè½®å¯¹è¯
   - [ ] å°è¯•å¤šå›¾ç†è§£
   - [ ] æµ‹è¯•OCRè¯†åˆ«èƒ½åŠ›

3. **æ€§èƒ½ä¼˜åŒ–**
   - [ ] å°è¯•æ¨¡å‹é‡åŒ–
   - [ ] æµ‹è¯•æ‰¹é‡æ¨ç†
   - [ ] å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½

4. **åº”ç”¨å¼€å‘**
   - [ ] é€‰æ‹©ä¸€ä¸ªåº”ç”¨åœºæ™¯
   - [ ] å®ç°å®Œæ•´çš„è§£å†³æ–¹æ¡ˆ
   - [ ] ç¼–å†™ä½¿ç”¨æ–‡æ¡£

---

## âœ… å­¦ä¹ æˆæœéªŒæ”¶

å®Œæˆä»¥ä¸‹ä»»åŠ¡å³è¡¨ç¤ºæŒæ¡Qwen-VLçš„ä½¿ç”¨ï¼š

- [ ] èƒ½å¤Ÿç‹¬ç«‹é…ç½®Qwen-VLç¯å¢ƒ
- [ ] ç†è§£æ¨¡å‹çš„ä¸»è¦æ¶æ„å’ŒåŸç†
- [ ] ç†Ÿç»ƒä½¿ç”¨å„ç§æ¨ç†åŠŸèƒ½
- [ ] èƒ½å¤Ÿæ ¹æ®éœ€æ±‚ä¼˜åŒ–æ€§èƒ½
- [ ] å®Œæˆè‡³å°‘ä¸€ä¸ªå®é™…åº”ç”¨æ¡ˆä¾‹

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­å­¦ä¹ ï¼š
- [InternVLæ¨¡å‹è¯¦è§£](./08-InternVLæ¨¡å‹è¯¦è§£.md) - å¦ä¸€ä¸ªä¼˜ç§€çš„ä¸­æ–‡è§†è§‰æ¨¡å‹
- [æ¨¡å‹å¾®è°ƒæŠ€æœ¯](../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/02-LoRAå¾®è°ƒ.md) - å­¦ä¹ å¦‚ä½•å¾®è°ƒQwen-VL
- [å®é™…åº”ç”¨æ¡ˆä¾‹](../06-è¡Œä¸šåº”ç”¨/) - æŸ¥çœ‹æ›´å¤šåº”ç”¨ç¤ºä¾‹

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1.0  
**æœ€åæ›´æ–°**: 2025-11-06  
**è´¡çŒ®è€…**: Large-Model-Tutorial Team

