# 模型对比与评测

## 💡 学习者提示

本章节提供详细的模型对比数据和评测方法，帮助您基于量化指标做出选型决策。

**学习目标**：
- 理解视觉大模型的评测指标
- 掌握多维度对比方法
- 能够根据指标选择合适的模型

**先修要求**：
- 已阅读"主流视觉大模型概述"
- 了解基本的评测指标（准确率、F1等）

**难度**：⭐⭐☆☆☆（初级）  
**预计时间**：45-60分钟

---

## 📐 评测维度说明

### 1. 性能指标

不同任务有不同的评测标准：

| 任务类型 | 主要指标 | 说明 |
|---------|---------|------|
| **图像分类** | Top-1/Top-5准确率 | 预测正确的比例 |
| **目标检测** | mAP (mean Average Precision) | 检测精度和召回率的综合 |
| **图像分割** | mIoU (mean Intersection over Union) | 分割区域的重叠度 |
| **图像描述** | CIDEr, BLEU, METEOR | 生成文本的质量 |
| **视觉问答** | 准确率, BLEU | 回答的正确性 |
| **图文检索** | Recall@K | 检索到正确结果的比例 |

### 2. 效率指标

| 指标 | 单位 | 说明 |
|------|------|------|
| **推理速度** | images/sec 或 ms/image | 处理速度 |
| **显存占用** | GB | GPU内存需求 |
| **模型大小** | MB/GB | 存储空间需求 |
| **FLOPs** | G (Giga) | 计算量 |

### 3. 资源需求

| 资源类型 | 评估维度 |
|---------|---------|
| **GPU** | 型号、显存、数量 |
| **CPU** | 核心数、内存 |
| **存储** | 模型权重、缓存数据 |
| **网络** | 下载速度、API调用 |

---

## 📊 综合性能对比

### 图文多模态任务

基于 COCO 图文检索和 ImageNet 零样本分类的对比：

| 模型 | ImageNet<br/>Zero-shot | COCO<br/>Text→Image<br/>R@1 | COCO<br/>Image→Text<br/>R@1 | 推理速度<br/>(V100) | 显存<br/>(推理) |
|------|:---:|:---:|:---:|:---:|:---:|
| **CLIP-ViT-B/32** | 63.2% | 33.6% | 58.4% | ~50 img/s | 2.5 GB |
| **CLIP-ViT-L/14** | 75.5% | 41.2% | 63.9% | ~15 img/s | 4.8 GB |
| **BLIP-Base** | - | 42.4% | 71.2% | ~30 img/s | 3.2 GB |
| **BLIP-Large** | - | 46.3% | 74.5% | ~12 img/s | 5.5 GB |
| **BLIP-2** | - | 51.7% | 77.2% | ~8 img/s | 6.8 GB |

**💡 解读**：
- CLIP适合需要快速推理的场景
- BLIP系列在图文检索精度上更优
- BLIP-2性能最强但推理最慢

---

### 图像描述生成

基于 COCO Captioning 数据集：

| 模型 | BLEU-4 | CIDEr | METEOR | SPICE | 模型大小 |
|------|:---:|:---:|:---:|:---:|:---:|
| **Show-Attend-Tell** | 32.1 | 107.5 | 25.7 | 19.2 | ~200MB |
| **BLIP-Base** | 38.6 | 129.3 | 28.6 | 22.4 | ~990MB |
| **BLIP-2-OPT2.7B** | 41.2 | 136.7 | 29.8 | 23.8 | ~5.1GB |
| **LLaVA-1.5-7B** | 39.5 | 132.4 | 29.1 | 23.2 | ~13GB |
| **Qwen-VL-Chat** | 40.8 | 135.2 | 30.2 | 24.1 | ~9.6GB |

**参考输出示例**：

```
输入图像: 一个人在海滩上冲浪

模型输出对比:
├─ BLIP-Base:     "A person surfing on a wave in the ocean"
├─ BLIP-2:        "A surfer riding a large wave at the beach during sunset"
├─ LLaVA:         "A skilled surfer is catching a wave at a beautiful beach"
└─ Qwen-VL(中文): "一个冲浪者正在海滩上乘风破浪，享受日落时分的美景"
```

---

### 视觉问答（VQA）

基于 VQAv2 数据集：

| 模型 | 整体准确率 | Yes/No | Number | Other | 推理时间<br/>(A100) |
|------|:---:|:---:|:---:|:---:|:---:|
| **BLIP-Base** | 77.5% | 88.6% | 51.2% | 68.3% | ~80ms |
| **BLIP-2-Flan-T5-XL** | 82.2% | 91.4% | 56.8% | 73.9% | ~150ms |
| **LLaVA-1.5-7B** | 78.5% | 89.2% | 53.6% | 70.1% | ~200ms |
| **LLaVA-1.5-13B** | 80.0% | 90.1% | 55.4% | 71.8% | ~350ms |
| **Qwen-VL-Chat** | 79.8% | 89.8% | 54.9% | 71.5% | ~220ms |

**示例问答**：

```
图片：一只猫坐在笔记本电脑旁边

Q: What animal is in the image?
└─ 所有模型: "A cat" ✅

Q: What is the cat doing?
├─ BLIP-2: "Sitting next to a laptop" ✅
├─ LLaVA: "The cat is sitting beside a laptop computer" ✅
└─ Qwen-VL: "猫咪正坐在笔记本电脑旁边" ✅

Q: What brand is the laptop?
├─ BLIP-2: "Apple" (如果能看清logo) ✅
├─ LLaVA: "It appears to be an Apple MacBook" ✅
└─ Qwen-VL: "看起来是苹果的MacBook" ✅
```

---

### 图像分割

基于 COCO 和 SA-1B 数据集：

| 模型 | mIoU<br/>(COCO) | 推理时间<br/>(单张) | 显存<br/>(推理) | 提示类型 |
|------|:---:|:---:|:---:|:---:|
| **Mask R-CNN** | 39.8% | ~100ms | 3.5GB | 无需提示 |
| **SAM-ViT-B** | 87.3% | ~50ms | 4.2GB | 点/框/掩码 |
| **SAM-ViT-L** | 90.2% | ~80ms | 6.5GB | 点/框/掩码 |
| **SAM-ViT-H** | 91.6% | ~150ms | 10.2GB | 点/框/掩码 |

**分割效果对比**：

```
场景：复杂背景下的多目标分割

输入提示：点击图片中的狗
├─ SAM-ViT-B: IoU=0.873, 边界略有毛刺
├─ SAM-ViT-L: IoU=0.902, 边界精细
└─ SAM-ViT-H: IoU=0.916, 边界非常精确

推荐：
- 实时应用 → SAM-ViT-B (速度快)
- 高质量需求 → SAM-ViT-H (精度高)
- 平衡选择 → SAM-ViT-L (性价比)
```

---

## 🔍 多维度对比分析

### 1. 架构对比

| 维度 | CLIP | SAM | BLIP-2 | LLaVA |
|------|------|-----|--------|-------|
| **视觉编码器** | ViT | ViT | EVA-CLIP | CLIP |
| **文本编码器** | Transformer | - | OPT/Flan-T5 | LLaMA |
| **对齐方式** | 对比学习 | - | Q-Former | 线性投影 |
| **训练范式** | 预训练 | 预训练 | 两阶段 | 指令微调 |
| **输出类型** | 特征向量 | 掩码 | 文本 | 文本 |

### 2. 训练数据规模

| 模型 | 图文对数量 | 数据来源 | 训练成本 |
|------|:---:|---------|:---:|
| **CLIP** | 400M | 互联网 | ~$100K |
| **SAM** | 11M图像<br/>1.1B掩码 | 自建数据集 | ~$500K |
| **BLIP-2** | 129M | 公开数据集 | ~$50K |
| **LLaVA** | 158K指令 | GPT-4生成 | ~$10K |
| **Qwen-VL** | - | 多源数据 | ~$500K |

**💡 微调建议**：
- 小数据集(<10K) → 使用LoRA微调CLIP或LLaVA
- 中等数据集(10K-100K) → 全参数微调BLIP-2
- 大数据集(>100K) → 预训练新模型

---

### 3. 多语言支持

| 模型 | 英文 | 中文 | 其他语言 | 备注 |
|------|:---:|:---:|:---:|------|
| **CLIP** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | 多语言CLIP效果一般 |
| **BLIP-2** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | 主要优化英文 |
| **LLaVA** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | 依赖LLaMA基座 |
| **Qwen-VL** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 中文最优 |
| **InternVL** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 多语言均衡 |

**中文测试结果**：

```
问题：图片中的文字是什么？（中文OCR）

输入：包含"请勿吸烟"标识的图片

├─ CLIP: "No Smoking" (翻译成英文) ⚠️
├─ BLIP-2: "A sign with Chinese characters" (无法识别) ❌
├─ LLaVA: "请勿吸烟" (基本正确) ✅
├─ Qwen-VL: "图片中显示'请勿吸烟'的警示标志" (完美) ✅✅
└─ InternVL: "标识牌上写着'请勿吸烟'" (优秀) ✅✅
```

---

### 4. 部署兼容性

| 平台 | CLIP | SAM | BLIP-2 | LLaVA | Qwen-VL |
|------|:---:|:---:|:---:|:---:|:---:|
| **NVIDIA GPU** | ✅ | ✅ | ✅ | ✅ | ✅ |
| **华为昇腾** | ✅ | ⚠️ | ⚠️ | ❌ | ✅ |
| **AMD GPU** | ✅ | ⚠️ | ⚠️ | ⚠️ | ⚠️ |
| **CPU (推理)** | ✅ | ⚠️ | ❌ | ❌ | ❌ |
| **移动端** | ✅ (量化) | ❌ | ❌ | ❌ | ❌ |
| **ONNX导出** | ✅ | ✅ | ⚠️ | ❌ | ⚠️ |
| **TensorRT** | ✅ | ✅ | ⚠️ | ❌ | ⚠️ |

**图例**：✅ 完全支持 | ⚠️ 部分支持/需适配 | ❌ 不支持

---

## 💰 成本分析

### 训练成本估算

基于 8×A100 (80GB) GPU集群：

| 任务 | 模型 | 数据规模 | 训练时间 | 成本估算 |
|------|------|:---:|:---:|:---:|
| **LoRA微调** | LLaVA-7B | 10K | 2小时 | ~$20 |
| **全量微调** | BLIP-Base | 50K | 12小时 | ~$150 |
| **预训练** | CLIP | 100M | 2周 | ~$100K |

### 推理成本对比

基于云服务（V100 GPU，按小时计费 $2.50/小时）：

| 模型 | 处理速度 | 单次成本 | 1000次成本 | 适用场景 |
|------|:---:|:---:|:---:|---------|
| **CLIP** | 50 img/s | $0.000014 | $14 | 大规模检索 |
| **SAM** | 20 img/s | $0.000035 | $35 | 批量标注 |
| **BLIP-2** | 8 img/s | $0.000087 | $87 | 高质量描述 |
| **LLaVA-7B** | 2 img/s | $0.000347 | $347 | 专业对话 |

**💡 成本优化建议**：
1. 批量处理降低单次成本
2. 模型量化减少资源需求
3. 混合使用不同规模模型
4. 考虑自建推理集群（规模>10万次/月）

---

## 🎯 选型决策树

```
开始
├─ 是否需要中文支持？
│  ├─ 是 → Qwen-VL / InternVL / CogVLM
│  └─ 否 → 继续
│
├─ 主要任务类型？
│  ├─ 图文检索/分类 → CLIP
│  ├─ 图像分割 → SAM
│  ├─ 图像描述 → BLIP-2
│  ├─ 视觉问答 → BLIP-2 / LLaVA
│  └─ 多轮对话 → LLaVA / Qwen-VL
│
├─ 资源约束？
│  ├─ 低资源(<8GB) → CLIP / BLIP-Base
│  ├─ 中等(8-16GB) → SAM / BLIP-2 / LLaVA-7B
│  └─ 高资源(>16GB) → LLaVA-13B / Qwen-VL
│
└─ 推理速度要求？
   ├─ 实时(<50ms) → CLIP / SAM-ViT-B
   ├─ 快速(<200ms) → BLIP-Base / SAM-ViT-L
   └─ 质量优先 → BLIP-2 / LLaVA
```

---

## 🔬 实践任务

### 任务1：模型性能对比表

创建一个Excel表格，对比3-5个候选模型的：
- 性能指标（针对您的任务）
- 资源需求
- 部署难度
- 估算成本

### 任务2：参考基准测试

参考以下数据集运行基准测试（不需要实际运行，了解测试方法即可）：
```python
# 伪代码示例
def benchmark_model(model, dataset, metric):
    """
    评测模型性能
    
    预期输出:
    - CLIP on ImageNet: Acc=76.2%
    - SAM on COCO: mIoU=91.6%
    - BLIP-2 on VQAv2: Acc=82.2%
    """
    pass
```

### 任务3：成本预算

假设您需要处理：
- 每日图片量：10,000张
- 任务类型：图像描述生成
- 服务时长：1年

计算使用不同模型的年度成本，选择最经济的方案。

---

## 📖 参考资料

- [CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark)
- [Open VLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard)
- [Papers with Code - Vision Benchmarks](https://paperswithcode.com/area/computer-vision)

---

## ➡️ 下一步

- [03-选型策略](./03-选型策略.md) - 系统化的选型方法
- [04-基准测试实践](./04-基准测试实践.md) - 动手测试模型

---

**文档版本**: v1.0  
**最后更新**: 2025-11-01

