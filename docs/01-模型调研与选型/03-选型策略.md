# 选型策略

## 💡 学习者提示

本章节提供系统化的模型选型方法论，帮助您基于实际需求做出最优选择。

**学习目标**：
- 掌握需求分析的方法和工具
- 理解资源约束评估的关键指标
- 能够使用决策树进行模型选型
- 通过实际案例学习选型思路

**先修要求**：
- 已阅读"主流视觉大模型概述"
- 已阅读"模型对比与评测"
- 了解您的实际任务需求和资源情况

**难度**：⭐⭐⭐☆☆（中级）  
**预计时间**：60-90分钟

---

## 🎯 选型流程概览

```
┌─────────────────┐
│  1. 需求分析    │ ← 明确任务类型、性能要求、应用场景
└────────┬────────┘
         ↓
┌─────────────────┐
│  2. 资源评估    │ ← 硬件、预算、时间、人力约束
└────────┬────────┘
         ↓
┌─────────────────┐
│  3. 候选筛选    │ ← 根据需求和约束缩小范围
└────────┬────────┘
         ↓
┌─────────────────┐
│  4. 对比验证    │ ← 小规模测试、性能对比
└────────┬────────┘
         ↓
┌─────────────────┐
│  5. 最终决策    │ ← 综合评估，做出选择
└─────────────────┘
```

---

## 📝 第一步：任务需求分析

### 1.1 任务类型识别

使用以下表格明确您的核心任务：

| 任务大类 | 具体任务 | 典型输入/输出 | 推荐模型方向 |
|---------|---------|--------------|-------------|
| **图文理解** | 图像分类、图文检索 | 图像→类别/文本→图像 | CLIP系列 |
| **图像分割** | 语义/实例/全景分割 | 图像+提示→mask | SAM |
| **图像描述** | 自动标注、Alt文本生成 | 图像→描述文本 | BLIP-2 |
| **视觉问答** | 图像内容问答、OCR+问答 | 图像+问题→答案 | LLaVA, Qwen-VL |
| **多模态对话** | 视觉助手、图像推理 | 多轮对话+图像 | LLaVA, GPT-4V |

### 1.2 性能需求定义

根据应用场景确定性能优先级：

**实时性要求**：
- **强实时（<50ms）**：边缘设备、视频流处理 → 优先考虑CLIP-B/32、量化模型
- **准实时（50-200ms）**：在线服务、交互式应用 → BLIP-2、SAM
- **离线批处理（>200ms）**：数据标注、离线分析 → LLaVA、Qwen-VL

**准确率要求**：
- **高精度（>80%）**：医疗、金融、安全领域 → 选择benchmark排名靠前的模型
- **中等精度（70-80%）**：电商、内容推荐 → 平衡性能和速度
- **探索性（<70%）**：原型验证、可行性研究 → 快速迭代优先

### 1.3 特殊需求清单

勾选您的特殊需求：

- [ ] **多语言支持**（中文/日文/韩文等）→ Qwen-VL, InternVL
- [ ] **长文本/OCR**（文档理解、票据识别）→ Qwen-VL, CogVLM
- [ ] **高分辨率图像**（4K+、医学影像）→ InternVL, SAM
- [ ] **视频理解**（帧序列、时序信息）→ 需要额外适配或选择视频模型
- [ ] **隐私保护**（本地部署、数据不出域）→ 开源模型优先
- [ ] **可解释性**（需要attention可视化）→ CLIP, SAM

---

## 💻 第二步：资源约束评估

### 2.1 硬件资源清单

填写您的硬件配置：

```yaml
计算资源:
  GPU型号: ___________ (如: RTX 3090, V100, A100)
  显存大小: ___________ GB
  GPU数量: ___________
  CPU核心数: ___________
  内存大小: ___________ GB

存储资源:
  可用存储: ___________ GB (模型+数据)
  存储类型: ___________ (SSD/HDD/网络存储)

网络环境:
  网络带宽: ___________ Mbps
  是否可访问HuggingFace: [ ] 是 [ ] 否
  是否可访问GitHub: [ ] 是 [ ] 否
```

### 2.2 显存需求估算

根据模型和批处理大小估算显存需求：

| 模型 | FP32 | FP16 | INT8 | 推荐显存 |
|------|:----:|:----:|:----:|:--------:|
| **CLIP-ViT-B/32** | 5GB | 2.5GB | 1.5GB | ≥4GB |
| **CLIP-ViT-L/14** | 10GB | 4.8GB | 2.5GB | ≥8GB |
| **SAM-ViT-B** | 8GB | 4GB | 2GB | ≥8GB |
| **BLIP-2-2.7B** | 14GB | 6.8GB | 3.5GB | ≥8GB |
| **LLaVA-7B** | 28GB | 14GB | 7GB | ≥16GB |
| **Qwen-VL-7B** | 36GB | 18GB | 9GB | ≥20GB |

> 💡 **提示**：
> - 批处理会额外增加显存（Batch=4约增加3-4倍）
> - 推理时显存需求比训练低约30-50%
> - 量化（INT8/INT4）可大幅降低显存需求，性能损失通常<5%

### 2.3 预算与时间约束

**开发预算**：
- **低预算（<1万元）**：使用开源模型+云服务API
- **中预算（1-10万元）**：租用云GPU或购买消费级显卡
- **高预算（>10万元）**：购买专业GPU服务器

**开发时间**：
- **紧急（<1周）**：使用预训练模型直接推理，无微调
- **正常（1-4周）**：少量微调，适配自有数据
- **充裕（>1月）**：深度微调，优化部署

---

## 🔍 第三步：候选模型筛选

### 3.1 决策树（交互式）

按以下流程逐步筛选：

```
Q1: 是否需要中文支持？
├─ 是 → 候选：Qwen-VL, InternVL, CogVLM, Yi-VL
└─ 否 → 继续 Q2

Q2: 显存资源？
├─ <8GB → 候选：CLIP, BLIP-2(小), 量化模型
├─ 8-16GB → 候选：SAM, BLIP-2, LLaVA-7B(量化)
└─ >16GB → 候选：LLaVA-7B/13B, Qwen-VL

Q3: 主要任务类型？
├─ 图文检索/分类 → CLIP
├─ 图像分割 → SAM
├─ 图像描述 → BLIP-2
├─ 视觉问答 → LLaVA, Qwen-VL
└─ 多模态对话 → LLaVA, GPT-4V

Q4: 实时性要求？
├─ 强实时(<50ms) → CLIP-B/32, 量化模型
├─ 准实时(50-200ms) → BLIP-2, SAM
└─ 离线批处理 → 任意模型

Q5: 是否需要本地部署？
├─ 是 → 开源模型（CLIP, SAM, LLaVA, Qwen-VL）
└─ 否 → 可考虑商业API（GPT-4V, Gemini）
```

### 3.2 候选对比表

根据决策树结果，填写候选对比表：

| 模型 | 满足需求 | 资源匹配 | 社区支持 | 综合评分 | 是否入围 |
|------|:-------:|:-------:|:-------:|:-------:|:-------:|
| _________ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐☆ | ⭐⭐⭐⭐⭐ | 14/15 | ✅ |
| _________ | ⭐⭐⭐⭐☆ | ⭐⭐⭐☆☆ | ⭐⭐⭐⭐☆ | 11/15 | ✅ |
| _________ | ⭐⭐⭐☆☆ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐☆☆ | 10/15 | ❌ |

**评分标准**：
- **满足需求**：功能覆盖度、性能指标达标
- **资源匹配**：显存、存储、推理速度符合约束
- **社区支持**：文档完善度、issue响应速度、教程数量

---

## 🧪 第四步：小规模验证

### 4.1 快速验证清单

在最终决策前，进行小规模测试：

**环境验证**（30分钟）：
- [ ] 模型下载成功（测试网络和存储）
- [ ] 成功加载模型（测试显存和兼容性）
- [ ] 运行官方demo（验证基础功能）

**性能验证**（2小时）：
- [ ] 推理速度测试（单图/批量）
- [ ] 显存占用监控（峰值/平均）
- [ ] 准确率评估（小样本测试集）

**易用性验证**（1小时）：
- [ ] API调用是否简洁
- [ ] 文档是否完善
- [ ] 错误处理是否友好

### 4.2 验证脚本模板

```python
import time
import torch
from transformers import AutoModel, AutoProcessor

# 1. 加载模型
model_name = "your-model-name"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Loading model: {model_name}")
model = AutoModel.from_pretrained(model_name).to(device)
processor = AutoProcessor.from_pretrained(model_name)

# 2. 推理速度测试
test_images = ["test1.jpg", "test2.jpg", "test3.jpg"]
start_time = time.time()

for img_path in test_images:
    inputs = processor(images=img_path, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)

elapsed = time.time() - start_time
print(f"平均推理时间: {elapsed/len(test_images)*1000:.2f} ms/image")

# 3. 显存占用监控
if torch.cuda.is_available():
    memory_allocated = torch.cuda.max_memory_allocated() / 1024**3
    print(f"峰值显存占用: {memory_allocated:.2f} GB")

# 4. 输出示例
print(f"模型输出示例: {outputs}")
```

---

## 📊 第五步：最终决策

### 5.1 决策矩阵

综合所有因素，使用加权评分：

| 评估维度 | 权重 | 模型A得分 | 模型B得分 | 模型C得分 |
|---------|:---:|:--------:|:--------:|:--------:|
| **功能匹配度** | 30% | 9/10 | 8/10 | 7/10 |
| **性能指标** | 25% | 8/10 | 9/10 | 7/10 |
| **资源效率** | 20% | 7/10 | 6/10 | 9/10 |
| **开发成本** | 15% | 8/10 | 7/10 | 8/10 |
| **维护成本** | 10% | 7/10 | 8/10 | 6/10 |
| **加权总分** | 100% | **8.05** | **7.85** | **7.55** |

### 5.2 风险评估

在最终决策前，评估潜在风险：

| 风险类型 | 可能性 | 影响 | 缓解措施 |
|---------|:-----:|:---:|---------|
| **显存不足** | 中 | 高 | 使用量化、降低batch size |
| **推理过慢** | 低 | 中 | 使用TensorRT优化、模型蒸馏 |
| **准确率不达标** | 中 | 高 | 微调、数据增强、集成学习 |
| **依赖冲突** | 低 | 低 | 使用虚拟环境、固定版本 |

---

## 📚 实际选型案例

### 案例1：电商图像搜索系统

**需求分析**：
- 任务：图文检索（输入文本查找相似商品图）
- 性能要求：<100ms延迟，准确率>65%
- 数据量：100万商品图
- 特殊需求：支持中文查询

**资源约束**：
- 硬件：4×V100 32GB
- 预算：中等
- 时间：2周POC

**选型结果**：**CLIP-ViT-L/14**（微调中文）

**理由**：
1. ✅ CLIP天然适合图文检索任务
2. ✅ 推理速度~15 img/s，满足实时要求
3. ✅ V100可运行，显存充足
4. ✅ 可通过中文数据微调提升中文理解
5. ✅ 社区成熟，参考案例多

---

### 案例2：医疗影像分割辅助系统

**需求分析**：
- 任务：器官/病灶分割
- 性能要求：mIoU>85%，延迟不限
- 数据量：5000张CT/MRI影像
- 特殊需求：高分辨率(512×512+)、本地部署

**资源约束**：
- 硬件：2×A100 40GB
- 预算：高
- 时间：1个月

**选型结果**：**SAM-ViT-H**

**理由**：
1. ✅ SAM专为分割任务设计
2. ✅ 支持点/框提示，交互式标注
3. ✅ 零样本能力强，减少标注成本
4. ✅ 开源模型，支持本地部署
5. ✅ A100可满足高分辨率推理

---

### 案例3：智能客服图文问答

**需求分析**：
- 任务：根据产品图回答用户问题
- 性能要求：准确率>75%，延迟<300ms
- 数据量：多轮对话，中文为主
- 特殊需求：理解产品细节、OCR能力

**资源约束**：
- 硬件：1×RTX 4090 24GB
- 预算：低
- 时间：紧急（1周）

**选型结果**：**Qwen-VL-Chat（量化版）**

**理由**：
1. ✅ 中文理解能力强
2. ✅ 内置OCR，识别产品文字
3. ✅ 7B量化版可在24GB显存运行
4. ✅ 对话能力好，支持多轮交互
5. ✅ 阿里开源，文档完善

---

## 🎯 实践任务

### 任务：为您的项目制定选型方案

**步骤**：
1. 填写"任务需求分析"表格
2. 填写"资源约束评估"清单
3. 使用决策树筛选3个候选模型
4. 填写"候选对比表"
5. 编写小规模验证计划（可选：实际运行验证脚本）
6. 填写"决策矩阵"，做出最终选择

**交付物**（建议保存为独立文档）：
- `my_model_selection.md`：包含完整的分析过程和决策结果
- `verification_results.txt`：验证脚本的输出结果（可选）

---

## ➡️ 下一步

- [04-基准测试实践](./04-基准测试实践.md) - 动手测试和对比模型
- [../02-模型微调技术/](../02-模型微调技术/) - 微调选定的模型

---

## 📚 参考资源

- [HuggingFace Model Hub](https://huggingface.co/models) - 查看模型详情和benchmark
- [Papers With Code Leaderboard](https://paperswithcode.com/sota) - 各任务的SOTA排行榜
- [OpenCompass](https://opencompass.org.cn/) - 多模态模型评测平台

---

**文档版本**: v1.0  
**最后更新**: 2025-11-01

