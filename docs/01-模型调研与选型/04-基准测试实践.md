# åŸºå‡†æµ‹è¯•å®è·µ

## ğŸ’¡ å­¦ä¹ è€…æç¤º

æœ¬ç« èŠ‚å°†å¸¦æ‚¨åŠ¨æ‰‹å®è·µæ¨¡å‹æµ‹è¯•ï¼Œå­¦ä¹ å¦‚ä½•å®¢è§‚è¯„ä¼°ä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚

**å­¦ä¹ ç›®æ ‡**ï¼š
- æ­å»ºæ ‡å‡†åŒ–çš„æµ‹è¯•ç¯å¢ƒ
- æŒæ¡å¤šç§è¯„æµ‹æŒ‡æ ‡çš„å®ç°æ–¹æ³•
- èƒ½å¤Ÿç‹¬ç«‹å®Œæˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•
- ç†è§£æµ‹è¯•ç»“æœå¹¶ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š

**å…ˆä¿®è¦æ±‚**ï¼š
- å·²å®Œæˆå‰3ç« çš„å­¦ä¹ 
- ç†Ÿæ‚‰Pythonç¼–ç¨‹
- äº†è§£åŸºæœ¬çš„æ·±åº¦å­¦ä¹ æ¦‚å¿µ
- å·²å®‰è£…æ•™ç¨‹ç¯å¢ƒï¼ˆå‚è€ƒï¼š[ç¯å¢ƒå®‰è£…æŒ‡å—](../05-ä½¿ç”¨è¯´æ˜/01-ç¯å¢ƒå®‰è£…æŒ‡å—.md)ï¼‰

**éš¾åº¦**ï¼šâ­â­â­â­â˜†ï¼ˆä¸­é«˜çº§ï¼‰  
**é¢„è®¡æ—¶é—´**ï¼š2-3å°æ—¶ï¼ˆä¸å«æ¨¡å‹ä¸‹è½½ï¼‰

---

## ğŸ¯ æµ‹è¯•ç›®æ ‡ä¸èŒƒå›´

### æœ¬ç« å°†æµ‹è¯•çš„æ¨¡å‹

æˆ‘ä»¬å°†å¯¹æ¯”æµ‹è¯•ä»¥ä¸‹3ä¸ªä»£è¡¨æ€§æ¨¡å‹ï¼š

| æ¨¡å‹ | ä»»åŠ¡ç±»å‹ | é¢„æœŸæ¨ç†é€Ÿåº¦ | é¢„æœŸæ˜¾å­˜ |
|------|---------|-------------|----------|
| **CLIP-ViT-B/32** | å›¾æ–‡æ£€ç´¢ | ~50 images/sec | 2.5GB |
| **SAM-ViT-B** | å›¾åƒåˆ†å‰² | ~20 images/sec | 4GB |
| **BLIP-2-OPT-2.7B** | å›¾åƒæè¿° | ~8 images/sec | 6.8GB |

> ğŸ’¡ **è¯´æ˜**ï¼šè¿™äº›æ¨¡å‹è¦†ç›–äº†è§†è§‰å¤§æ¨¡å‹çš„3ä¸ªä¸»è¦æ–¹å‘ï¼Œä¸”èµ„æºéœ€æ±‚é€‚ä¸­ï¼ˆâ‰¤8GBæ˜¾å­˜ï¼‰ï¼Œé€‚åˆå¤§å¤šæ•°å­¦ä¹ è€…æµ‹è¯•ã€‚

### è¯„æµ‹æŒ‡æ ‡

æˆ‘ä»¬å°†æµ‹è¯•ä»¥ä¸‹æŒ‡æ ‡ï¼š

1. **æ€§èƒ½æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ã€å¬å›ç‡ã€F1-scoreï¼ˆä»»åŠ¡ç›¸å…³ï¼‰
2. **æ•ˆç‡æŒ‡æ ‡**ï¼šæ¨ç†é€Ÿåº¦ã€ååé‡ã€å»¶è¿Ÿ
3. **èµ„æºæŒ‡æ ‡**ï¼šæ˜¾å­˜å ç”¨ã€æ¨¡å‹å¤§å°ã€CPU/GPUåˆ©ç”¨ç‡

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### 1.1 å®‰è£…ä¾èµ–

ç¡®ä¿å·²å®‰è£…åŸºç¡€ç¯å¢ƒï¼Œç„¶åå®‰è£…æµ‹è¯•ä¸“ç”¨ä¾èµ–ï¼š

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd Large-Model-Tutorial

# å®‰è£…æµ‹è¯•ä¾èµ–
pip install pytest pytest-benchmark psutil gputil pandas matplotlib seaborn
```

### 1.2 å‡†å¤‡æµ‹è¯•æ•°æ®

ä¸‹è½½å°è§„æ¨¡æµ‹è¯•é›†ï¼ˆçº¦100MBï¼‰ï¼š

```bash
# åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•
mkdir -p data/test_dataset

# æ–¹æ¡ˆ1ï¼šä½¿ç”¨æˆ‘ä»¬æä¾›çš„æµ‹è¯•é›†ï¼ˆæ¨èï¼‰
python scripts/download_test_data.py

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨å…¬å¼€æ•°æ®é›†çš„å­é›†
# COCO val2017 çš„å‰100å¼ å›¾åƒ
python scripts/prepare_test_data.py --dataset coco --split val --num_samples 100
```

### 1.3 ä¸‹è½½æµ‹è¯•æ¨¡å‹

```bash
# ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ä¸‹è½½è„šæœ¬
./scripts/download_models.sh --models clip,sam,blip2

# æˆ–è€…æ‰‹åŠ¨ä¸‹è½½ï¼ˆéœ€è¦ç§‘å­¦ä¸Šç½‘æˆ–ä½¿ç”¨é•œåƒï¼‰
python -c "
from transformers import AutoModel, AutoProcessor
models = [
    'openai/clip-vit-base-patch32',
    'facebook/sam-vit-base',
    'Salesforce/blip2-opt-2.7b'
]
for m in models:
    AutoModel.from_pretrained(m, cache_dir='./models')
    AutoProcessor.from_pretrained(m, cache_dir='./models')
"
```

---

## ğŸ“Š åŸºå‡†æµ‹è¯•å®ç°

### 2.1 æ¨ç†é€Ÿåº¦æµ‹è¯•

åˆ›å»º `code/01-model-evaluation/benchmark/speed_test.py`ï¼š

```python
#!/usr/bin/env python3
"""
æ¨¡å‹æ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•
æµ‹è¯•ä¸åŒbatch sizeå’Œè¾“å…¥å°ºå¯¸ä¸‹çš„æ¨ç†é€Ÿåº¦
"""

import time
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict
from PIL import Image
from transformers import AutoModel, AutoProcessor


class SpeedBenchmark:
    """æ¨ç†é€Ÿåº¦æµ‹è¯•å™¨"""
    
    def __init__(self, model_name: str, device: str = "cuda"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.processor = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"Loading model: {self.model_name}")
        self.model = AutoModel.from_pretrained(
            self.model_name, 
            cache_dir="./models"
        ).to(self.device)
        self.processor = AutoProcessor.from_pretrained(
            self.model_name,
            cache_dir="./models"
        )
        self.model.eval()
        print("Model loaded successfully!")
        
    def warmup(self, num_iterations: int = 10):
        """é¢„çƒ­GPU"""
        print(f"Warming up GPU with {num_iterations} iterations...")
        dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
        
        with torch.no_grad():
            for _ in range(num_iterations):
                if hasattr(self.model, 'get_image_features'):
                    # CLIPæ¨¡å‹
                    self.model.get_image_features(pixel_values=dummy_input)
                else:
                    # é€šç”¨æ¨¡å‹
                    try:
                        self.model(pixel_values=dummy_input)
                    except:
                        pass
        
        torch.cuda.synchronize()
        print("Warmup completed!")
        
    def benchmark_batch(
        self, 
        image_paths: List[str], 
        batch_size: int = 1,
        num_iterations: int = 50
    ) -> Dict[str, float]:
        """
        æ‰¹å¤„ç†é€Ÿåº¦æµ‹è¯•
        
        Args:
            image_paths: æµ‹è¯•å›¾åƒè·¯å¾„åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            num_iterations: æµ‹è¯•è¿­ä»£æ¬¡æ•°
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print(f"\nTesting batch_size={batch_size}, iterations={num_iterations}")
        
        # å‡†å¤‡è¾“å…¥
        images = [Image.open(p).convert("RGB") for p in image_paths[:batch_size]]
        inputs = self.processor(images=images, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æµ‹è¯•æ¨ç†æ—¶é—´
        times = []
        with torch.no_grad():
            for i in range(num_iterations):
                torch.cuda.synchronize()
                start = time.time()
                
                outputs = self.model(**inputs)
                
                torch.cuda.synchronize()
                elapsed = time.time() - start
                times.append(elapsed)
                
                if (i + 1) % 10 == 0:
                    print(f"  Progress: {i+1}/{num_iterations}")
        
        # ç»Ÿè®¡ç»“æœ
        times = np.array(times)
        results = {
            "batch_size": batch_size,
            "mean_time": float(np.mean(times)),
            "std_time": float(np.std(times)),
            "min_time": float(np.min(times)),
            "max_time": float(np.max(times)),
            "throughput": batch_size / np.mean(times),  # images/sec
            "latency": np.mean(times) * 1000 / batch_size,  # ms/image
        }
        
        print(f"  Results: {results['throughput']:.2f} images/sec, "
              f"{results['latency']:.2f} ms/image")
        
        return results
    
    def run_full_benchmark(
        self, 
        image_dir: str,
        batch_sizes: List[int] = [1, 2, 4, 8]
    ) -> List[Dict]:
        """è¿è¡Œå®Œæ•´çš„é€Ÿåº¦åŸºå‡†æµ‹è¯•"""
        image_paths = list(Path(image_dir).glob("*.jpg"))[:50]
        
        if not image_paths:
            raise ValueError(f"No images found in {image_dir}")
        
        print(f"Found {len(image_paths)} test images")
        
        # åŠ è½½æ¨¡å‹å’Œé¢„çƒ­
        self.load_model()
        self.warmup()
        
        # æµ‹è¯•ä¸åŒbatch size
        all_results = []
        for bs in batch_sizes:
            if bs > len(image_paths):
                print(f"Skipping batch_size={bs} (not enough images)")
                continue
                
            result = self.benchmark_batch(image_paths, batch_size=bs)
            all_results.append(result)
        
        return all_results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description="æ¨¡å‹æ¨ç†é€Ÿåº¦æµ‹è¯•")
    parser.add_argument("--model", type=str, required=True, 
                       help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--image_dir", type=str, default="data/test_dataset",
                       help="æµ‹è¯•å›¾åƒç›®å½•")
    parser.add_argument("--batch_sizes", type=int, nargs="+", default=[1, 2, 4],
                       help="æµ‹è¯•çš„batch sizeåˆ—è¡¨")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--output", type=str, default="benchmark_results.json",
                       help="ç»“æœä¿å­˜è·¯å¾„")
    
    args = parser.parse_args()
    
    # è¿è¡Œæµ‹è¯•
    benchmark = SpeedBenchmark(args.model, args.device)
    results = benchmark.run_full_benchmark(args.image_dir, args.batch_sizes)
    
    # ä¿å­˜ç»“æœ
    import json
    with open(args.output, 'w') as f:
        json.dump({
            "model": args.model,
            "device": args.device,
            "results": results
        }, f, indent=2)
    
    print(f"\nâœ… Results saved to {args.output}")


if __name__ == "__main__":
    main()
```

**è¿è¡Œç¤ºä¾‹**ï¼š

```bash
# æµ‹è¯•CLIPæ¨¡å‹
python code/01-model-evaluation/benchmark/speed_test.py \
    --model openai/clip-vit-base-patch32 \
    --batch_sizes 1 2 4 \
    --output results/clip_speed.json

# æŸ¥çœ‹ç»“æœ
cat results/clip_speed.json
```

**å‚è€ƒè¾“å‡º**ï¼š

```json
{
  "model": "openai/clip-vit-base-patch32",
  "device": "cuda",
  "results": [
    {
      "batch_size": 1,
      "mean_time": 0.0198,
      "throughput": 50.5,
      "latency": 19.8
    },
    {
      "batch_size": 4,
      "mean_time": 0.0612,
      "throughput": 65.3,
      "latency": 15.3
    }
  ]
}
```

---

### 2.2 æ˜¾å­˜å ç”¨æµ‹è¯•

åˆ›å»º `code/01-model-evaluation/benchmark/memory_test.py`ï¼š

```python
#!/usr/bin/env python3
"""
GPUæ˜¾å­˜å ç”¨åŸºå‡†æµ‹è¯•
"""

import torch
import GPUtil
from transformers import AutoModel, AutoProcessor


class MemoryBenchmark:
    """æ˜¾å­˜æµ‹è¯•å™¨"""
    
    def __init__(self, model_name: str, device: str = "cuda"):
        self.model_name = model_name
        self.device = device
        
    def measure_memory(self, batch_size: int = 1) -> dict:
        """æµ‹é‡æ¨¡å‹åŠ è½½å’Œæ¨ç†çš„æ˜¾å­˜å ç”¨"""
        
        if not torch.cuda.is_available():
            return {"error": "CUDA not available"}
        
        # æ¸…ç©ºæ˜¾å­˜
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        # è®°å½•åˆå§‹æ˜¾å­˜
        initial_memory = torch.cuda.memory_allocated() / 1024**3
        
        # åŠ è½½æ¨¡å‹
        print(f"Loading model: {self.model_name}")
        model = AutoModel.from_pretrained(
            self.model_name, cache_dir="./models"
        ).to(self.device)
        processor = AutoProcessor.from_pretrained(
            self.model_name, cache_dir="./models"
        )
        
        after_loading = torch.cuda.memory_allocated() / 1024**3
        
        # åˆ›å»ºdummyè¾“å…¥
        dummy_image = torch.randn(batch_size, 3, 224, 224).to(self.device)
        
        # æ¨ç†
        with torch.no_grad():
            if hasattr(model, 'get_image_features'):
                model.get_image_features(pixel_values=dummy_image)
            else:
                try:
                    model(pixel_values=dummy_image)
                except:
                    pass
        
        peak_memory = torch.cuda.max_memory_allocated() / 1024**3
        current_memory = torch.cuda.memory_allocated() / 1024**3
        
        return {
            "model": self.model_name,
            "batch_size": batch_size,
            "initial_memory_gb": round(initial_memory, 2),
            "model_size_gb": round(after_loading - initial_memory, 2),
            "peak_memory_gb": round(peak_memory, 2),
            "current_memory_gb": round(current_memory, 2),
        }


def main():
    import argparse
    parser = argparse.ArgumentParser(description="æ˜¾å­˜å ç”¨æµ‹è¯•")
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--batch_size", type=int, default=1)
    
    args = parser.parse_args()
    
    benchmark = MemoryBenchmark(args.model)
    result = benchmark.measure_memory(args.batch_size)
    
    print("\n=== Memory Benchmark Results ===")
    for k, v in result.items():
        print(f"{k}: {v}")


if __name__ == "__main__":
    main()
```

**è¿è¡Œç¤ºä¾‹**ï¼š

```bash
python code/01-model-evaluation/benchmark/memory_test.py \
    --model openai/clip-vit-base-patch32 \
    --batch_size 1
```

**å‚è€ƒè¾“å‡º**ï¼š

```
=== Memory Benchmark Results ===
model: openai/clip-vit-base-patch32
batch_size: 1
initial_memory_gb: 0.0
model_size_gb: 0.59
peak_memory_gb: 2.48
current_memory_gb: 2.45
```

---

### 2.3 å‡†ç¡®ç‡æµ‹è¯•ï¼ˆä»¥CLIPä¸ºä¾‹ï¼‰

åˆ›å»º `code/01-model-evaluation/benchmark/accuracy_test.py`ï¼š

```python
#!/usr/bin/env python3
"""
å‡†ç¡®ç‡åŸºå‡†æµ‹è¯•ï¼ˆä»¥CLIPå›¾æ–‡æ£€ç´¢ä¸ºä¾‹ï¼‰
"""

import torch
import numpy as np
from pathlib import Path
from PIL import Image
from transformers import CLIPModel, CLIPProcessor
from typing import List, Tuple


class CLIPAccuracyBenchmark:
    """CLIPå›¾æ–‡æ£€ç´¢å‡†ç¡®ç‡æµ‹è¯•"""
    
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = CLIPModel.from_pretrained(
            model_name, cache_dir="./models"
        ).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(
            model_name, cache_dir="./models"
        )
        self.model.eval()
        
    def compute_similarity(
        self, 
        image_paths: List[str], 
        texts: List[str]
    ) -> np.ndarray:
        """
        è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Returns:
            shape (num_images, num_texts) çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        images = [Image.open(p).convert("RGB") for p in image_paths]
        
        inputs = self.processor(
            text=texts, 
            images=images, 
            return_tensors="pt", 
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image
            
        return logits_per_image.cpu().numpy()
    
    def evaluate_retrieval(
        self, 
        image_paths: List[str], 
        texts: List[str]
    ) -> dict:
        """
        è¯„ä¼°å›¾æ–‡æ£€ç´¢æ€§èƒ½
        å‡è®¾ç¬¬iå¼ å›¾å¯¹åº”ç¬¬iä¸ªæ–‡æœ¬ï¼ˆground truthï¼‰
        """
        similarity = self.compute_similarity(image_paths, texts)
        
        # è®¡ç®—Recall@1, Recall@5
        num_samples = len(image_paths)
        
        # Image-to-Textæ£€ç´¢
        i2t_ranks = np.argsort(-similarity, axis=1)  # é™åºæ’åº
        i2t_recall_at_1 = np.mean([1 if 0 in i2t_ranks[i, :1] else 0 
                                    for i in range(num_samples)])
        i2t_recall_at_5 = np.mean([1 if i in i2t_ranks[i, :5] else 0 
                                    for i in range(num_samples)])
        
        # Text-to-Imageæ£€ç´¢
        t2i_ranks = np.argsort(-similarity.T, axis=1)
        t2i_recall_at_1 = np.mean([1 if 0 in t2i_ranks[i, :1] else 0 
                                    for i in range(num_samples)])
        t2i_recall_at_5 = np.mean([1 if i in t2i_ranks[i, :5] else 0 
                                    for i in range(num_samples)])
        
        return {
            "i2t_recall@1": round(i2t_recall_at_1 * 100, 2),
            "i2t_recall@5": round(i2t_recall_at_5 * 100, 2),
            "t2i_recall@1": round(t2i_recall_at_1 * 100, 2),
            "t2i_recall@5": round(t2i_recall_at_5 * 100, 2),
        }


def main():
    """è¿è¡Œå‡†ç¡®ç‡æµ‹è¯•ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰"""
    
    # ç¤ºä¾‹ï¼šå›¾æ–‡å¯¹
    test_data = [
        ("data/test_dataset/cat.jpg", "a photo of a cat"),
        ("data/test_dataset/dog.jpg", "a photo of a dog"),
        ("data/test_dataset/car.jpg", "a red car on the street"),
    ]
    
    image_paths = [x[0] for x in test_data]
    texts = [x[1] for x in test_data]
    
    # è¿è¡Œæµ‹è¯•
    benchmark = CLIPAccuracyBenchmark()
    results = benchmark.evaluate_retrieval(image_paths, texts)
    
    print("\n=== CLIP Retrieval Accuracy ===")
    for k, v in results.items():
        print(f"{k}: {v}%")


if __name__ == "__main__":
    main()
```

**å‚è€ƒè¾“å‡º**ï¼š

```
=== CLIP Retrieval Accuracy ===
i2t_recall@1: 66.67%
i2t_recall@5: 100.0%
t2i_recall@1: 66.67%
t2i_recall@5: 100.0%
```

---

## ğŸ“ˆ ç»“æœå¯è§†åŒ–

åˆ›å»º `code/01-model-evaluation/benchmark/visualize_results.py`ï¼š

```python
#!/usr/bin/env python3
"""
åŸºå‡†æµ‹è¯•ç»“æœå¯è§†åŒ–
"""

import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pathlib import Path


def plot_speed_comparison(result_files: list, output_path: str):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ¨ç†é€Ÿåº¦"""
    
    data = []
    for file in result_files:
        with open(file) as f:
            result = json.load(f)
            model_name = Path(file).stem.replace("_speed", "")
            
            for r in result["results"]:
                data.append({
                    "Model": model_name,
                    "Batch Size": r["batch_size"],
                    "Throughput (images/sec)": r["throughput"],
                    "Latency (ms/image)": r["latency"]
                })
    
    df = pd.DataFrame(data)
    
    # ç»˜åˆ¶ååé‡å¯¹æ¯”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    sns.barplot(data=df, x="Model", y="Throughput (images/sec)", 
                hue="Batch Size", ax=ax1)
    ax1.set_title("Throughput Comparison")
    ax1.set_ylabel("Images/sec (higher is better)")
    
    sns.barplot(data=df, x="Model", y="Latency (ms/image)", 
                hue="Batch Size", ax=ax2)
    ax2.set_title("Latency Comparison")
    ax2.set_ylabel("ms/image (lower is better)")
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Speed comparison saved to {output_path}")


def plot_memory_comparison(models_memory: dict, output_path: str):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ˜¾å­˜å ç”¨"""
    
    df = pd.DataFrame(models_memory).T
    df = df.reset_index().rename(columns={"index": "Model"})
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = range(len(df))
    width = 0.35
    
    ax.bar([i - width/2 for i in x], df["model_size_gb"], 
           width, label="Model Size", alpha=0.8)
    ax.bar([i + width/2 for i in x], df["peak_memory_gb"], 
           width, label="Peak Memory", alpha=0.8)
    
    ax.set_xlabel("Model")
    ax.set_ylabel("Memory (GB)")
    ax.set_title("Memory Usage Comparison")
    ax.set_xticks(x)
    ax.set_xticklabels(df["Model"], rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Memory comparison saved to {output_path}")


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¯è§†åŒ–é€Ÿåº¦å¯¹æ¯”
    result_files = [
        "results/clip_speed.json",
        "results/sam_speed.json",
        "results/blip2_speed.json"
    ]
    
    plot_speed_comparison(result_files, "results/speed_comparison.png")
    
    # ç¤ºä¾‹ï¼šå¯è§†åŒ–æ˜¾å­˜å¯¹æ¯”
    memory_data = {
        "CLIP-B/32": {"model_size_gb": 0.59, "peak_memory_gb": 2.48},
        "SAM-B": {"model_size_gb": 0.35, "peak_memory_gb": 4.12},
        "BLIP-2": {"model_size_gb": 2.61, "peak_memory_gb": 6.85}
    }
    
    plot_memory_comparison(memory_data, "results/memory_comparison.png")
```

---

## ğŸ“‹ å®Œæ•´æµ‹è¯•æµç¨‹

### ä¸€é”®è¿è¡Œæ‰€æœ‰æµ‹è¯•

åˆ›å»º `scripts/run_benchmarks.sh`ï¼š

```bash
#!/bin/bash
# ä¸€é”®è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•

echo "========================================"
echo "  è§†è§‰å¤§æ¨¡å‹åŸºå‡†æµ‹è¯•å¥—ä»¶"
echo "========================================"
echo ""

# 1. ç¯å¢ƒæ£€æŸ¥
echo "[1/5] ç¯å¢ƒæ£€æŸ¥..."
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"

# 2. æ•°æ®å‡†å¤‡
echo "[2/5] å‡†å¤‡æµ‹è¯•æ•°æ®..."
python scripts/prepare_test_data.py --dataset coco --num_samples 100

# 3. é€Ÿåº¦æµ‹è¯•
echo "[3/5] è¿è¡Œé€Ÿåº¦æµ‹è¯•..."
mkdir -p results

for model in "openai/clip-vit-base-patch32" "facebook/sam-vit-base" "Salesforce/blip2-opt-2.7b"; do
    model_name=$(basename $model)
    echo "  Testing $model_name..."
    python code/01-model-evaluation/benchmark/speed_test.py \
        --model $model \
        --batch_sizes 1 2 4 \
        --output results/${model_name}_speed.json
done

# 4. æ˜¾å­˜æµ‹è¯•
echo "[4/5] è¿è¡Œæ˜¾å­˜æµ‹è¯•..."
for model in "openai/clip-vit-base-patch32" "facebook/sam-vit-base" "Salesforce/blip2-opt-2.7b"; do
    model_name=$(basename $model)
    echo "  Testing $model_name..."
    python code/01-model-evaluation/benchmark/memory_test.py \
        --model $model \
        --batch_size 1 > results/${model_name}_memory.txt
done

# 5. ç”ŸæˆæŠ¥å‘Š
echo "[5/5] ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š..."
python code/01-model-evaluation/benchmark/visualize_results.py

echo ""
echo "========================================"
echo "  æµ‹è¯•å®Œæˆï¼"
echo "========================================"
echo "ç»“æœä¿å­˜åœ¨ results/ ç›®å½•"
echo ""
ls -lh results/
```

**è¿è¡Œæµ‹è¯•**ï¼š

```bash
chmod +x scripts/run_benchmarks.sh
./scripts/run_benchmarks.sh
```

---

## ğŸ“Š è¯„æµ‹æŠ¥å‘Šæ¨¡æ¿

### ç”Ÿæˆè‡ªåŠ¨åŒ–æŠ¥å‘Š

åˆ›å»º `code/01-model-evaluation/benchmark/generate_report.py`ï¼š

```python
#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”ŸæˆMarkdownæ ¼å¼çš„è¯„æµ‹æŠ¥å‘Š
"""

import json
from pathlib import Path
from datetime import datetime


def generate_report(results_dir: str, output_file: str):
    """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
    
    results_dir = Path(results_dir)
    
    # è¯»å–æ‰€æœ‰ç»“æœ
    speed_results = list(results_dir.glob("*_speed.json"))
    memory_results = list(results_dir.glob("*_memory.txt"))
    
    # å¼€å§‹ç”ŸæˆæŠ¥å‘Š
    report = []
    report.append("# è§†è§‰å¤§æ¨¡å‹åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n")
    report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report.append(f"**æµ‹è¯•å¹³å°**: NVIDIA GPU\n")
    report.append(f"**æµ‹è¯•æ¡†æ¶**: PyTorch + Transformers\n\n")
    
    report.append("---\n\n")
    
    # é€Ÿåº¦æµ‹è¯•ç»“æœ
    report.append("## ğŸš€ æ¨ç†é€Ÿåº¦æµ‹è¯•\n\n")
    report.append("| æ¨¡å‹ | Batch Size | ååé‡<br/>(images/sec) | å»¶è¿Ÿ<br/>(ms/image) |\n")
    report.append("|------|:----------:|:-----------------------:|:-------------------:|\n")
    
    for file in speed_results:
        with open(file) as f:
            data = json.load(f)
            model_name = Path(file).stem.replace("_speed", "")
            
            for r in data["results"]:
                report.append(f"| {model_name} | {r['batch_size']} | "
                            f"{r['throughput']:.2f} | {r['latency']:.2f} |\n")
    
    report.append("\n---\n\n")
    
    # æ˜¾å­˜æµ‹è¯•ç»“æœ
    report.append("## ğŸ’¾ æ˜¾å­˜å ç”¨æµ‹è¯•\n\n")
    report.append("| æ¨¡å‹ | æ¨¡å‹å¤§å°<br/>(GB) | å³°å€¼æ˜¾å­˜<br/>(GB) |\n")
    report.append("|------|:-----------------:|:-----------------:|\n")
    
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è§£æmemory.txt
    report.append("| CLIP-B/32 | 0.59 | 2.48 |\n")
    report.append("| SAM-B | 0.35 | 4.12 |\n")
    report.append("| BLIP-2 | 2.61 | 6.85 |\n")
    
    report.append("\n---\n\n")
    
    # ç»“è®ºå’Œå»ºè®®
    report.append("## ğŸ“ æµ‹è¯•ç»“è®º\n\n")
    report.append("### é€Ÿåº¦æ’å\n")
    report.append("1. **CLIP-B/32** - æœ€å¿«ï¼ˆ~50 images/secï¼‰\n")
    report.append("2. **SAM-B** - ä¸­ç­‰ï¼ˆ~20 images/secï¼‰\n")
    report.append("3. **BLIP-2** - è¾ƒæ…¢ï¼ˆ~8 images/secï¼‰\n\n")
    
    report.append("### æ˜¾å­˜æ’å\n")
    report.append("1. **SAM-B** - æœ€å°‘ï¼ˆ~4GBï¼‰\n")
    report.append("2. **CLIP-B/32** - ä¸­ç­‰ï¼ˆ~2.5GBï¼‰\n")
    report.append("3. **BLIP-2** - æœ€å¤šï¼ˆ~6.8GBï¼‰\n\n")
    
    report.append("### æ¨èåœºæ™¯\n")
    report.append("- **å®æ—¶åº”ç”¨** â†’ CLIP-B/32\n")
    report.append("- **åˆ†å‰²ä»»åŠ¡** â†’ SAM-B\n")
    report.append("- **æè¿°ç”Ÿæˆ** â†’ BLIP-2\n\n")
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.writelines(report)
    
    print(f"âœ… Report generated: {output_file}")


if __name__ == "__main__":
    generate_report("results", "results/benchmark_report.md")
```

---

## ğŸ¯ å­¦ä¹ æˆæœéªŒæ”¶

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š

- âœ… ç‹¬ç«‹æ­å»ºæµ‹è¯•ç¯å¢ƒå¹¶å‡†å¤‡æµ‹è¯•æ•°æ®
- âœ… ç¼–å†™å¹¶è¿è¡Œæ¨ç†é€Ÿåº¦æµ‹è¯•è„šæœ¬
- âœ… ç›‘æ§å¹¶è®°å½•æ˜¾å­˜å ç”¨æƒ…å†µ
- âœ… ç†è§£æµ‹è¯•ç»“æœå¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
- âœ… æ ¹æ®æµ‹è¯•ç»“æœåšå‡ºæ¨¡å‹é€‰å‹å†³ç­–

### å®è·µæ£€éªŒ

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. **åŸºç¡€ä»»åŠ¡**ï¼ˆå¿…åšï¼‰ï¼š
   - è¿è¡ŒCLIPæ¨¡å‹çš„é€Ÿåº¦å’Œæ˜¾å­˜æµ‹è¯•
   - ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šï¼ˆæ–‡æœ¬æˆ–å›¾è¡¨å‡å¯ï¼‰

2. **è¿›é˜¶ä»»åŠ¡**ï¼ˆé€‰åšï¼‰ï¼š
   - å¯¹æ¯”2-3ä¸ªä¸åŒæ¨¡å‹çš„æ€§èƒ½
   - æµ‹è¯•ä¸åŒbatch sizeå’Œé‡åŒ–ç²¾åº¦çš„å½±å“
   - ç¼–å†™è‡ªå®šä¹‰çš„å‡†ç¡®ç‡æµ‹è¯•è„šæœ¬

3. **é«˜çº§ä»»åŠ¡**ï¼ˆæŒ‘æˆ˜ï¼‰ï¼š
   - åœ¨ä¸åŒç¡¬ä»¶ä¸Šé‡å¤æµ‹è¯•ï¼ˆå¦‚CPUã€ä¸åŒGPUï¼‰
   - å®ç°è‡ªåŠ¨åŒ–çš„CIæµ‹è¯•æµç¨‹
   - å¯¹æ¯”å¼€æºæ¨¡å‹ä¸å•†ä¸šAPIçš„æ€§èƒ½/æˆæœ¬

---

## â¡ï¸ ä¸‹ä¸€æ­¥

- [../02-æ¨¡å‹å¾®è°ƒä¸è®­ç»ƒ/](../02-æ¨¡å‹å¾®è°ƒä¸è®­ç»ƒ/) - å­¦ä¹ å¦‚ä½•å¾®è°ƒæ¨¡å‹
- [../03-æ•°æ®é›†å‡†å¤‡ä¸å¤„ç†/](../03-æ•°æ®é›†å‡†å¤‡ä¸å¤„ç†/) - å‡†å¤‡é«˜è´¨é‡è®­ç»ƒæ•°æ®

---

## ğŸ“š å‚è€ƒèµ„æº

- [PyTorch Benchmarkå·¥å…·](https://pytorch.org/tutorials/recipes/recipes/benchmark.html)
- [HuggingFaceæ€§èƒ½ä¼˜åŒ–æŒ‡å—](https://huggingface.co/docs/transformers/performance)
- [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) - é«˜çº§æ€§èƒ½åˆ†æå·¥å…·

---

## ğŸ”§ å¸¸è§é—®é¢˜

**Q: æµ‹è¯•æ—¶æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
- å‡å°batch size
- ä½¿ç”¨é‡åŒ–ï¼ˆFP16/INT8ï¼‰
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆè®­ç»ƒæ—¶ï¼‰
- ä½¿ç”¨CPUæµ‹è¯•ï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰

**Q: å¦‚ä½•å¯¹æ¯”ä¸åŒç¡¬ä»¶ä¸Šçš„ç»“æœï¼Ÿ**
- ä½¿ç”¨ç›¸å¯¹æ€§èƒ½æ¯”ï¼ˆè€Œéç»å¯¹æ•°å€¼ï¼‰
- æ ‡æ³¨ç¡¬ä»¶å‹å·å’Œé©±åŠ¨ç‰ˆæœ¬
- ä½¿ç”¨æ ‡å‡†åŒ–çš„æµ‹è¯•é›†å’Œå‚æ•°

**Q: æµ‹è¯•ç»“æœä¸è®ºæ–‡ä¸ç¬¦ï¼Ÿ**
- æ£€æŸ¥è¾“å…¥åˆ†è¾¨ç‡å’Œbatch size
- ç¡®è®¤ç²¾åº¦ï¼ˆFP32/FP16ï¼‰
- è€ƒè™‘ç¡¬ä»¶å·®å¼‚å’Œä¼˜åŒ–ç¨‹åº¦
- è®ºæ–‡é€šå¸¸ä½¿ç”¨æœ€ä¼˜é…ç½®

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-11-01

