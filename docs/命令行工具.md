# å‘½ä»¤è¡Œå·¥å…·å¿«é€Ÿå‚è€ƒ

æœ¬æ–‡æ¡£æä¾›æ‰€æœ‰å‘½ä»¤è¡Œå·¥å…·çš„å¿«é€Ÿå‚è€ƒã€‚

---

## ğŸ“‹ å·¥å…·åˆ—è¡¨

| å·¥å…· | ç”¨é€” | ä½ç½® |
|------|------|------|
| `train.py` | è®­ç»ƒæ¨¡å‹ | `code/02-fine-tuning/*/train.py` |
| `evaluate.py` | è¯„ä¼°æ¨¡å‹ | `code/02-fine-tuning/*/evaluate.py` |
| `inference.py` | æ¨¡å‹æ¨ç† | `code/02-fine-tuning/*/inference.py` |
| `convert_to_onnx.py` | æ¨¡å‹è½¬æ¢ | `code/04-deployment/nvidia/onnx/` |
| `app.py` | APIæœåŠ¡ | `code/04-deployment/api-server/` |
| `setup.sh` | ç¯å¢ƒå®‰è£… | `scripts/` |
| `run_benchmarks.sh` | åŸºå‡†æµ‹è¯• | `scripts/` |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…
```bash
bash scripts/setup.sh
```

### 2. è®­ç»ƒæ¨¡å‹
```bash
# ä½¿ç”¨LoRAçš„é…ç½®æ–‡ä»¶
python code/02-fine-tuning/lora/train.py \
    --config code/02-fine-tuning/lora/config.yaml

# æˆ–ä½¿ç”¨é€šç”¨é…ç½®æ–‡ä»¶
python code/02-fine-tuning/lora/train.py \
    --config configs/base.yaml
```

### 3. è¯„ä¼°æ¨¡å‹
```bash
python code/02-fine-tuning/lora/evaluate.py \
    --model-path checkpoints/best_model.pth \
    --data-dir data/test
```

### 4. å¯åŠ¨APIæœåŠ¡
```bash
python code/04-deployment/api-server/app.py
```

---

## ğŸ’» å¸¸ç”¨å‘½ä»¤

### è®­ç»ƒ

```bash
# LoRAå¾®è°ƒ
python code/02-fine-tuning/lora/train.py \
    --data-dir data/my_dataset \
    --batch-size 64 \
    --epochs 20

# å…¨å‚æ•°å¾®è°ƒ
python code/02-fine-tuning/full-finetuning/train.py \
    --config code/02-fine-tuning/full-finetuning/config.yaml

# SAMå¾®è°ƒ
python code/02-fine-tuning/sam/train.py \
    --task segmentation \
    --data-dir data/segmentation
```

### è¯„ä¼°

```bash
# å®Œæ•´è¯„ä¼°
python code/02-fine-tuning/lora/evaluate.py \
    --model-path checkpoints/best_model.pth \
    --data-dir data/test \
    --metrics accuracy,precision,recall,f1

# å¿«é€Ÿè¯„ä¼°
python code/02-fine-tuning/lora/evaluate.py \
    --model-path checkpoints/best_model.pth \
    --data-dir data/test \
    --metrics accuracy
```

### æ¨ç†

```bash
# å•å›¾æ¨ç†
python code/02-fine-tuning/lora/inference.py \
    test.jpg \
    --model-path checkpoints/best_model.pth

# æ‰¹é‡æ¨ç†
python code/02-fine-tuning/lora/inference.py \
    images/ \
    --model-path checkpoints/best_model.pth \
    --output-file results.json
```

### æ¨¡å‹è½¬æ¢

```bash
# PyTorch â†’ ONNX
python code/04-deployment/nvidia/onnx/convert_to_onnx.py \
    --model-path checkpoints/best_model.pth \
    --output-path models/model.onnx

# ONNX â†’ TensorRT (éœ€è¦trtexec)
trtexec --onnx=models/model.onnx \
        --saveEngine=models/model.trt \
        --fp16
```

### éƒ¨ç½²

```bash
# æœ¬åœ°å¼€å‘
python code/04-deployment/api-server/app.py

# ç”Ÿäº§éƒ¨ç½²ï¼ˆGunicornï¼‰
gunicorn code.04-deployment.api-server.app:app \
    --workers 4 \
    --bind 0.0.0.0:8000 \
    --worker-class uvicorn.workers.UvicornWorker

# Dockeréƒ¨ç½²
docker-compose up -d nvidia-api
```

---

## ğŸ”§ è„šæœ¬å·¥å…·

### setup.sh - ç¯å¢ƒå®‰è£…

```bash
# å®Œæ•´å®‰è£…
bash scripts/setup.sh

# åªå®‰è£…åŸºç¡€ä¾èµ–
bash scripts/setup.sh --basic

# å®‰è£…å¼€å‘ä¾èµ–
bash scripts/setup.sh --dev
```

### download_models.sh - æ¨¡å‹ä¸‹è½½

```bash
# ä¸‹è½½é»˜è®¤æ¨¡å‹
bash scripts/download_models.sh

# ä¸‹è½½ç‰¹å®šæ¨¡å‹
bash scripts/download_models.sh clip-vit-large-patch14

# ä¸‹è½½æ‰€æœ‰æ¨¡å‹
bash scripts/download_models.sh --all
```

### run_benchmarks.sh - åŸºå‡†æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
bash scripts/run_benchmarks.sh

# æŒ‡å®šæ¨¡å‹
bash scripts/run_benchmarks.sh checkpoints/my_model.pth

# æŒ‡å®šæ•°æ®é›†
bash scripts/run_benchmarks.sh \
    checkpoints/my_model.pth \
    data/my_test
```

### prepare_dog_dataset.py - æ•°æ®å‡†å¤‡

```bash
# å‡†å¤‡ç‹—åˆ†ç±»æ•°æ®é›†
python scripts/prepare_dog_dataset.py

# è‡ªå®šä¹‰è¾“å‡ºç›®å½•
python scripts/prepare_dog_dataset.py --output-dir data/my_dogs

# é™åˆ¶æ ·æœ¬æ•°
python scripts/prepare_dog_dataset.py --max-samples 1000
```

---

## ğŸ¨ å®ç”¨æŠ€å·§

### 1. ä½¿ç”¨é…ç½®æ–‡ä»¶

åˆ›å»ºé…ç½®æ–‡ä»¶ `configs/my_experiment.yaml`:
```yaml
model:
  name: "openai/clip-vit-base-patch32"
data:
  train_dir: "data/train"
  batch_size: 64
training:
  epochs: 20
  learning_rate: 2e-5
```

ä½¿ç”¨é…ç½®æ–‡ä»¶:
```bash
# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python code/02-fine-tuning/lora/train.py --config configs/my_experiment.yaml

# æˆ–ä½¿ç”¨é¡¹ç›®æä¾›çš„åŸºç¡€é…ç½®
python code/02-fine-tuning/lora/train.py --config configs/base.yaml

# æˆ–ä½¿ç”¨æ¨¡å—è‡ªå¸¦çš„é…ç½®
python code/02-fine-tuning/lora/train.py --config code/02-fine-tuning/lora/config.yaml
```

### 2. å¤šGPUè®­ç»ƒ

```bash
# ä½¿ç”¨ç‰¹å®šGPU
CUDA_VISIBLE_DEVICES=0,1 python train.py

# ä½¿ç”¨æ‰€æœ‰GPU
python train.py --distributed
```

### 3. åå°è¿è¡Œ

```bash
# ä½¿ç”¨nohup
nohup python train.py > train.log 2>&1 &

# ä½¿ç”¨screen
screen -S training
python train.py
# Ctrl+A, D é€€å‡ºscreen

# ä½¿ç”¨tmux
tmux new -s training
python train.py
# Ctrl+B, D é€€å‡ºtmux
```

### 4. ç›‘æ§è®­ç»ƒ

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f logs/train.log

# ç›‘æ§GPU
watch -n 1 nvidia-smi

# ç›‘æ§TensorBoard
tensorboard --logdir=logs --port=6006
```

### 5. æ‰¹å¤„ç†è„šæœ¬

åˆ›å»ºæ‰¹å¤„ç†è„šæœ¬ `batch_train.sh`:
```bash
#!/bin/bash

# è®­ç»ƒå¤šä¸ªå®éªŒ
for lr in 1e-5 2e-5 5e-5; do
    for bs in 32 64; do
        python train.py \
            --lr $lr \
            --batch-size $bs \
            --output-dir "outputs/lr${lr}_bs${bs}"
    done
done
```

è¿è¡Œ:
```bash
bash batch_train.sh
```

---

## ğŸ› è°ƒè¯•æŠ€å·§

### 1. è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨Pythonè°ƒè¯•
python -m pdb train.py

# è¯¦ç»†æ—¥å¿—
python train.py --log-level DEBUG

# æ‰“å°æ¨¡å‹ç»“æ„
python train.py --print-model
```

### 2. æ€§èƒ½åˆ†æ

```bash
# ä½¿ç”¨cProfile
python -m cProfile -o profile.stats train.py

# æŸ¥çœ‹ç»“æœ
python -c "import pstats; p = pstats.Stats('profile.stats'); p.sort_stats('cumulative'); p.print_stats(20)"

# ä½¿ç”¨PyTorch Profiler
python train.py --profile
```

### 3. å†…å­˜åˆ†æ

```bash
# ä½¿ç”¨memory_profiler
pip install memory_profiler
python -m memory_profiler train.py

# PyTorchå†…å­˜å¿«ç…§
python train.py --memory-snapshot
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ•°æ®åŠ è½½ä¼˜åŒ–

```bash
# å¢åŠ num_workers
python train.py --num-workers 8

# ä½¿ç”¨æŒä¹…åŒ–worker
python train.py --persistent-workers
```

### 2. è®­ç»ƒåŠ é€Ÿ

```bash
# æ··åˆç²¾åº¦è®­ç»ƒ
python train.py --mixed-precision

# æ¢¯åº¦ç´¯ç§¯
python train.py --accumulation-steps 4

# ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
python train.py --compile
```

### 3. æ¨ç†åŠ é€Ÿ

```bash
# ä½¿ç”¨ONNX
python inference.py --model-type onnx

# ä½¿ç”¨TensorRT
python inference.py --model-type tensorrt

# æ‰¹å¤„ç†
python inference.py --batch-size 32
```

---

## ğŸ“Š ç¤ºä¾‹å·¥ä½œæµ

### å®Œæ•´è®­ç»ƒæµç¨‹

```bash
# 1. å‡†å¤‡ç¯å¢ƒ
bash scripts/setup.sh

# 2. å‡†å¤‡æ•°æ®
python scripts/prepare_dog_dataset.py

# 3. è®­ç»ƒæ¨¡å‹
# ä½¿ç”¨åŸºç¡€é…ç½®å¹¶è‡ªå®šä¹‰å‚æ•°
python code/02-fine-tuning/lora/train.py \
    --config configs/base.yaml \
    --data-dir data/dog_dataset \
    --output-dir outputs/dog_exp1

# 4. è¯„ä¼°æ¨¡å‹
python code/02-fine-tuning/lora/evaluate.py \
    --model-path outputs/dog_exp1/best_model.pth \
    --data-dir data/dog_dataset/test \
    --output-file outputs/dog_exp1/eval_results.json

# 5. è½¬æ¢æ¨¡å‹
python code/04-deployment/nvidia/onnx/convert_to_onnx.py \
    --model-path outputs/dog_exp1/best_model.pth \
    --output-path models/dog_classifier.onnx

# 6. éƒ¨ç½²æœåŠ¡
python code/04-deployment/api-server/app.py \
    --model-path models/dog_classifier.onnx \
    --port 8000
```

### å¿«é€Ÿå®éªŒæµç¨‹

```bash
# 1. å°æ•°æ®é›†å¿«é€ŸéªŒè¯
python train.py \
    --data-dir data/small_subset \
    --epochs 5 \
    --batch-size 16

# 2. æ£€æŸ¥è¿‡æ‹Ÿåˆ
python evaluate.py \
    --model-path checkpoints/checkpoint_epoch_5.pth \
    --data-dir data/small_subset

# 3. å¦‚æœæ•ˆæœå¥½ï¼Œä½¿ç”¨å®Œæ•´æ•°æ®é›†
python train.py \
    --data-dir data/full_dataset \
    --epochs 50 \
    --resume checkpoints/checkpoint_epoch_5.pth
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [APIæ–‡æ¡£](./APIæ–‡æ¡£.md) - è¯¦ç»†çš„APIè¯´æ˜
- [ä½¿ç”¨è¯´æ˜](./docs/05-ä½¿ç”¨è¯´æ˜/) - å®Œæ•´çš„ä½¿ç”¨æŒ‡å—
- [æœ€ä½³å®è·µ](./docs/05-ä½¿ç”¨è¯´æ˜/04-æœ€ä½³å®è·µ.md) - æ¨èåšæ³•

---

**æœ€åæ›´æ–°**: 2025-11-05  
**ç‰ˆæœ¬**: v1.0

