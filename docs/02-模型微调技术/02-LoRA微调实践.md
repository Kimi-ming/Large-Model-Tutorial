# 02 - LoRAå¾®è°ƒå®è·µ

> ğŸ“š **å­¦ä¹ ç›®æ ‡**  
> - æŒæ¡ä½¿ç”¨LoRAå¾®è°ƒCLIPæ¨¡å‹çš„å®Œæ•´æµç¨‹
> - å­¦ä¼šå‡†å¤‡å’Œå¤„ç†å¾®è°ƒæ•°æ®
> - ç†è§£è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®å‚æ•°
> - èƒ½å¤Ÿè¯„ä¼°å’Œä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹

> ğŸ¯ **å…ˆä¿®è¦æ±‚**  
> - å®Œæˆ [01-å¾®è°ƒç†è®ºåŸºç¡€](01-å¾®è°ƒç†è®ºåŸºç¡€.md)
> - ç†Ÿæ‚‰PyTorchåŸºç¡€æ“ä½œ
> - äº†è§£æ•°æ®åŠ è½½å’Œé¢„å¤„ç†

> â±ï¸ **é¢„è®¡å­¦ä¹ æ—¶é—´**: 1-2å°æ—¶ï¼ˆå«å®è·µï¼‰  
> ğŸ·ï¸ **éš¾åº¦**: â­â­â­â˜†â˜† ä¸­çº§

> âœ… **ä»£ç å¯ç”¨æ€§**  
> æœ¬æ•™ç¨‹çš„æ‰€æœ‰ç¤ºä¾‹ä»£ç å·²å®Œæ•´å®ç°ï¼Œå¯ç›´æ¥è¿è¡Œï¼š
> - æ•°æ®å‡†å¤‡è„šæœ¬: `scripts/prepare_dog_dataset.py`
> - è®­ç»ƒ/è¯„ä¼°/æ¨ç†: `code/02-fine-tuning/lora/`
> - è¯¦ç»†ä½¿ç”¨è¯´æ˜: `code/02-fine-tuning/lora/README.md`

---

## ğŸ“– ç›®å½•

- [å®è·µæ¦‚è¿°](#å®è·µæ¦‚è¿°)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [æ¨¡å‹é…ç½®](#æ¨¡å‹é…ç½®)
- [è®­ç»ƒè¿‡ç¨‹](#è®­ç»ƒè¿‡ç¨‹)
- [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)
- [æ¨¡å‹ä½¿ç”¨](#æ¨¡å‹ä½¿ç”¨)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [å­¦ä¹ æˆæœéªŒæ”¶](#å­¦ä¹ æˆæœéªŒæ”¶)

---

## å®è·µæ¦‚è¿°

### æœ¬ç« ç›®æ ‡

é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„å®ä¾‹ï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨LoRAå¾®è°ƒCLIPæ¨¡å‹ï¼Œä½¿å…¶åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚å® ç‰©å“ç§è¯†åˆ«ï¼‰ä¸Šè¡¨ç°æ›´å¥½ã€‚

### å®è·µä»»åŠ¡

**ä»»åŠ¡**ï¼šå¾®è°ƒCLIPæ¨¡å‹è¿›è¡Œå® ç‰©å“ç§è¯†åˆ«

**æ•°æ®é›†**ï¼šStanford Dogs Datasetï¼ˆéƒ¨åˆ†ï¼‰
- è®­ç»ƒé›†ï¼š1,000å¼ å›¾åƒï¼ˆ10ä¸ªçŠ¬ç§ï¼‰
- éªŒè¯é›†ï¼š200å¼ å›¾åƒ
- æµ‹è¯•é›†ï¼š200å¼ å›¾åƒ

**é¢„æœŸæ•ˆæœ**ï¼š
- åŸºçº¿ï¼ˆé¢„è®­ç»ƒCLIPï¼‰ï¼šTop-1å‡†ç¡®ç‡ ~60%
- å¾®è°ƒåï¼šTop-1å‡†ç¡®ç‡ ~85%+

### ä»£ç ç»“æ„

```
code/02-fine-tuning/lora/
â”œâ”€â”€ train.py              # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ evaluate.py           # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ inference.py          # æ¨ç†è„šæœ¬
â”œâ”€â”€ dataset.py            # æ•°æ®é›†ç±»
â”œâ”€â”€ config.yaml           # é…ç½®æ–‡ä»¶
â””â”€â”€ README.md             # ä½¿ç”¨è¯´æ˜
```

---

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ç¯å¢ƒï¼ˆå¦‚æœè¿˜æ²¡å®‰è£…ï¼‰
pip install torch torchvision transformers

# LoRAç›¸å…³
pip install peft

# è®­ç»ƒå·¥å…·
pip install accelerate
pip install tensorboard

# æ•°æ®å¤„ç†
pip install pillow
pip install scikit-learn
```

### 2. éªŒè¯å®‰è£…

```python
import torch
import transformers
import peft

print(f"PyTorch: {torch.__version__}")
print(f"Transformers: {transformers.__version__}")
print(f"PEFT: {peft.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

**é¢„æœŸè¾“å‡º**:
```
PyTorch: 2.0.0+cu118
Transformers: 4.35.0
PEFT: 0.7.0
CUDA available: True
```

### 3. ç¡¬ä»¶è¦æ±‚

**æœ€ä½é…ç½®**:
- GPU: 8GBæ˜¾å­˜ï¼ˆå¦‚RTX 3070ï¼‰
- å†…å­˜: 16GB
- ç¡¬ç›˜: 10GB

**æ¨èé…ç½®**:
- GPU: 16GB+æ˜¾å­˜ï¼ˆå¦‚RTX 4080ï¼‰
- å†…å­˜: 32GB
- ç¡¬ç›˜: 20GBï¼ˆSSDï¼‰

---

## æ•°æ®å‡†å¤‡

### 1. æ•°æ®é›†ä¸‹è½½

æˆ‘ä»¬ä½¿ç”¨Stanford Dogs Datasetçš„ä¸€ä¸ªå­é›†ä½œä¸ºç¤ºä¾‹ã€‚

**æ–¹å¼1ï¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆæ¨èï¼‰**

```bash
# è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬ï¼ˆä¼šè‡ªåŠ¨ä¸‹è½½çº¦750MBæ•°æ®ï¼‰
python scripts/prepare_dog_dataset.py --output_dir data/dogs --num_classes 10
```

**è„šæœ¬åŠŸèƒ½**ï¼š
- âœ… è‡ªåŠ¨ä»å®˜æ–¹æºä¸‹è½½Stanford Dogsæ•°æ®é›†ï¼ˆ~750MBï¼‰
- âœ… è§£å‹å¹¶ç»„ç»‡æ•°æ®é›†
- âœ… é€‰æ‹©æŒ‡å®šæ•°é‡çš„çŠ¬ç§ï¼ˆ1-120ä¸ªï¼‰
- âœ… æŒ‰8:2æ¯”ä¾‹åˆ†å‰²è®­ç»ƒ/æµ‹è¯•é›†
- âœ… éªŒè¯æ•°æ®é›†å®Œæ•´æ€§

**â±ï¸ é¢„è®¡æ—¶é—´**ï¼š5-10åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦ï¼‰

**ğŸ’¡ å¦‚æœä¸‹è½½å¤±è´¥**ï¼š
```bash
# æ‰‹åŠ¨ä¸‹è½½æ–¹æ¡ˆ
# 1. è®¿é—® http://vision.stanford.edu/aditya86/ImageNetDogs/
# 2. ä¸‹è½½ images.tar æ–‡ä»¶
# 3. æ”¾åˆ° data/dogs/downloads/ ç›®å½•
# 4. è¿è¡Œè„šæœ¬ï¼ˆè·³è¿‡ä¸‹è½½æ­¥éª¤ï¼‰
python scripts/prepare_dog_dataset.py --output_dir data/dogs --num_classes 10 --no-download
```

**æ–¹å¼2ï¼šæ‰‹åŠ¨å‡†å¤‡**

å¦‚æœæ‚¨æœ‰è‡ªå·±çš„æ•°æ®é›†ï¼ŒæŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

```
data/dogs/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ golden_retriever/
â”‚   â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”‚   â”œâ”€â”€ img_002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ labrador/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â””â”€â”€ ...
```

### 2. æ•°æ®é›†ç±»å®ç°

åˆ›å»º `code/02-fine-tuning/lora/dataset.py`:

```python
import os
from PIL import Image
from torch.utils.data import Dataset
from transformers import CLIPProcessor

class DogBreedDataset(Dataset):
    """
    çŠ¬ç§åˆ†ç±»æ•°æ®é›†
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        split: 'train', 'val', æˆ– 'test'
        processor: CLIPå¤„ç†å™¨
        transform: é¢å¤–çš„å›¾åƒå˜æ¢ï¼ˆå¯é€‰ï¼‰
    """
    def __init__(self, data_dir, split='train', processor=None, transform=None):
        self.data_dir = os.path.join(data_dir, split)
        self.processor = processor
        self.transform = transform
        
        # åŠ è½½ç±»åˆ«å’Œå›¾åƒè·¯å¾„
        self.classes = sorted(os.listdir(self.data_dir))
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}
        
        self.samples = []
        for class_name in self.classes:
            class_dir = os.path.join(self.data_dir, class_name)
            for img_name in os.listdir(class_dir):
                img_path = os.path.join(class_dir, img_name)
                self.samples.append((img_path, self.class_to_idx[class_name]))
        
        print(f"Loaded {len(self.samples)} images from {split} set")
        print(f"Classes: {self.classes}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        # åº”ç”¨é¢å¤–å˜æ¢
        if self.transform:
            image = self.transform(image)
        
        # ä½¿ç”¨CLIP processorå¤„ç†
        if self.processor:
            inputs = self.processor(images=image, return_tensors="pt")
            # ç§»é™¤batchç»´åº¦
            pixel_values = inputs['pixel_values'].squeeze(0)
        else:
            pixel_values = image
        
        return {
            'pixel_values': pixel_values,
            'labels': label
        }
```

### 3. æ•°æ®åŠ è½½å™¨

```python
from torch.utils.data import DataLoader
from transformers import CLIPProcessor

# åˆå§‹åŒ–processor
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# åˆ›å»ºæ•°æ®é›†
train_dataset = DogBreedDataset(
    data_dir='data/dogs',
    split='train',
    processor=processor
)

val_dataset = DogBreedDataset(
    data_dir='data/dogs',
    split='val',
    processor=processor
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=4,
    pin_memory=True
)
```

---

## æ¨¡å‹é…ç½®

### 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
from transformers import CLIPModel, CLIPProcessor
import torch

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model_name = "openai/clip-vit-base-patch32"
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# ç§»åŠ¨åˆ°GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print(f"Model loaded on {device}")
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
```

### 2. é…ç½®LoRA

```python
from peft import LoraConfig, get_peft_model

# LoRAé…ç½®
lora_config = LoraConfig(
    r=8,                          # LoRAç§©ï¼ˆrankï¼‰
    lora_alpha=32,                # LoRAç¼©æ”¾å› å­
    target_modules=[              # ç›®æ ‡æ¨¡å—
        "q_proj",                 # QueryæŠ•å½±
        "v_proj",                 # ValueæŠ•å½±
    ],
    lora_dropout=0.1,             # Dropoutæ¦‚ç‡
    bias="none",                  # ä¸è®­ç»ƒbias
    task_type="FEATURE_EXTRACTION"  # ä»»åŠ¡ç±»å‹
)

# åº”ç”¨LoRA
model = get_peft_model(model, lora_config)

# æ‰“å°å¯è®­ç»ƒå‚æ•°
model.print_trainable_parameters()
```

**é¢„æœŸè¾“å‡º**:
```
trainable params: 294,912 || all params: 149,620,224 || trainable%: 0.20%
```

### 3. æ·»åŠ åˆ†ç±»å¤´

ç”±äºCLIPåŸæœ¬ä¸æ˜¯åˆ†ç±»æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼š

```python
import torch.nn as nn

class CLIPClassifier(nn.Module):
    """
    CLIP + åˆ†ç±»å¤´
    """
    def __init__(self, clip_model, num_classes):
        super().__init__()
        self.clip_model = clip_model
        self.classifier = nn.Linear(
            clip_model.config.projection_dim,  # CLIPè¾“å‡ºç»´åº¦
            num_classes
        )
    
    def forward(self, pixel_values):
        # è·å–å›¾åƒç‰¹å¾
        image_features = self.clip_model.get_image_features(pixel_values=pixel_values)
        
        # åˆ†ç±»
        logits = self.classifier(image_features)
        return logits

# åˆ›å»ºåˆ†ç±»å™¨
num_classes = len(train_dataset.classes)
classifier = CLIPClassifier(model, num_classes).to(device)

print(f"Classifier created for {num_classes} classes")
```

---

## è®­ç»ƒè¿‡ç¨‹

### 1. è®­ç»ƒé…ç½®

```python
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import torch.nn.functional as F

# ä¼˜åŒ–å™¨
optimizer = AdamW(
    classifier.parameters(),
    lr=5e-4,              # LoRAé€šå¸¸ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
    weight_decay=0.01
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
num_epochs = 10
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=num_epochs * len(train_loader)
)

# æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss()
```

### 2. è®­ç»ƒå¾ªç¯

```python
from tqdm import tqdm
import numpy as np

def train_epoch(model, train_loader, optimizer, scheduler, criterion, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc="Training")
    for batch in pbar:
        # æ•°æ®ç§»åŠ¨åˆ°GPU
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)
        
        # å‰å‘ä¼ æ’­
        logits = model(pixel_values)
        loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        _, predicted = logits.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{100.*correct/total:.2f}%'
        })
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy

def validate(model, val_loader, criterion, device):
    """éªŒè¯"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validating"):
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels'].to(device)
            
            logits = model(pixel_values)
            loss = criterion(logits, labels)
            
            total_loss += loss.item()
            _, predicted = logits.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy
```

### 3. å®Œæ•´è®­ç»ƒè„šæœ¬

```python
# è®­ç»ƒ
best_acc = 0
for epoch in range(num_epochs):
    print(f"\nEpoch {epoch+1}/{num_epochs}")
    print("-" * 50)
    
    # è®­ç»ƒ
    train_loss, train_acc = train_epoch(
        classifier, train_loader, optimizer, scheduler, criterion, device
    )
    
    # éªŒè¯
    val_loss, val_acc = validate(
        classifier, val_loader, criterion, device
    )
    
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_acc:
        best_acc = val_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': classifier.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
        }, 'best_model.pth')
        print(f"âœ… Best model saved! (Val Acc: {val_acc:.2f}%)")

print(f"\nğŸ‰ Training completed! Best Val Acc: {best_acc:.2f}%")
```

### 4. è®­ç»ƒç›‘æ§ï¼ˆä½¿ç”¨TensorBoardï¼‰

```python
from torch.utils.tensorboard import SummaryWriter

# åˆ›å»ºwriter
writer = SummaryWriter('runs/lora_finetuning')

# åœ¨è®­ç»ƒå¾ªç¯ä¸­è®°å½•
writer.add_scalar('Loss/train', train_loss, epoch)
writer.add_scalar('Loss/val', val_loss, epoch)
writer.add_scalar('Accuracy/train', train_acc, epoch)
writer.add_scalar('Accuracy/val', val_acc, epoch)
writer.add_scalar('Learning_rate', optimizer.param_groups[0]['lr'], epoch)

# å…³é—­writer
writer.close()
```

**æŸ¥çœ‹TensorBoard**:
```bash
tensorboard --logdir=runs
```

---

## æ¨¡å‹è¯„ä¼°

### 1. åŠ è½½æœ€ä½³æ¨¡å‹

```python
# åŠ è½½checkpoint
checkpoint = torch.load('best_model.pth')
classifier.load_state_dict(checkpoint['model_state_dict'])
print(f"Loaded model from epoch {checkpoint['epoch']} with val_acc {checkpoint['val_acc']:.2f}%")
```

### 2. æµ‹è¯•é›†è¯„ä¼°

```python
# åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
test_dataset = DogBreedDataset(
    data_dir='data/dogs',
    split='test',
    processor=processor
)

test_loader = DataLoader(
    test_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=4
)

# è¯„ä¼°
test_loss, test_acc = validate(classifier, test_loader, criterion, device)
print(f"\nğŸ“Š Test Results:")
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.2f}%")
```

### 3. è¯¦ç»†è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def detailed_evaluation(model, test_loader, class_names, device):
    """è¯¦ç»†è¯„ä¼°"""
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels'].to(device)
            
            logits = model(pixel_values)
            _, predicted = logits.max(1)
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“ˆ Classification Report:")
    print(classification_report(
        all_labels, all_preds,
        target_names=class_names,
        digits=4
    ))
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        cm, annot=True, fmt='d',
        xticklabels=class_names,
        yticklabels=class_names,
        cmap='Blues'
    )
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300)
    print("âœ… Confusion matrix saved to confusion_matrix.png")

# è¿è¡Œè¯¦ç»†è¯„ä¼°
detailed_evaluation(classifier, test_loader, test_dataset.classes, device)
```

---

## æ¨¡å‹ä½¿ç”¨

### 1. ä¿å­˜æ¨¡å‹

```python
# ä¿å­˜LoRAæƒé‡ï¼ˆæ¨èï¼‰
model.save_pretrained("./lora_weights")

# æˆ–ä¿å­˜å®Œæ•´æ¨¡å‹
torch.save(classifier.state_dict(), "classifier_full.pth")
```

### 2. åŠ è½½å’Œæ¨ç†

```python
from PIL import Image

def predict_single_image(image_path, model, processor, class_names, device):
    """
    å¯¹å•å¼ å›¾åƒè¿›è¡Œé¢„æµ‹
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        model: æ¨¡å‹
        processor: CLIPå¤„ç†å™¨
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        device: è®¾å¤‡
    
    Returns:
        predicted_class: é¢„æµ‹ç±»åˆ«
        confidence: ç½®ä¿¡åº¦
    """
    # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    inputs = processor(images=image, return_tensors="pt")
    pixel_values = inputs['pixel_values'].to(device)
    
    # æ¨ç†
    model.eval()
    with torch.no_grad():
        logits = model(pixel_values)
        probs = F.softmax(logits, dim=1)
        confidence, predicted = probs.max(1)
    
    predicted_class = class_names[predicted.item()]
    confidence = confidence.item()
    
    return predicted_class, confidence

# ç¤ºä¾‹ä½¿ç”¨
image_path = "data/dogs/test/golden_retriever/test_001.jpg"
predicted_class, confidence = predict_single_image(
    image_path, classifier, processor, test_dataset.classes, device
)

print(f"Predicted: {predicted_class}")
print(f"Confidence: {confidence:.2%}")
```

### 3. æ‰¹é‡æ¨ç†

```python
def batch_inference(image_dir, model, processor, class_names, device):
    """æ‰¹é‡æ¨ç†"""
    import glob
    
    image_paths = glob.glob(os.path.join(image_dir, "*.jpg"))
    results = []
    
    for img_path in tqdm(image_paths, desc="Inference"):
        pred_class, confidence = predict_single_image(
            img_path, model, processor, class_names, device
        )
        results.append({
            'image': os.path.basename(img_path),
            'prediction': pred_class,
            'confidence': confidence
        })
    
    return results

# ä½¿ç”¨
results = batch_inference(
    "data/dogs/test/golden_retriever",
    classifier, processor, test_dataset.classes, device
)

# ä¿å­˜ç»“æœ
import json
with open('inference_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

---

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°batch size
2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
3. ä½¿ç”¨QLoRAï¼ˆ4bité‡åŒ–ï¼‰
4. å‡å°LoRA rank (r)

```python
# æ¢¯åº¦ç´¯ç§¯ç¤ºä¾‹
accumulation_steps = 4
for i, batch in enumerate(train_loader):
    loss = ...
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### Q2: è®­ç»ƒè¿‡æ‹Ÿåˆæ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ æ•°æ®å¢å¼º
2. å¢åŠ Dropout
3. å‡å°‘è®­ç»ƒè½®æ•°
4. ä½¿ç”¨Early Stopping

```python
# Early Stoppingç¤ºä¾‹
patience = 3
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(...)
    val_loss, val_acc = validate(...)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # ä¿å­˜æ¨¡å‹
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping!")
            break
```

### Q3: å¦‚ä½•è°ƒæ•´è¶…å‚æ•°ï¼Ÿ

**æ¨èè°ƒæ•´é¡ºåº**:
1. å­¦ä¹ ç‡ï¼ˆæœ€é‡è¦ï¼‰ï¼š1e-5 ~ 1e-3
2. Batch sizeï¼š8, 16, 32, 64
3. LoRA rankï¼š4, 8, 16, 32
4. LoRA alphaï¼šé€šå¸¸è®¾ä¸ºrankçš„2-4å€

### Q4: è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

**å‚è€ƒæ—¶é—´**ï¼ˆ1000å¼ å›¾åƒï¼Œ10ä¸ªç±»åˆ«ï¼‰:
- RTX 3090ï¼ˆ24GBï¼‰: ~30åˆ†é’Ÿ
- RTX 4080ï¼ˆ16GBï¼‰: ~40åˆ†é’Ÿ
- V100ï¼ˆ16GBï¼‰: ~45åˆ†é’Ÿ

---

## å­¦ä¹ æˆæœéªŒæ”¶

### ğŸ“‹ å®è·µæ£€æŸ¥æ¸…å•

- [ ] æˆåŠŸå®‰è£…æ‰€æœ‰ä¾èµ–
- [ ] å‡†å¤‡å¥½è®­ç»ƒæ•°æ®ï¼ˆè‡³å°‘100å¼ å›¾åƒï¼‰
- [ ] æˆåŠŸé…ç½®LoRAå¹¶æ‰“å°å¯è®­ç»ƒå‚æ•°
- [ ] å®Œæˆè‡³å°‘3ä¸ªepochçš„è®­ç»ƒ
- [ ] éªŒè¯é›†å‡†ç¡®ç‡è¾¾åˆ°åˆç†æ°´å¹³ï¼ˆ>60%ï¼‰
- [ ] æˆåŠŸä¿å­˜å’ŒåŠ è½½æ¨¡å‹
- [ ] èƒ½å¯¹æ–°å›¾åƒè¿›è¡Œæ¨ç†

### ğŸ¯ è¿›é˜¶æŒ‘æˆ˜

- [ ] å°è¯•ä¸åŒçš„LoRAé…ç½®ï¼ˆrank, alphaï¼‰
- [ ] æ·»åŠ æ•°æ®å¢å¼ºå¹¶è§‚å¯Ÿæ•ˆæœ
- [ ] ä½¿ç”¨TensorBoardç›‘æ§è®­ç»ƒ
- [ ] å®ç°Early Stopping
- [ ] åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒ

### ğŸ“Š é¢„æœŸç»“æœ

**è®­ç»ƒæ›²çº¿**:
- è®­ç»ƒæŸå¤±ï¼šç¨³å®šä¸‹é™
- éªŒè¯æŸå¤±ï¼šå…ˆä¸‹é™åè¶‹äºå¹³ç¨³
- è®­ç»ƒå‡†ç¡®ç‡ï¼šé€æ­¥æå‡è‡³90%+
- éªŒè¯å‡†ç¡®ç‡ï¼šæå‡è‡³80-90%

**æ€§èƒ½æå‡**:
- åŸºçº¿ï¼ˆé¢„è®­ç»ƒCLIPï¼‰ï¼š~60%
- å¾®è°ƒåï¼š~85%+
- æå‡ï¼š+25ä¸ªç™¾åˆ†ç‚¹

---

## ä¸‹ä¸€æ­¥

æ­å–œå®ŒæˆLoRAå¾®è°ƒå®è·µï¼æ¥ä¸‹æ¥æ‚¨å¯ä»¥ï¼š

1. **å­¦ä¹ å…¨å‚æ•°å¾®è°ƒ** â†’ [03-å…¨å‚æ•°å¾®è°ƒ](03-å…¨å‚æ•°å¾®è°ƒ.md)
2. **æ¢ç´¢QLoRA** â†’ [04-å…¶ä»–PEFTæ–¹æ³•](04-å…¶ä»–PEFTæ–¹æ³•.md)
3. **å‡†å¤‡éƒ¨ç½²** â†’ [../04-å¤šå¹³å°éƒ¨ç½²/01-NVIDIAå¹³å°éƒ¨ç½².md](../04-å¤šå¹³å°éƒ¨ç½²/01-NVIDIAå¹³å°éƒ¨ç½².md)

---

## å‚è€ƒèµ„æº

### ä»£ç 

- å®Œæ•´è®­ç»ƒè„šæœ¬ï¼š`code/02-fine-tuning/lora/train.py`
- è¯„ä¼°è„šæœ¬ï¼š`code/02-fine-tuning/lora/evaluate.py`
- æ¨ç†è„šæœ¬ï¼š`code/02-fine-tuning/lora/inference.py`

### æ–‡æ¡£

- [PEFT Documentation](https://huggingface.co/docs/peft)
- [Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)

---

**ğŸ“ æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**âœï¸ æœ€åæ›´æ–°**: 2025-11-01  
**ğŸ‘¥ è´¡çŒ®è€…**: Large-Model-Tutorial Team

