# 02 - æ•°æ®é¢„å¤„ç†æ–¹æ³•

> ğŸ“š **å­¦ä¹ ç›®æ ‡**  
> - æŒæ¡å¸¸ç”¨çš„æ•°æ®é¢„å¤„ç†æŠ€æœ¯
> - äº†è§£ä¸åŒé¢„å¤„ç†æ–¹æ³•çš„é€‚ç”¨åœºæ™¯
> - å­¦ä¼šä½¿ç”¨PyTorchå’ŒTransformersè¿›è¡Œæ•°æ®é¢„å¤„ç†

> ğŸ¯ **å…ˆä¿®è¦æ±‚**  
> - å®Œæˆ [01-å¸¸ç”¨æ•°æ®é›†ä»‹ç»](./01-å¸¸ç”¨æ•°æ®é›†ä»‹ç».md)
> - ç†Ÿæ‚‰Pythonå’ŒPyTorchåŸºç¡€

> â±ï¸ **é¢„è®¡å­¦ä¹ æ—¶é—´**: 45-60åˆ†é’Ÿ  
> ğŸ·ï¸ **éš¾åº¦**: â­â­â­â˜†â˜† ä¸­çº§

---

## ğŸ“– ç›®å½•

- [ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®é¢„å¤„ç†](#ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®é¢„å¤„ç†)
- [å›¾åƒé¢„å¤„ç†](#å›¾åƒé¢„å¤„ç†)
- [æ–‡æœ¬é¢„å¤„ç†](#æ–‡æœ¬é¢„å¤„ç†)
- [å¤šæ¨¡æ€é¢„å¤„ç†](#å¤šæ¨¡æ€é¢„å¤„ç†)
- [é¢„å¤„ç†æµæ°´çº¿](#é¢„å¤„ç†æµæ°´çº¿)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®é¢„å¤„ç†

### ä¸»è¦ç›®çš„

1. **æ ‡å‡†åŒ–è¾“å…¥**
   - ç»Ÿä¸€å›¾åƒå°ºå¯¸
   - å½’ä¸€åŒ–åƒç´ å€¼
   - ç»Ÿä¸€æ•°æ®æ ¼å¼

2. **æå‡æ¨¡å‹æ€§èƒ½**
   - åŠ é€Ÿæ”¶æ•›
   - æé«˜å‡†ç¡®ç‡
   - å¢å¼ºæ³›åŒ–èƒ½åŠ›

3. **é€‚é…æ¨¡å‹è¦æ±‚**
   - åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹çš„è¾“å…¥æ ¼å¼
   - æ»¡è¶³ç‰¹å®šçš„æ•°æ®åˆ†å¸ƒ

### é¢„å¤„ç† vs æ•°æ®å¢å¼º

| ç‰¹æ€§ | é¢„å¤„ç† | æ•°æ®å¢å¼º |
|------|--------|---------|
| **ç›®çš„** | æ ‡å‡†åŒ–è¾“å…¥ | æ‰©å……æ•°æ®ã€æå‡æ³›åŒ– |
| **æ—¶æœº** | è®­ç»ƒ+æ¨ç† | ä»…è®­ç»ƒ |
| **æ“ä½œ** | å›ºå®šçš„ | éšæœºçš„ |
| **ç¤ºä¾‹** | Resize, Normalize | RandomCrop, ColorJitter |

---

## å›¾åƒé¢„å¤„ç†

### 1. å°ºå¯¸è°ƒæ•´ï¼ˆResizeï¼‰

**ç›®çš„**: ç»Ÿä¸€å›¾åƒå°ºå¯¸ï¼Œæ»¡è¶³æ¨¡å‹è¾“å…¥è¦æ±‚

**æ–¹æ³•**:

```python
from torchvision import transforms
from PIL import Image

# æ–¹æ³•1: ç›´æ¥ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
resize = transforms.Resize((224, 224))

# æ–¹æ³•2: ä¿æŒå®½é«˜æ¯”ï¼ŒçŸ­è¾¹ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
resize = transforms.Resize(224)

# æ–¹æ³•3: ä¿æŒå®½é«˜æ¯”ï¼Œé•¿è¾¹ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
resize = transforms.Resize(224, max_size=256)

# ä½¿ç”¨
image = Image.open('dog.jpg')
resized_image = resize(image)
```

**é€‰æ‹©å»ºè®®**:

- **åˆ†ç±»ä»»åŠ¡**: ç›´æ¥ç¼©æ”¾ `Resize((224, 224))`
- **æ£€æµ‹ä»»åŠ¡**: ä¿æŒå®½é«˜æ¯” `Resize(800, max_size=1333)`
- **åˆ†å‰²ä»»åŠ¡**: ç›´æ¥ç¼©æ”¾æˆ–Pad

### 2. ä¸­å¿ƒè£å‰ªï¼ˆCenterCropï¼‰

**ç›®çš„**: ä»å›¾åƒä¸­å¿ƒè£å‰ªå›ºå®šå°ºå¯¸

```python
# ä»ä¸­å¿ƒè£å‰ª224Ã—224åŒºåŸŸ
center_crop = transforms.CenterCrop(224)

# å¸¸ä¸Resizeé…åˆä½¿ç”¨
transform = transforms.Compose([
    transforms.Resize(256),      # çŸ­è¾¹ç¼©æ”¾åˆ°256
    transforms.CenterCrop(224),  # ä¸­å¿ƒè£å‰ª224Ã—224
])
```

**é€‚ç”¨åœºæ™¯**:
- âœ… æ¨ç†é˜¶æ®µï¼ˆç¡®å®šæ€§ï¼‰
- âœ… ä¸»ä½“å±…ä¸­çš„å›¾åƒ
- âŒ è®­ç»ƒé˜¶æ®µï¼ˆæ¨èRandomCropï¼‰

### 3. å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰

**ç›®çš„**: å°†åƒç´ å€¼æ ‡å‡†åŒ–åˆ°ç‰¹å®šåˆ†å¸ƒ

**åŸç†**:
```
normalized = (image - mean) / std
```

**å¸¸ç”¨å‚æ•°**:

```python
# ImageNeté¢„è®­ç»ƒæ¨¡å‹çš„æ ‡å‡†å‚æ•°
normalize = transforms.Normalize(
    mean=[0.485, 0.456, 0.406],  # RGBä¸‰é€šé“å‡å€¼
    std=[0.229, 0.224, 0.225]    # RGBä¸‰é€šé“æ ‡å‡†å·®
)

# CLIPæ¨¡å‹çš„å‚æ•°
normalize = transforms.Normalize(
    mean=[0.48145466, 0.4578275, 0.40821073],
    std=[0.26862954, 0.26130258, 0.27577711]
)
```

**ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿ**

1. **åŠ é€Ÿæ”¶æ•›**: å‡å°‘æ¢¯åº¦æ›´æ–°çš„éœ‡è¡
2. **æ•°å€¼ç¨³å®š**: é¿å…æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
3. **åŒ¹é…é¢„è®­ç»ƒ**: ä½¿ç”¨ä¸é¢„è®­ç»ƒç›¸åŒçš„åˆ†å¸ƒ

**æ³¨æ„äº‹é¡¹**:
- âš ï¸ å½’ä¸€åŒ–å‰éœ€è¦å…ˆè½¬æ¢ä¸ºTensor
- âš ï¸ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ç›¸åŒçš„å½’ä¸€åŒ–å‚æ•°

### 4. è½¬æ¢ä¸ºTensor

**ç›®çš„**: å°†PIL Imageæˆ–numpyæ•°ç»„è½¬æ¢ä¸ºPyTorch Tensor

```python
to_tensor = transforms.ToTensor()

# PIL Image (HÃ—WÃ—C, [0, 255]) â†’ Tensor (CÃ—HÃ—W, [0.0, 1.0])
image = Image.open('dog.jpg')  # (480, 640, 3), uint8
tensor = to_tensor(image)      # (3, 480, 640), float32, [0.0, 1.0]
```

**è‡ªåŠ¨å®Œæˆçš„æ“ä½œ**:
1. é€šé“é¡ºåº: HWC â†’ CHW
2. æ•°æ®ç±»å‹: uint8 â†’ float32
3. å€¼èŒƒå›´: [0, 255] â†’ [0.0, 1.0]

### 5. å®Œæ•´çš„å›¾åƒé¢„å¤„ç†æµæ°´çº¿

**è®­ç»ƒé˜¶æ®µ**:
```python
from torchvision import transforms

train_transform = transforms.Compose([
    transforms.Resize(256),                    # çŸ­è¾¹ç¼©æ”¾
    transforms.RandomCrop(224),                # éšæœºè£å‰ªï¼ˆæ•°æ®å¢å¼ºï¼‰
    transforms.RandomHorizontalFlip(p=0.5),    # éšæœºæ°´å¹³ç¿»è½¬
    transforms.ToTensor(),                     # è½¬æ¢ä¸ºTensor
    transforms.Normalize(                      # å½’ä¸€åŒ–
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])
```

**æ¨ç†é˜¶æ®µ**:
```python
val_transform = transforms.Compose([
    transforms.Resize(256),                    # çŸ­è¾¹ç¼©æ”¾
    transforms.CenterCrop(224),                # ä¸­å¿ƒè£å‰ªï¼ˆç¡®å®šæ€§ï¼‰
    transforms.ToTensor(),                     # è½¬æ¢ä¸ºTensor
    transforms.Normalize(                      # å½’ä¸€åŒ–
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])
```

---

## æ–‡æœ¬é¢„å¤„ç†

### 1. åˆ†è¯ï¼ˆTokenizationï¼‰

**ç›®çš„**: å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„tokenåºåˆ—

**ä½¿ç”¨Transformersåº“**:

```python
from transformers import CLIPTokenizer

# åŠ è½½åˆ†è¯å™¨
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

# åˆ†è¯
text = "A dog playing in the park"
tokens = tokenizer(
    text,
    padding='max_length',      # å¡«å……åˆ°æœ€å¤§é•¿åº¦
    max_length=77,              # CLIPçš„æœ€å¤§é•¿åº¦
    truncation=True,            # è¶…é•¿æˆªæ–­
    return_tensors='pt'         # è¿”å›PyTorch Tensor
)

print(tokens['input_ids'])      # Token IDs
print(tokens['attention_mask']) # æ³¨æ„åŠ›æ©ç 
```

**è¾“å‡º**:
```python
{
    'input_ids': tensor([[49406,   320,  1929, ...ï¼Œ49407]]),  # [CLS] ... [SEP]
    'attention_mask': tensor([[1, 1, 1, ..., 0, 0, 0]])        # 1=çœŸå®token, 0=padding
}
```

### 2. å¡«å……ï¼ˆPaddingï¼‰

**ç›®çš„**: å°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦

**ç­–ç•¥**:

```python
# æ–¹æ³•1: å¡«å……åˆ°æœ€å¤§é•¿åº¦
tokens = tokenizer(texts, padding='max_length', max_length=77)

# æ–¹æ³•2: å¡«å……åˆ°æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•¿åº¦ï¼ˆèŠ‚çœè®¡ç®—ï¼‰
tokens = tokenizer(texts, padding='longest')

# æ–¹æ³•3: ä¸å¡«å……ï¼ˆéœ€è¦æ‰‹åŠ¨å¤„ç†ï¼‰
tokens = tokenizer(texts, padding=False)
```

### 3. æˆªæ–­ï¼ˆTruncationï¼‰

**ç›®çš„**: å¤„ç†è¶…è¿‡æœ€å¤§é•¿åº¦çš„æ–‡æœ¬

```python
# å¯ç”¨æˆªæ–­
tokens = tokenizer(
    very_long_text,
    max_length=77,
    truncation=True,           # è¶…é•¿éƒ¨åˆ†ä¼šè¢«æˆªæ–­
    truncation_strategy='longest_first'  # æˆªæ–­ç­–ç•¥
)
```

**æˆªæ–­ç­–ç•¥**:
- `longest_first`: ä»æœ€é•¿çš„åºåˆ—å¼€å§‹æˆªæ–­
- `only_first`: åªæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—
- `only_second`: åªæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—

### 4. ç‰¹æ®ŠToken

**å¸¸è§ç‰¹æ®ŠToken**:

| Token | ç”¨é€” | ç¤ºä¾‹ |
|-------|------|------|
| `[CLS]` / `<|startoftext|>` | åºåˆ—å¼€å§‹ | BERT / CLIP |
| `[SEP]` / `<|endoftext|>` | åºåˆ—ç»“æŸ | BERT / CLIP |
| `[PAD]` | å¡«å…… | æ‰€æœ‰æ¨¡å‹ |
| `[MASK]` | æ©ç ï¼ˆMLMï¼‰ | BERT |
| `[UNK]` | æœªçŸ¥è¯ | æ‰€æœ‰æ¨¡å‹ |

**è‡ªåŠ¨å¤„ç†**:
```python
# Transformersä¼šè‡ªåŠ¨æ·»åŠ ç‰¹æ®Štoken
text = "Hello world"
tokens = tokenizer(text)
# å®é™…å¤„ç†: [CLS] Hello world [SEP]
```

---

## å¤šæ¨¡æ€é¢„å¤„ç†

### 1. CLIPé¢„å¤„ç†

**å›¾åƒ+æ–‡æœ¬**:

```python
from transformers import CLIPProcessor
from PIL import Image

# åŠ è½½å¤„ç†å™¨ï¼ˆåŒ…å«å›¾åƒå’Œæ–‡æœ¬é¢„å¤„ç†ï¼‰
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# å‡†å¤‡æ•°æ®
image = Image.open("dog.jpg")
text = "A photo of a dog"

# ä¸€æ¬¡æ€§å¤„ç†å›¾åƒå’Œæ–‡æœ¬
inputs = processor(
    text=text,
    images=image,
    return_tensors="pt",
    padding=True
)

# è¾“å‡º
print(inputs.keys())
# dict_keys(['input_ids', 'attention_mask', 'pixel_values'])
```

**æ‰¹é‡å¤„ç†**:

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªå›¾åƒå’Œæ–‡æœ¬
images = [Image.open(f"image_{i}.jpg") for i in range(4)]
texts = ["A dog", "A cat", "A bird", "A fish"]

inputs = processor(
    text=texts,
    images=images,
    return_tensors="pt",
    padding=True
)

# è¾“å‡ºå½¢çŠ¶
print(inputs['pixel_values'].shape)  # (4, 3, 224, 224)
print(inputs['input_ids'].shape)     # (4, 77)
```

### 2. LLaVAé¢„å¤„ç†

**å›¾åƒ+å¯¹è¯**:

```python
from transformers import LlavaProcessor

processor = LlavaProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

# å‡†å¤‡å¯¹è¯æ ¼å¼çš„è¾“å…¥
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What is in this image?"}
        ]
    }
]

# å¤„ç†
inputs = processor(
    text=processor.apply_chat_template(conversation, add_generation_prompt=True),
    images=image,
    return_tensors="pt"
)
```

---

## é¢„å¤„ç†æµæ°´çº¿

### 1. ä½¿ç”¨torchvision.transforms

**ä¼˜ç‚¹**: ç®€å•ã€é«˜æ•ˆã€GPUåŠ é€Ÿ

```python
from torchvision import transforms

# å®šä¹‰æµæ°´çº¿
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225]),
])

# åº”ç”¨åˆ°æ•°æ®é›†
from torchvision.datasets import ImageFolder

dataset = ImageFolder(
    root='data/train',
    transform=transform  # è‡ªåŠ¨åº”ç”¨åˆ°æ¯ä¸ªæ ·æœ¬
)
```

### 2. ä½¿ç”¨Transformers Processor

**ä¼˜ç‚¹**: è‡ªåŠ¨åŒ¹é…æ¨¡å‹ã€å¤šæ¨¡æ€æ”¯æŒ

```python
from transformers import CLIPProcessor

processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# è‡ªå®šä¹‰Dataset
class MyDataset(torch.utils.data.Dataset):
    def __init__(self, image_paths, texts, processor):
        self.image_paths = image_paths
        self.texts = texts
        self.processor = processor
    
    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx])
        text = self.texts[idx]
        
        # ä½¿ç”¨processorå¤„ç†
        inputs = self.processor(
            text=text,
            images=image,
            return_tensors="pt",
            padding='max_length',
            max_length=77
        )
        
        # ç§»é™¤batchç»´åº¦
        return {
            'pixel_values': inputs['pixel_values'].squeeze(0),
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0)
        }
```

### 3. è‡ªå®šä¹‰é¢„å¤„ç†å‡½æ•°

**çµæ´»æ€§æœ€é«˜**:

```python
def custom_preprocess(image, text):
    """è‡ªå®šä¹‰é¢„å¤„ç†å‡½æ•°"""
    
    # å›¾åƒé¢„å¤„ç†
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # å°ºå¯¸è°ƒæ•´
    image = image.resize((224, 224), Image.BILINEAR)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    image = np.array(image).astype(np.float32) / 255.0
    
    # å½’ä¸€åŒ–
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    image = (image - mean) / std
    
    # è½¬æ¢ä¸ºTensor (HWC -> CHW)
    image = torch.from_numpy(image).permute(2, 0, 1)
    
    # æ–‡æœ¬é¢„å¤„ç†
    # ... (ä½¿ç”¨tokenizer)
    
    return image, text_tokens
```

---

## æœ€ä½³å®è·µ

### 1. ä½¿ç”¨ä¸é¢„è®­ç»ƒç›¸åŒçš„é¢„å¤„ç†

```python
# âŒ é”™è¯¯ï¼šä½¿ç”¨ä¸åŒçš„å½’ä¸€åŒ–å‚æ•°
transform = transforms.Normalize(
    mean=[0.5, 0.5, 0.5],  # è‡ªå®šä¹‰å‚æ•°
    std=[0.5, 0.5, 0.5]
)

# âœ… æ­£ç¡®ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°
from transformers import CLIPProcessor
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
# è‡ªåŠ¨ä½¿ç”¨æ­£ç¡®çš„é¢„å¤„ç†å‚æ•°
```

### 2. è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ä¸€è‡´çš„é¢„å¤„ç†

```python
# å®šä¹‰åŸºç¡€é¢„å¤„ç†
base_transform = [
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225]),
]

# è®­ç»ƒï¼šæ·»åŠ æ•°æ®å¢å¼º
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),  # é¢å¤–çš„å¢å¼º
    transforms.RandomHorizontalFlip(),
] + base_transform[2:])  # å¤ç”¨åé¢çš„æ­¥éª¤

# æ¨ç†ï¼šåªç”¨åŸºç¡€é¢„å¤„ç†
val_transform = transforms.Compose(base_transform)
```

### 3. ç¼“å­˜é¢„å¤„ç†ç»“æœ

```python
# å¯¹äºä¸å˜çš„é¢„å¤„ç†ï¼Œå¯ä»¥æå‰å¤„ç†å¹¶ç¼“å­˜
class CachedDataset(torch.utils.data.Dataset):
    def __init__(self, data, transform, cache_dir='./cache'):
        self.data = data
        self.transform = transform
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def __getitem__(self, idx):
        cache_path = os.path.join(self.cache_dir, f'{idx}.pt')
        
        # æ£€æŸ¥ç¼“å­˜
        if os.path.exists(cache_path):
            return torch.load(cache_path)
        
        # å¤„ç†å¹¶ç¼“å­˜
        item = self.transform(self.data[idx])
        torch.save(item, cache_path)
        return item
```

### 4. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# ä½¿ç”¨DataLoaderçš„collate_fnè¿›è¡Œæ‰¹å¤„ç†
def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    images = [item['image'] for item in batch]
    texts = [item['text'] for item in batch]
    
    # æ‰¹é‡å¤„ç†ï¼ˆæ›´é«˜æ•ˆï¼‰
    inputs = processor(
        text=texts,
        images=images,
        return_tensors="pt",
        padding=True
    )
    
    return inputs

# ä½¿ç”¨
dataloader = DataLoader(
    dataset,
    batch_size=32,
    collate_fn=collate_fn  # è‡ªå®šä¹‰æ‰¹å¤„ç†
)
```

### 5. é”™è¯¯å¤„ç†

```python
class RobustDataset(torch.utils.data.Dataset):
    def __getitem__(self, idx):
        try:
            image = Image.open(self.image_paths[idx])
            
            # æ£€æŸ¥å›¾åƒæ¨¡å¼
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # åº”ç”¨é¢„å¤„ç†
            image = self.transform(image)
            
            return image, self.labels[idx]
        
        except Exception as e:
            print(f"Error loading image {idx}: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤å›¾åƒ
            return torch.zeros(3, 224, 224), -1
```

---

## ğŸ“Š é¢„å¤„ç†æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | é€Ÿåº¦ | çµæ´»æ€§ | GPUåŠ é€Ÿ | æ¨èåœºæ™¯ |
|------|------|--------|---------|---------|
| **torchvision.transforms** | â­â­â­â­â­ | â­â­â­ | âœ… | å›¾åƒåˆ†ç±» |
| **Transformers Processor** | â­â­â­â­ | â­â­â­â­ | âœ… | å¤šæ¨¡æ€ |
| **è‡ªå®šä¹‰å‡½æ•°** | â­â­â­ | â­â­â­â­â­ | âŒ | ç‰¹æ®Šéœ€æ±‚ |
| **Albumentations** | â­â­â­â­ | â­â­â­â­â­ | âœ… | æ•°æ®å¢å¼º |

---

## â¡ï¸ ä¸‹ä¸€æ­¥

- [03-æ•°æ®å¢å¼ºæŠ€æœ¯](./03-æ•°æ®å¢å¼ºæŠ€æœ¯.md) - å­¦ä¹ æ•°æ®å¢å¼ºæ–¹æ³•
- [04-è‡ªå®šä¹‰æ•°æ®é›†åˆ¶ä½œ](./04-è‡ªå®šä¹‰æ•°æ®é›†åˆ¶ä½œ.md) - åˆ¶ä½œè‡ªå·±çš„æ•°æ®é›†
- [../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/](../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/) - å¼€å§‹æ¨¡å‹è®­ç»ƒ

---

## ğŸ“š å‚è€ƒèµ„æº

- [PyTorch Transformsæ–‡æ¡£](https://pytorch.org/vision/stable/transforms.html)
- [Hugging Face Processors](https://huggingface.co/docs/transformers/main_classes/processors)
- [æ•°æ®é¢„å¤„ç†æœ€ä½³å®è·µ](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)

