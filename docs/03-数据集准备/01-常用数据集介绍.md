# 01 - å¸¸ç”¨æ•°æ®é›†ä»‹ç»

> ğŸ“š **å­¦ä¹ ç›®æ ‡**  
> - äº†è§£è§†è§‰å¤§æ¨¡å‹å¸¸ç”¨çš„æ•°æ®é›†
> - æŒæ¡ä¸åŒæ•°æ®é›†çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯
> - å­¦ä¼šé€‰æ‹©åˆé€‚çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°

> ğŸ¯ **å…ˆä¿®è¦æ±‚**  
> - å®Œæˆ [æ¨¡å‹è°ƒç ”ä¸é€‰å‹](../01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/) éƒ¨åˆ†
> - äº†è§£åŸºæœ¬çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡

> â±ï¸ **é¢„è®¡å­¦ä¹ æ—¶é—´**: 30-45åˆ†é’Ÿ  
> ğŸ·ï¸ **éš¾åº¦**: â­â­â˜†â˜†â˜† åˆçº§

---

## ğŸ“– ç›®å½•

- [å›¾åƒåˆ†ç±»æ•°æ®é›†](#å›¾åƒåˆ†ç±»æ•°æ®é›†)
- [ç›®æ ‡æ£€æµ‹æ•°æ®é›†](#ç›®æ ‡æ£€æµ‹æ•°æ®é›†)
- [å›¾åƒåˆ†å‰²æ•°æ®é›†](#å›¾åƒåˆ†å‰²æ•°æ®é›†)
- [å›¾æ–‡å¤šæ¨¡æ€æ•°æ®é›†](#å›¾æ–‡å¤šæ¨¡æ€æ•°æ®é›†)
- [è§†è§‰é—®ç­”æ•°æ®é›†](#è§†è§‰é—®ç­”æ•°æ®é›†)
- [æ•°æ®é›†é€‰æ‹©æŒ‡å—](#æ•°æ®é›†é€‰æ‹©æŒ‡å—)

---

## å›¾åƒåˆ†ç±»æ•°æ®é›†

### 1. ImageNet

**å®˜ç½‘**: [https://www.image-net.org/](https://www.image-net.org/)

**è§„æ¨¡**:
- è®­ç»ƒé›†: 1,281,167å¼ å›¾åƒ
- éªŒè¯é›†: 50,000å¼ å›¾åƒ
- ç±»åˆ«æ•°: 1,000ç±»

**ç‰¹ç‚¹**:
- âœ… æœ€ç»å…¸çš„å¤§è§„æ¨¡å›¾åƒåˆ†ç±»æ•°æ®é›†
- âœ… é¢„è®­ç»ƒæ¨¡å‹çš„æ ‡å‡†åŸºå‡†
- âœ… å›¾åƒè´¨é‡é«˜ï¼Œæ ‡æ³¨å‡†ç¡®

**é€‚ç”¨åœºæ™¯**:
- é¢„è®­ç»ƒè§†è§‰æ¨¡å‹
- è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
- è¿ç§»å­¦ä¹ çš„èµ·ç‚¹

**ä¸‹è½½æ–¹å¼**:
```bash
# éœ€è¦æ³¨å†Œè´¦å·åä¸‹è½½
# æˆ–ä½¿ç”¨Kaggleç‰ˆæœ¬
kaggle competitions download -c imagenet-object-localization-challenge
```

**æ•°æ®æ ¼å¼**:
```
ImageNet/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ n01440764/  # ç±»åˆ«ID
â”‚   â”‚   â”œâ”€â”€ n01440764_10026.JPEG
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â””â”€â”€ val/
    â”œâ”€â”€ ILSVRC2012_val_00000001.JPEG
    â””â”€â”€ ...
```

### 2. CIFAR-10/100

**å®˜ç½‘**: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)

**è§„æ¨¡**:
- **CIFAR-10**: 60,000å¼ 32Ã—32å½©è‰²å›¾åƒï¼Œ10ç±»
- **CIFAR-100**: 60,000å¼ 32Ã—32å½©è‰²å›¾åƒï¼Œ100ç±»

**ç‰¹ç‚¹**:
- âœ… å°å·§è½»é‡ï¼Œé€‚åˆå¿«é€Ÿå®éªŒ
- âœ… ä½åˆ†è¾¨ç‡ï¼Œå¯¹ç¡¬ä»¶è¦æ±‚ä½
- âœ… ç»å…¸çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†

**é€‚ç”¨åœºæ™¯**:
- ç®—æ³•éªŒè¯å’Œå¿«é€ŸåŸå‹
- æ•™å­¦å’Œå­¦ä¹ 
- èµ„æºå—é™çš„åœºæ™¯

**ä¸‹è½½æ–¹å¼**:
```python
from torchvision import datasets

# è‡ªåŠ¨ä¸‹è½½
train_dataset = datasets.CIFAR10(
    root='./data',
    train=True,
    download=True
)
```

### 3. Stanford Dogs

**å®˜ç½‘**: [http://vision.stanford.edu/aditya86/ImageNetDogs/](http://vision.stanford.edu/aditya86/ImageNetDogs/)

**è§„æ¨¡**:
- å›¾åƒæ•°: 20,580å¼ 
- ç±»åˆ«æ•°: 120ä¸ªçŠ¬ç§

**ç‰¹ç‚¹**:
- âœ… ç»†ç²’åº¦åˆ†ç±»æ•°æ®é›†
- âœ… ç±»å†…å·®å¼‚å¤§ï¼Œç±»é—´å·®å¼‚å°
- âœ… é€‚åˆæµ‹è¯•æ¨¡å‹çš„ç»†èŠ‚è¯†åˆ«èƒ½åŠ›

**é€‚ç”¨åœºæ™¯**:
- ç»†ç²’åº¦å›¾åƒåˆ†ç±»
- è¿ç§»å­¦ä¹ å®éªŒ
- æœ¬æ•™ç¨‹çš„å¾®è°ƒç¤ºä¾‹

**ä½¿ç”¨ç¤ºä¾‹**:
```python
# æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡è„šæœ¬
python scripts/prepare_dog_dataset.py \
    --output_dir data/dogs \
    --num_classes 10
```

---

## ç›®æ ‡æ£€æµ‹æ•°æ®é›†

### 1. COCO (Common Objects in Context)

**å®˜ç½‘**: [https://cocodataset.org/](https://cocodataset.org/)

**è§„æ¨¡**:
- å›¾åƒæ•°: 330Kå¼ ï¼ˆ>200Kæ ‡æ³¨ï¼‰
- ç›®æ ‡å®ä¾‹: 1.5Mä¸ª
- ç±»åˆ«æ•°: 80ç±»

**æ ‡æ³¨ç±»å‹**:
- ç›®æ ‡æ£€æµ‹ï¼ˆè¾¹ç•Œæ¡†ï¼‰
- å®ä¾‹åˆ†å‰²ï¼ˆåƒç´ çº§æ©ç ï¼‰
- å…³é”®ç‚¹æ£€æµ‹ï¼ˆäººä½“å§¿æ€ï¼‰
- å›¾åƒæè¿°ï¼ˆ5ä¸ªæè¿°/å›¾ï¼‰

**ç‰¹ç‚¹**:
- âœ… æœ€å…¨é¢çš„è§†è§‰æ•°æ®é›†
- âœ… å¤šä»»åŠ¡æ ‡æ³¨
- âœ… çœŸå®åœºæ™¯ï¼Œå¤æ‚èƒŒæ™¯

**é€‚ç”¨åœºæ™¯**:
- ç›®æ ‡æ£€æµ‹æ¨¡å‹è®­ç»ƒ
- å®ä¾‹åˆ†å‰²
- å¤šæ¨¡æ€å­¦ä¹ ï¼ˆå›¾æ–‡ï¼‰

**ä¸‹è½½æ–¹å¼**:
```bash
# è®­ç»ƒé›†ï¼ˆ18GBï¼‰
wget http://images.cocodataset.org/zips/train2017.zip

# éªŒè¯é›†ï¼ˆ1GBï¼‰
wget http://images.cocodataset.org/zips/val2017.zip

# æ ‡æ³¨æ–‡ä»¶
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
```

**æ•°æ®æ ¼å¼**:
```json
{
  "images": [
    {
      "id": 1,
      "file_name": "000000000001.jpg",
      "height": 480,
      "width": 640
    }
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "bbox": [x, y, width, height],
      "area": 1234,
      "segmentation": [[x1,y1,x2,y2,...]]
    }
  ],
  "categories": [
    {"id": 1, "name": "person", "supercategory": "person"}
  ]
}
```

### 2. Pascal VOC

**å®˜ç½‘**: [http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/)

**è§„æ¨¡**:
- VOC 2012: 11,530å¼ å›¾åƒ
- ç±»åˆ«æ•°: 20ç±»

**ç‰¹ç‚¹**:
- âœ… ç»å…¸çš„ç›®æ ‡æ£€æµ‹åŸºå‡†
- âœ… æ ‡æ³¨è´¨é‡é«˜
- âœ… é€‚åˆæ•™å­¦å’Œå…¥é—¨

**é€‚ç”¨åœºæ™¯**:
- ç›®æ ‡æ£€æµ‹å…¥é—¨
- ç®—æ³•å¯¹æ¯”å®éªŒ
- å°è§„æ¨¡å®éªŒ

---

## å›¾åƒåˆ†å‰²æ•°æ®é›†

### 1. ADE20K

**å®˜ç½‘**: [https://groups.csail.mit.edu/vision/datasets/ADE20K/](https://groups.csail.mit.edu/vision/datasets/ADE20K/)

**è§„æ¨¡**:
- å›¾åƒæ•°: 25Kå¼ 
- ç±»åˆ«æ•°: 150ç±»ï¼ˆåœºæ™¯åˆ†å‰²ï¼‰

**ç‰¹ç‚¹**:
- âœ… åœºæ™¯ç†è§£æ•°æ®é›†
- âœ… å¯†é›†æ ‡æ³¨
- âœ… å¤šæ ·åŒ–åœºæ™¯

**é€‚ç”¨åœºæ™¯**:
- è¯­ä¹‰åˆ†å‰²
- åœºæ™¯ç†è§£
- å…¨æ™¯åˆ†å‰²

### 2. Cityscapes

**å®˜ç½‘**: [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/)

**è§„æ¨¡**:
- ç²¾ç»†æ ‡æ³¨: 5,000å¼ 
- ç²—ç•¥æ ‡æ³¨: 20,000å¼ 
- ç±»åˆ«æ•°: 30ç±»

**ç‰¹ç‚¹**:
- âœ… è‡ªåŠ¨é©¾é©¶åœºæ™¯
- âœ… é«˜åˆ†è¾¨ç‡ï¼ˆ1024Ã—2048ï¼‰
- âœ… æ—¶åºæ ‡æ³¨

**é€‚ç”¨åœºæ™¯**:
- è‡ªåŠ¨é©¾é©¶
- è¡—æ™¯ç†è§£
- å®æ—¶åˆ†å‰²

---

## å›¾æ–‡å¤šæ¨¡æ€æ•°æ®é›†

### 1. Flickr30K

**å®˜ç½‘**: [http://shannon.cs.illinois.edu/DenotationGraph/](http://shannon.cs.illinois.edu/DenotationGraph/)

**è§„æ¨¡**:
- å›¾åƒæ•°: 31,783å¼ 
- æè¿°æ•°: 158,915æ¡ï¼ˆ5æ¡/å›¾ï¼‰

**ç‰¹ç‚¹**:
- âœ… å›¾æ–‡é…å¯¹æ•°æ®
- âœ… å¤šæ ·åŒ–æè¿°
- âœ… é€‚åˆå›¾æ–‡æ£€ç´¢

**é€‚ç”¨åœºæ™¯**:
- å›¾æ–‡æ£€ç´¢
- å›¾åƒæè¿°ç”Ÿæˆ
- CLIPç­‰æ¨¡å‹è¯„ä¼°

**æ•°æ®æ ¼å¼**:
```
image_id: 1000092795.jpg
captions:
  - Two young guys with shaggy hair look at their hands while hanging out in the yard.
  - Two young, White males are outside near many bushes.
  - ...
```

### 2. Conceptual Captions

**å®˜ç½‘**: [https://ai.google.com/research/ConceptualCaptions/](https://ai.google.com/research/ConceptualCaptions/)

**è§„æ¨¡**:
- CC3M: 3.3Må›¾æ–‡å¯¹
- CC12M: 12Må›¾æ–‡å¯¹

**ç‰¹ç‚¹**:
- âœ… å¤§è§„æ¨¡å›¾æ–‡æ•°æ®
- âœ… è‡ªåŠ¨æ”¶é›†ï¼ˆå™ªå£°è¾ƒå¤šï¼‰
- âœ… é€‚åˆé¢„è®­ç»ƒ

**é€‚ç”¨åœºæ™¯**:
- å¤§è§„æ¨¡é¢„è®­ç»ƒ
- å›¾æ–‡å¯¹é½å­¦ä¹ 
- CLIPç±»æ¨¡å‹è®­ç»ƒ

---

## è§†è§‰é—®ç­”æ•°æ®é›†

### 1. VQA v2

**å®˜ç½‘**: [https://visualqa.org/](https://visualqa.org/)

**è§„æ¨¡**:
- å›¾åƒæ•°: 200K+å¼ ï¼ˆæ¥è‡ªCOCOï¼‰
- é—®é¢˜æ•°: 1.1M+ä¸ª
- ç­”æ¡ˆæ•°: 11M+ä¸ª

**ç‰¹ç‚¹**:
- âœ… éœ€è¦è§†è§‰æ¨ç†
- âœ… å¼€æ”¾å¼é—®ç­”
- âœ… å¤šæ ·åŒ–é—®é¢˜ç±»å‹

**é—®é¢˜ç±»å‹**:
- æ˜¯/å¦é—®é¢˜: "Is there a dog?"
- æ•°é‡é—®é¢˜: "How many people?"
- é¢œè‰²é—®é¢˜: "What color is the car?"
- ä½ç½®é—®é¢˜: "Where is the cat?"

**é€‚ç”¨åœºæ™¯**:
- è§†è§‰é—®ç­”æ¨¡å‹è®­ç»ƒ
- å¤šæ¨¡æ€æ¨ç†
- LLaVAç­‰æ¨¡å‹è¯„ä¼°

**æ•°æ®æ ¼å¼**:
```json
{
  "question_id": 1,
  "image_id": 262145,
  "question": "What is the color of the fire hydrant?",
  "answers": [
    {"answer": "red", "answer_confidence": "yes"},
    {"answer": "red", "answer_confidence": "yes"},
    ...
  ]
}
```

### 2. GQA (Visual Reasoning)

**å®˜ç½‘**: [https://cs.stanford.edu/people/dorarad/gqa/](https://cs.stanford.edu/people/dorarad/gqa/)

**è§„æ¨¡**:
- å›¾åƒæ•°: 113Kå¼ 
- é—®é¢˜æ•°: 22Mä¸ª

**ç‰¹ç‚¹**:
- âœ… ç»“æ„åŒ–é—®é¢˜
- âœ… éœ€è¦å¤šæ­¥æ¨ç†
- âœ… åœºæ™¯å›¾æ ‡æ³¨

**é€‚ç”¨åœºæ™¯**:
- å¤æ‚è§†è§‰æ¨ç†
- å…³ç³»ç†è§£
- ç»„åˆæ³›åŒ–

---

## æ•°æ®é›†é€‰æ‹©æŒ‡å—

### æŒ‰ä»»åŠ¡ç±»å‹é€‰æ‹©

| ä»»åŠ¡ | æ¨èæ•°æ®é›† | è§„æ¨¡ | éš¾åº¦ |
|------|-----------|------|------|
| **å›¾åƒåˆ†ç±»** | ImageNet, CIFAR-10 | å¤§/å° | â­â­ |
| **ç»†ç²’åº¦åˆ†ç±»** | Stanford Dogs, CUB-200 | ä¸­ | â­â­â­ |
| **ç›®æ ‡æ£€æµ‹** | COCO, Pascal VOC | å¤§/ä¸­ | â­â­â­ |
| **å›¾åƒåˆ†å‰²** | ADE20K, Cityscapes | ä¸­ | â­â­â­â­ |
| **å›¾æ–‡æ£€ç´¢** | Flickr30K, COCO Captions | ä¸­/å¤§ | â­â­â­ |
| **è§†è§‰é—®ç­”** | VQA v2, GQA | å¤§ | â­â­â­â­ |

### æŒ‰èµ„æºé€‰æ‹©

**æœ‰é™èµ„æº**ï¼ˆ<16GBæ˜¾å­˜ï¼Œ<100GBå­˜å‚¨ï¼‰:
- âœ… CIFAR-10/100
- âœ… Stanford Dogsï¼ˆå­é›†ï¼‰
- âœ… Flickr30K

**ä¸­ç­‰èµ„æº**ï¼ˆ16-32GBæ˜¾å­˜ï¼Œ100-500GBå­˜å‚¨ï¼‰:
- âœ… Pascal VOC
- âœ… ADE20K
- âœ… VQA v2

**å……è¶³èµ„æº**ï¼ˆ>32GBæ˜¾å­˜ï¼Œ>1TBå­˜å‚¨ï¼‰:
- âœ… ImageNet
- âœ… COCO
- âœ… Conceptual Captions

### æŒ‰å­¦ä¹ é˜¶æ®µé€‰æ‹©

**å…¥é—¨é˜¶æ®µ**:
1. CIFAR-10 - å¿«é€Ÿå®éªŒ
2. Stanford Dogsï¼ˆ10ç±»ï¼‰ - å¾®è°ƒå®è·µ
3. Flickr30Kï¼ˆå­é›†ï¼‰ - å¤šæ¨¡æ€å…¥é—¨

**è¿›é˜¶é˜¶æ®µ**:
1. COCO - å¤šä»»åŠ¡å­¦ä¹ 
2. VQA v2 - è§†è§‰æ¨ç†
3. ImageNet - å¤§è§„æ¨¡è®­ç»ƒ

**ç”Ÿäº§åº”ç”¨**:
1. è‡ªå®šä¹‰æ•°æ®é›† - ä¸šåŠ¡ç›¸å…³
2. å…¬å¼€æ•°æ®é›† - é¢„è®­ç»ƒ
3. æ··åˆæ•°æ®é›† - æå‡æ³›åŒ–

---

## ğŸ“Š æ•°æ®é›†å¯¹æ¯”è¡¨

| æ•°æ®é›† | ä»»åŠ¡ | å›¾åƒæ•° | ç±»åˆ«æ•° | å¤§å° | ä¸‹è½½éš¾åº¦ |
|--------|------|--------|--------|------|---------|
| **ImageNet** | åˆ†ç±» | 1.2M | 1000 | ~150GB | â­â­â­ |
| **CIFAR-10** | åˆ†ç±» | 60K | 10 | ~170MB | â­ |
| **Stanford Dogs** | åˆ†ç±» | 20K | 120 | ~750MB | â­â­ |
| **COCO** | æ£€æµ‹/åˆ†å‰² | 330K | 80 | ~25GB | â­â­ |
| **Pascal VOC** | æ£€æµ‹ | 11K | 20 | ~2GB | â­ |
| **Flickr30K** | å›¾æ–‡ | 32K | - | ~3GB | â­â­ |
| **VQA v2** | é—®ç­” | 200K | - | ~25GB | â­â­ |

---

## ğŸ”— æ•°æ®é›†èµ„æºæ±‡æ€»

### å®˜æ–¹èµ„æº

- **Papers With Code Datasets**: [https://paperswithcode.com/datasets](https://paperswithcode.com/datasets)
- **Hugging Face Datasets**: [https://huggingface.co/datasets](https://huggingface.co/datasets)
- **Google Dataset Search**: [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)

### å›½å†…é•œåƒ

- **æ¸…åå¤§å­¦å¼€æºé•œåƒ**: [https://mirrors.tuna.tsinghua.edu.cn/](https://mirrors.tuna.tsinghua.edu.cn/)
- **é˜¿é‡Œäº‘é•œåƒ**: [https://developer.aliyun.com/mirror/](https://developer.aliyun.com/mirror/)

### å·¥å…·åº“

- **TorchVision Datasets**: å†…ç½®å¸¸ç”¨æ•°æ®é›†
- **Hugging Face Datasets**: ç»Ÿä¸€çš„æ•°æ®é›†æ¥å£
- **TensorFlow Datasets**: TensorFlowç”Ÿæ€

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### 1. ä»å°æ•°æ®é›†å¼€å§‹

```python
# å…ˆç”¨å°æ•°æ®é›†éªŒè¯ä»£ç 
train_dataset = datasets.CIFAR10(root='./data', train=True)

# éªŒè¯é€šè¿‡åå†ç”¨å¤§æ•°æ®é›†
# train_dataset = ImageNetDataset(root='./data/imagenet')
```

### 2. ä½¿ç”¨æ•°æ®é›†å­é›†

```python
# åªä½¿ç”¨10%çš„æ•°æ®è¿›è¡Œå¿«é€Ÿå®éªŒ
from torch.utils.data import Subset
import numpy as np

indices = np.random.choice(len(dataset), size=len(dataset)//10, replace=False)
subset = Subset(dataset, indices)
```

### 3. ç¼“å­˜é¢„å¤„ç†ç»“æœ

```python
# é¢„å¤„ç†ä¸€æ¬¡ï¼Œä¿å­˜ç»“æœ
processed_data = preprocess(raw_data)
torch.save(processed_data, 'processed_data.pt')

# åç»­ç›´æ¥åŠ è½½
processed_data = torch.load('processed_data.pt')
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

- [02-æ•°æ®é¢„å¤„ç†æ–¹æ³•](./02-æ•°æ®é¢„å¤„ç†æ–¹æ³•.md) - å­¦ä¹ å¦‚ä½•å¤„ç†æ•°æ®
- [03-æ•°æ®å¢å¼ºæŠ€æœ¯](./03-æ•°æ®å¢å¼ºæŠ€æœ¯.md) - æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›
- [04-è‡ªå®šä¹‰æ•°æ®é›†åˆ¶ä½œ](./04-è‡ªå®šä¹‰æ•°æ®é›†åˆ¶ä½œ.md) - åˆ¶ä½œè‡ªå·±çš„æ•°æ®é›†

---

## ğŸ“š å‚è€ƒèµ„æº

- [ImageNetå®˜æ–¹è®ºæ–‡](https://arxiv.org/abs/1409.0575)
- [COCOæ•°æ®é›†è®ºæ–‡](https://arxiv.org/abs/1405.0312)
- [VQAæ•°æ®é›†è®ºæ–‡](https://arxiv.org/abs/1505.00468)
- [æ•°æ®é›†ç»¼è¿°è®ºæ–‡](https://arxiv.org/abs/2103.00020)

