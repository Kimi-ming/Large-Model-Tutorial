# 04 - è‡ªå®šä¹‰æ•°æ®é›†åˆ¶ä½œ

> ğŸ“š **å­¦ä¹ ç›®æ ‡**  
> - å­¦ä¼šåˆ¶ä½œè‡ªå·±çš„æ•°æ®é›†
> - æŒæ¡PyTorch Datasetç±»çš„ä½¿ç”¨
> - äº†è§£æ•°æ®é›†çš„ç»„ç»‡å’Œæ ‡æ³¨æ–¹æ³•

> ğŸ¯ **å…ˆä¿®è¦æ±‚**  
> - å®Œæˆ [02-æ•°æ®é¢„å¤„ç†æ–¹æ³•](./02-æ•°æ®é¢„å¤„ç†æ–¹æ³•.md)
> - ç†Ÿæ‚‰Pythonå’ŒPyTorchåŸºç¡€

> â±ï¸ **é¢„è®¡å­¦ä¹ æ—¶é—´**: 45-60åˆ†é’Ÿ  
> ğŸ·ï¸ **éš¾åº¦**: â­â­â­â˜†â˜† ä¸­çº§

> âœ… **ä»£ç å¯ç”¨æ€§**  
> æœ¬æ•™ç¨‹çš„ç¤ºä¾‹ä»£ç å·²å®ç°ï¼š
> - æ•°æ®é›†ç±»ç¤ºä¾‹: `code/02-fine-tuning/lora/dataset.py`
> - æ•°æ®å‡†å¤‡è„šæœ¬: `scripts/prepare_dog_dataset.py`

---

## ğŸ“– ç›®å½•

- [æ•°æ®é›†ç»„ç»‡ç»“æ„](#æ•°æ®é›†ç»„ç»‡ç»“æ„)
- [åˆ›å»ºDatasetç±»](#åˆ›å»ºdatasetç±»)
- [æ•°æ®æ ‡æ³¨æ–¹æ³•](#æ•°æ®æ ‡æ³¨æ–¹æ³•)
- [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)

---

## æ•°æ®é›†ç»„ç»‡ç»“æ„

### 1. å›¾åƒåˆ†ç±»æ•°æ®é›†

**æ¨èç»“æ„**ï¼ˆImageFolderæ ¼å¼ï¼‰:

```
my_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ class_1/
â”‚   â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”‚   â”œâ”€â”€ img_002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ class_2/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ class_n/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ class_1/
â”‚   â”œâ”€â”€ class_2/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â”œâ”€â”€ class_1/
    â”œâ”€â”€ class_2/
    â””â”€â”€ ...
```

**ä½¿ç”¨æ–¹å¼**:

```python
from torchvision.datasets import ImageFolder

# è‡ªåŠ¨åŠ è½½ï¼Œç±»åˆ«åä»ç›®å½•åæ¨æ–­
train_dataset = ImageFolder(
    root='my_dataset/train',
    transform=train_transform
)

print(train_dataset.classes)  # ['class_1', 'class_2', ...]
print(train_dataset.class_to_idx)  # {'class_1': 0, 'class_2': 1, ...}
```

### 2. å¸¦æ ‡æ³¨æ–‡ä»¶çš„ç»“æ„

```
my_dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”œâ”€â”€ img_002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ train.json
â”‚   â”œâ”€â”€ val.json
â”‚   â””â”€â”€ test.json
â””â”€â”€ classes.txt
```

**æ ‡æ³¨æ–‡ä»¶æ ¼å¼**ï¼ˆJSONï¼‰:

```json
{
  "images": [
    {
      "id": 1,
      "file_name": "img_001.jpg",
      "width": 640,
      "height": 480
    }
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 0,
      "category_name": "dog"
    }
  ],
  "categories": [
    {"id": 0, "name": "dog"},
    {"id": 1, "name": "cat"}
  ]
}
```

---

## åˆ›å»ºDatasetç±»

### 1. åŸºç¡€Datasetç±»

```python
import os
from PIL import Image
import torch
from torch.utils.data import Dataset

class CustomImageDataset(Dataset):
    """
    è‡ªå®šä¹‰å›¾åƒåˆ†ç±»æ•°æ®é›†
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        split: 'train', 'val', æˆ– 'test'
        transform: å›¾åƒå˜æ¢
    """
    
    def __init__(self, data_dir, split='train', transform=None):
        self.data_dir = os.path.join(data_dir, split)
        self.transform = transform
        
        # åŠ è½½ç±»åˆ«
        self.classes = sorted(os.listdir(self.data_dir))
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}
        
        # åŠ è½½æ‰€æœ‰å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        self.samples = []
        for class_name in self.classes:
            class_dir = os.path.join(self.data_dir, class_name)
            for img_name in os.listdir(class_dir):
                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                    img_path = os.path.join(class_dir, img_name)
                    label = self.class_to_idx[class_name]
                    self.samples.append((img_path, label))
        
        print(f"Loaded {len(self.samples)} images from {split} set")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        
        return image, label
```

### 2. å¸¦æ ‡æ³¨æ–‡ä»¶çš„Dataset

```python
import json

class AnnotatedDataset(Dataset):
    """
    ä½¿ç”¨JSONæ ‡æ³¨æ–‡ä»¶çš„æ•°æ®é›†
    """
    
    def __init__(self, data_dir, annotation_file, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        
        # åŠ è½½æ ‡æ³¨
        with open(annotation_file, 'r') as f:
            self.annotations = json.load(f)
        
        # åˆ›å»ºæ˜ å°„
        self.images = {img['id']: img for img in self.annotations['images']}
        self.categories = {cat['id']: cat['name'] 
                          for cat in self.annotations['categories']}
        
        # åˆ›å»ºæ ·æœ¬åˆ—è¡¨
        self.samples = []
        for ann in self.annotations['annotations']:
            img_info = self.images[ann['image_id']]
            self.samples.append({
                'image_path': os.path.join(self.data_dir, img_info['file_name']),
                'label': ann['category_id'],
                'category_name': self.categories[ann['category_id']]
            })
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(sample['image_path']).convert('RGB')
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        
        return image, sample['label']
```

### 3. å¤šæ¨¡æ€Datasetï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰

```python
class ImageTextDataset(Dataset):
    """
    å›¾åƒ-æ–‡æœ¬é…å¯¹æ•°æ®é›†ï¼ˆç”¨äºCLIPç­‰æ¨¡å‹ï¼‰
    """
    
    def __init__(self, data_dir, annotation_file, processor):
        self.data_dir = data_dir
        self.processor = processor
        
        # åŠ è½½æ ‡æ³¨
        with open(annotation_file, 'r') as f:
            self.data = json.load(f)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(
            os.path.join(self.data_dir, item['image'])
        ).convert('RGB')
        
        # è·å–æ–‡æœ¬
        text = item['caption']
        
        # ä½¿ç”¨processorå¤„ç†
        inputs = self.processor(
            text=text,
            images=image,
            return_tensors="pt",
            padding='max_length',
            max_length=77
        )
        
        # ç§»é™¤batchç»´åº¦
        return {
            'pixel_values': inputs['pixel_values'].squeeze(0),
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0)
        }
```

---

## æ•°æ®æ ‡æ³¨æ–¹æ³•

### 1. æ‰‹åŠ¨æ ‡æ³¨å·¥å…·

**å›¾åƒåˆ†ç±»**:
- **LabelImg**: ç®€å•æ˜“ç”¨
- **Labelbox**: åœ¨çº¿æ ‡æ³¨å¹³å°
- **CVAT**: æ”¯æŒå¤šç§ä»»åŠ¡

**ä½¿ç”¨LabelImg**:

```bash
# å®‰è£…
pip install labelImg

# å¯åŠ¨
labelImg

# æ“ä½œï¼š
# 1. æ‰“å¼€å›¾åƒç›®å½•
# 2. é€‰æ‹©ä¿å­˜ç›®å½•
# 3. å¼€å§‹æ ‡æ³¨
# 4. ä¿å­˜ä¸ºPASCAL VOCæˆ–YOLOæ ¼å¼
```

### 2. åŠè‡ªåŠ¨æ ‡æ³¨

**ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¾…åŠ©æ ‡æ³¨**:

```python
from transformers import CLIPModel, CLIPProcessor
import torch

def auto_label_images(image_paths, candidate_labels, model, processor):
    """
    ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ï¼Œè¾…åŠ©æ ‡æ³¨
    """
    results = []
    
    for img_path in image_paths:
        image = Image.open(img_path)
        
        # å‡†å¤‡è¾“å…¥
        inputs = processor(
            text=candidate_labels,
            images=image,
            return_tensors="pt",
            padding=True
        )
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits_per_image
            probs = logits.softmax(dim=1)[0]
        
        # è·å–æœ€å¯èƒ½çš„æ ‡ç­¾
        pred_idx = probs.argmax().item()
        pred_label = candidate_labels[pred_idx]
        confidence = probs[pred_idx].item()
        
        results.append({
            'image': img_path,
            'label': pred_label,
            'confidence': confidence
        })
    
    return results

# ä½¿ç”¨
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

candidate_labels = ["a photo of a dog", "a photo of a cat", "a photo of a bird"]
results = auto_label_images(image_paths, candidate_labels, model, processor)

# äººå·¥å®¡æ ¸é«˜ç½®ä¿¡åº¦çš„æ ‡æ³¨ï¼Œæ‰‹åŠ¨ä¿®æ­£ä½ç½®ä¿¡åº¦çš„
```

### 3. ä¼—åŒ…æ ‡æ³¨

**å¹³å°**:
- Amazon Mechanical Turk
- Labelbox
- Scale AI

**è´¨é‡æ§åˆ¶**:
```python
def quality_control(annotations, min_agreement=0.8):
    """
    å¤šäººæ ‡æ³¨çš„è´¨é‡æ§åˆ¶
    """
    # è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§
    agreement = calculate_inter_annotator_agreement(annotations)
    
    # è¿‡æ»¤ä½è´¨é‡æ ‡æ³¨
    filtered = [ann for ann in annotations 
                if ann['agreement'] >= min_agreement]
    
    return filtered
```

---

## å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ï¼šçŠ¬ç§åˆ†ç±»æ•°æ®é›†

**1. æ•°æ®æ”¶é›†**

```python
# ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡è„šæœ¬
python scripts/prepare_dog_dataset.py \
    --output_dir data/dogs \
    --num_classes 10
```

**2. åˆ›å»ºDatasetç±»**

```python
# ä½¿ç”¨æˆ‘ä»¬å®ç°çš„Datasetç±»
from code.utils.dataset import DogBreedDataset
from transformers import CLIPProcessor

processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

train_dataset = DogBreedDataset(
    data_dir='data/dogs',
    split='train',
    processor=processor
)

print(f"Dataset size: {len(train_dataset)}")
print(f"Classes: {train_dataset.classes}")
```

**3. åˆ›å»ºDataLoader**

```python
from torch.utils.data import DataLoader

train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)

# æµ‹è¯•åŠ è½½
for batch in train_loader:
    pixel_values, labels = batch
    print(f"Batch shape: {pixel_values.shape}")
    print(f"Labels: {labels}")
    break
```

**4. æ•°æ®é›†ç»Ÿè®¡**

```python
def analyze_dataset(dataset):
    """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    
    # ç±»åˆ«åˆ†å¸ƒ
    distribution = dataset.get_class_distribution()
    print("\nç±»åˆ«åˆ†å¸ƒ:")
    for class_name, count in distribution.items():
        print(f"  {class_name}: {count} å¼ ")
    
    # æ£€æŸ¥æ•°æ®å¹³è¡¡
    counts = list(distribution.values())
    imbalance_ratio = max(counts) / min(counts)
    print(f"\næ•°æ®ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}")
    
    if imbalance_ratio > 2.0:
        print("âš ï¸  è­¦å‘Š: æ•°æ®é›†ä¸å¹³è¡¡ï¼Œè€ƒè™‘ä½¿ç”¨åŠ æƒé‡‡æ ·")

analyze_dataset(train_dataset)
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ•°æ®é›†åˆ’åˆ†

```python
from sklearn.model_selection import train_test_split

def split_dataset(data_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """
    åˆ’åˆ†æ•°æ®é›†
    """
    assert train_ratio + val_ratio + test_ratio == 1.0
    
    all_files = []
    all_labels = []
    
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
    for class_name in os.listdir(data_dir):
        class_dir = os.path.join(data_dir, class_name)
        for img_name in os.listdir(class_dir):
            all_files.append(os.path.join(class_dir, img_name))
            all_labels.append(class_name)
    
    # åˆ†å±‚åˆ’åˆ†
    train_files, temp_files, train_labels, temp_labels = train_test_split(
        all_files, all_labels,
        test_size=(1 - train_ratio),
        stratify=all_labels,  # ä¿æŒç±»åˆ«æ¯”ä¾‹
        random_state=42
    )
    
    val_files, test_files, val_labels, test_labels = train_test_split(
        temp_files, temp_labels,
        test_size=test_ratio / (val_ratio + test_ratio),
        stratify=temp_labels,
        random_state=42
    )
    
    return {
        'train': (train_files, train_labels),
        'val': (val_files, val_labels),
        'test': (test_files, test_labels)
    }
```

### 2. æ•°æ®éªŒè¯

```python
def validate_dataset(dataset):
    """éªŒè¯æ•°æ®é›†å®Œæ•´æ€§"""
    
    errors = []
    
    for idx in range(len(dataset)):
        try:
            image, label = dataset[idx]
            
            # æ£€æŸ¥å›¾åƒ
            if image.size(0) != 3:
                errors.append(f"Image {idx}: wrong channels")
            
            # æ£€æŸ¥æ ‡ç­¾
            if label < 0 or label >= len(dataset.classes):
                errors.append(f"Image {idx}: invalid label")
        
        except Exception as e:
            errors.append(f"Image {idx}: {str(e)}")
    
    if errors:
        print(f"Found {len(errors)} errors:")
        for error in errors[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {error}")
    else:
        print("âœ… Dataset validation passed!")

validate_dataset(train_dataset)
```

### 3. æ•°æ®å¢å¼ºéªŒè¯

```python
import matplotlib.pyplot as plt

def visualize_augmentations(dataset, num_samples=5):
    """å¯è§†åŒ–æ•°æ®å¢å¼ºæ•ˆæœ"""
    
    fig, axes = plt.subplots(num_samples, 5, figsize=(15, num_samples*3))
    
    for i in range(num_samples):
        # è·å–ä¸€ä¸ªæ ·æœ¬
        image, label = dataset[i]
        
        # æ˜¾ç¤ºåŸå›¾å’Œ4æ¬¡å¢å¼ºç»“æœ
        for j in range(5):
            aug_image, _ = dataset[i]  # æ¯æ¬¡éƒ½ä¼šåº”ç”¨ä¸åŒçš„å¢å¼º
            
            # åå½’ä¸€åŒ–ä»¥ä¾¿æ˜¾ç¤º
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            aug_image = aug_image * std + mean
            aug_image = torch.clamp(aug_image, 0, 1)
            
            # æ˜¾ç¤º
            axes[i, j].imshow(aug_image.permute(1, 2, 0))
            axes[i, j].axis('off')
            if j == 0:
                axes[i, j].set_title(f"Class: {dataset.classes[label]}")
    
    plt.tight_layout()
    plt.savefig('augmentation_samples.png')
    print("âœ… Saved augmentation samples to augmentation_samples.png")

visualize_augmentations(train_dataset)
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

- [../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/](../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/) - ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
- [02-æ•°æ®é¢„å¤„ç†æ–¹æ³•](./02-æ•°æ®é¢„å¤„ç†æ–¹æ³•.md) - å›é¡¾é¢„å¤„ç†æ–¹æ³•

---

## ğŸ“š å‚è€ƒèµ„æº

- [PyTorch Datasetæ•™ç¨‹](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)
- [æ•°æ®é›†åˆ¶ä½œæœ€ä½³å®è·µ](https://github.com/pytorch/vision/tree/main/references)
- [LabelImgå·¥å…·](https://github.com/tzutalin/labelImg)

