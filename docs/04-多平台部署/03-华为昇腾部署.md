# 03 - åä¸ºæ˜‡è…¾éƒ¨ç½²

> ğŸ“š **å­¦ä¹ ç›®æ ‡**  
> - äº†è§£åä¸ºæ˜‡è…¾AIå¤„ç†å™¨åŠå…¶ç”Ÿæ€
> - æŒæ¡CANNå¼€å‘å·¥å…·é“¾çš„ä½¿ç”¨
> - å­¦ä¼šå°†PyTorchæ¨¡å‹è¿ç§»åˆ°æ˜‡è…¾å¹³å°
> - ç†è§£æ˜‡è…¾å¹³å°çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ³•

> ğŸ¯ **å…ˆä¿®è¦æ±‚**  
> - å®Œæˆ [NVIDIAéƒ¨ç½²åŸºç¡€](./01-NVIDIAéƒ¨ç½²åŸºç¡€.md)
> - ç†Ÿæ‚‰PyTorchæ¨¡å‹éƒ¨ç½²
> - äº†è§£åŸºæœ¬çš„AIç¼–è¯‘ä¼˜åŒ–æ¦‚å¿µ

> â±ï¸ **é¢„è®¡å­¦ä¹ æ—¶é—´**: 90-120åˆ†é’Ÿ  
> ğŸ·ï¸ **éš¾åº¦**: â­â­â­â­â­ ä¸“å®¶çº§

> âœ… **ä»£ç å¯ç”¨æ€§**  
> æœ¬æ•™ç¨‹çš„ç¤ºä¾‹ä»£ç ï¼š
> - éƒ¨ç½²è„šæœ¬: `code/04-deployment/huawei/`
> - æ¨¡å‹è½¬æ¢å·¥å…·å’Œé…ç½®

---

## ğŸ“– ç›®å½•

- [æ˜‡è…¾å¹³å°æ¦‚è¿°](#æ˜‡è…¾å¹³å°æ¦‚è¿°)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [PyTorchæ¨¡å‹è¿ç§»](#pytorchæ¨¡å‹è¿ç§»)
- [æ¨¡å‹è½¬æ¢ä¸ä¼˜åŒ–](#æ¨¡å‹è½¬æ¢ä¸ä¼˜åŒ–)
- [æ¨ç†éƒ¨ç½²](#æ¨ç†éƒ¨ç½²)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## æ˜‡è…¾å¹³å°æ¦‚è¿°

### ä»€ä¹ˆæ˜¯åä¸ºæ˜‡è…¾

**åä¸ºæ˜‡è…¾ï¼ˆAscendï¼‰** æ˜¯åä¸ºæ¨å‡ºçš„AIå¤„ç†å™¨åŠå…¨æ ˆAIè®¡ç®—å¹³å°ï¼ŒåŒ…æ‹¬ï¼š

- **æ˜‡è…¾AIå¤„ç†å™¨**ï¼šAtlasç³»åˆ—ç¡¬ä»¶
- **CANN**ï¼šå¼‚æ„è®¡ç®—æ¶æ„ï¼ˆCompute Architecture for Neural Networksï¼‰
- **MindSpore**ï¼šåä¸ºè‡ªç ”AIæ¡†æ¶
- **ModelArts**ï¼šåä¸ºäº‘AIå¼€å‘å¹³å°

### æ ¸å¿ƒä¼˜åŠ¿

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         åä¸ºæ˜‡è…¾æ ¸å¿ƒä¼˜åŠ¿                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… è‡ªä¸»å¯æ§ - å®Œå…¨å›½äº§åŒ–è§£å†³æ–¹æ¡ˆ        â”‚
â”‚ âœ… é«˜æ€§èƒ½ - è¾¾èŠ¬å¥‡æ¶æ„ï¼ŒAIç®—åŠ›å¼ºåŠ²      â”‚
â”‚ âœ… ç”Ÿæ€å®Œå–„ - å…¨æ ˆAIè§£å†³æ–¹æ¡ˆ            â”‚
â”‚ âœ… æ˜“è¿ç§» - æ”¯æŒä¸»æµAIæ¡†æ¶              â”‚
â”‚ âœ… æˆæœ¬ä¼˜åŠ¿ - æ€§èƒ½/ä»·æ ¼æ¯”ä¼˜ç§€           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç¡¬ä»¶äº§å“çº¿

| äº§å“ç³»åˆ— | å®šä½ | å…¸å‹ç®—åŠ› | åº”ç”¨åœºæ™¯ |
|---------|------|---------|---------|
| **Atlas 300I** | æ¨ç†å¡ | 256 TOPS INT8 | æ•°æ®ä¸­å¿ƒæ¨ç† |
| **Atlas 300V** | è§†é¢‘åˆ†æ | 128 TOPS INT8 | æ™ºèƒ½è§†é¢‘åˆ†æ |
| **Atlas 500** | è¾¹ç¼˜æœåŠ¡å™¨ | 128 TOPS INT8 | è¾¹ç¼˜è®¡ç®— |
| **Atlas 800** | è®­ç»ƒæœåŠ¡å™¨ | 2 PFLOPS | å¤§è§„æ¨¡è®­ç»ƒ |

### è½¯ä»¶æ ˆ

```
åº”ç”¨å±‚
â”œâ”€ MindSpore / PyTorch / TensorFlow
â”‚
ä¸­é—´å±‚
â”œâ”€ CANN (å¼‚æ„è®¡ç®—æ¶æ„)
â”‚  â”œâ”€ ATC (æ¨¡å‹è½¬æ¢å·¥å…·)
â”‚  â”œâ”€ AscendCL (C/C++ API)
â”‚  â””â”€ AIPP (å›¾åƒé¢„å¤„ç†åŠ é€Ÿ)
â”‚
é©±åŠ¨å±‚
â”œâ”€ NPUé©±åŠ¨
â”‚
ç¡¬ä»¶å±‚
â””â”€ æ˜‡è…¾AIå¤„ç†å™¨ (è¾¾èŠ¬å¥‡æ¶æ„)
```

---

## ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚

**æ”¯æŒçš„æ“ä½œç³»ç»Ÿ**ï¼š
- Ubuntu 18.04 / 20.04
- CentOS 7.6 / 8.2
- EulerOS 2.8 / 2.10
- Kylin V10

**ç¡¬ä»¶è¦æ±‚**ï¼š
- æ˜‡è…¾AIå¤„ç†å™¨ï¼ˆAtlas 300I/300V/500/800ç­‰ï¼‰
- CPUï¼šx86_64 æˆ– ARM64
- å†…å­˜ï¼šâ‰¥32GBï¼ˆæ¨è64GB+ï¼‰
- å­˜å‚¨ï¼šâ‰¥100GB

### å®‰è£…CANNå·¥å…·é“¾

#### 1. ä¸‹è½½CANNè½¯ä»¶åŒ…

è®¿é—®åä¸ºæ˜‡è…¾ç¤¾åŒºï¼šhttps://www.hiascend.com/software/cann

ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„CANNå·¥å…·åŒ…ï¼š
```bash
# ç¤ºä¾‹ï¼šCANN 6.0.1
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%206.0.1/Ascend-cann-toolkit_6.0.1_linux-x86_64.run
```

#### 2. å®‰è£…CANN Toolkit

```bash
# æ·»åŠ å¯æ‰§è¡Œæƒé™
chmod +x Ascend-cann-toolkit_*.run

# å®‰è£…ï¼ˆé»˜è®¤è·¯å¾„ï¼š/usr/local/Ascendï¼‰
./Ascend-cann-toolkit_*.run --install

# æˆ–æŒ‡å®šå®‰è£…è·¯å¾„
./Ascend-cann-toolkit_*.run --install --install-path=/opt/ascend
```

#### 3. é…ç½®ç¯å¢ƒå˜é‡

```bash
# è®¾ç½®CANNç¯å¢ƒå˜é‡
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# æ°¸ä¹…ç”Ÿæ•ˆï¼ˆæ·»åŠ åˆ°~/.bashrcï¼‰
echo "source /usr/local/Ascend/ascend-toolkit/set_env.sh" >> ~/.bashrc
```

#### 4. éªŒè¯å®‰è£…

```bash
# æŸ¥çœ‹NPUè®¾å¤‡
npu-smi info

# ç¤ºä¾‹è¾“å‡ºï¼š
# +-----------------------------------------------------------------------------+
# | npu-smi 6.0.1                    Version: 6.0.1                             |
# +-------------------------------+----------------------+----------------------+
# | NPU     Name                  | Health               | Power(W)     Temp(C) |
# | Chip    Device                | Bus-Id               | AICore(%)    Memory  |
# +===============================+======================+======================+
# | 0       Ascend910             | OK                   | 95.0         50      |
# | 0       0                     | 0000:C1:00.0         | 0            0 / 32GB|
# +-------------------------------+----------------------+----------------------+
```

### å®‰è£…Ascend-PyTorch

åä¸ºæä¾›äº†æ˜‡è…¾é€‚é…çš„PyTorchç‰ˆæœ¬ï¼š

> **âš ï¸ é‡è¦è­¦å‘Š**ï¼šPyPIä¸Šçš„torch-npuä»…ä¸ºCPUç‰ˆæœ¬ï¼Œä¸æ˜‡è…¾é©±åŠ¨ä¸å…¼å®¹ï¼å¿…é¡»ä»æ˜‡è…¾å®˜æ–¹æºå®‰è£…ã€‚

#### æ–¹å¼1ï¼šä»æ˜‡è…¾é•œåƒæºå®‰è£…ï¼ˆæ¨èï¼‰

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n ascend_pytorch python=3.8
conda activate ascend_pytorch

# é…ç½®æ˜‡è…¾PyTorché•œåƒæº
pip config set global.index-url https://repo.huaweicloud.com/repository/pypi/simple
pip config set global.trusted-host repo.huaweicloud.com

# å®‰è£…torchï¼ˆæ˜‡è…¾é€‚é…ç‰ˆæœ¬ï¼‰
pip install torch==1.11.0

# ä»æ˜‡è…¾æºå®‰è£…torch-npu
pip install torch-npu==1.11.0 -i https://repo.huaweicloud.com/repository/pypi/simple

# æˆ–ä½¿ç”¨CANNé…å¥—çš„å›ºå®šç‰ˆæœ¬
# æŸ¥çœ‹CANNç‰ˆæœ¬å¯¹åº”çš„PyTorchç‰ˆæœ¬ï¼š
# https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html
```

#### æ–¹å¼2ï¼šä¸‹è½½wheelåŒ…å®‰è£…ï¼ˆç¦»çº¿ç¯å¢ƒï¼‰

```bash
# 1. ä»æ˜‡è…¾ç¤¾åŒºä¸‹è½½å¯¹åº”çš„wheelåŒ…
# https://www.hiascend.com/zh/software/ai-frameworks/former-versions

# ç¤ºä¾‹ï¼šCANN 6.0.1 + PyTorch 1.11.0
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/PyTorch/torch-1.11.0-cp38-cp38-linux_aarch64.whl
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/PyTorch/torch_npu-1.11.0-cp38-cp38-linux_aarch64.whl

# 2. å®‰è£…
pip install torch-1.11.0-cp38-cp38-linux_aarch64.whl
pip install torch_npu-1.11.0-cp38-cp38-linux_aarch64.whl

# æ³¨æ„ï¼šæ ¹æ®æ‚¨çš„æ¶æ„é€‰æ‹©å¯¹åº”çš„wheelåŒ…
# - x86_64: linux_x86_64.whl
# - ARM64: linux_aarch64.whl
```

#### æ–¹å¼3ï¼šä»æºç ç¼–è¯‘ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰

```bash
# 1. å…‹éš†PyTorché€‚é…ä»“åº“
git clone https://gitee.com/ascend/pytorch.git
cd pytorch

# 2. åˆ‡æ¢åˆ°å¯¹åº”åˆ†æ”¯
git checkout v1.11.0-ascend

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. ç¼–è¯‘å®‰è£…
bash ci/build.sh --python=3.8

# 5. å®‰è£…torch-npu
cd ..
git clone https://gitee.com/ascend/pytorch.git torch-npu-src
cd torch-npu-src
bash ci/build.sh --python=3.8
```

#### ç‰ˆæœ¬å¯¹åº”å…³ç³»

| CANNç‰ˆæœ¬ | PyTorchç‰ˆæœ¬ | torch-npuç‰ˆæœ¬ | Pythonç‰ˆæœ¬ |
|---------|------------|--------------|-----------|
| 6.3.RC2 | 2.0.1 | 2.0.1 | 3.8, 3.9, 3.10 |
| 6.0.1 | 1.11.0 | 1.11.0 | 3.7, 3.8, 3.9 |
| 5.1.RC2 | 1.8.1 | 1.8.1 | 3.7, 3.8 |

> **æŸ¥è¯¢æœ€æ–°ç‰ˆæœ¬å¯¹åº”å…³ç³»**ï¼š  
> https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/softwareinstall/instg/atlasdeploy_03_0031.html

### éªŒè¯PyTorch-NPUç¯å¢ƒ

```python
import torch
import torch_npu

# æ£€æŸ¥NPUæ˜¯å¦å¯ç”¨
print(f"NPU available: {torch.npu.is_available()}")
print(f"NPU count: {torch.npu.device_count()}")
print(f"Current NPU: {torch.npu.current_device()}")

# æµ‹è¯•NPUè¿ç®—
x = torch.randn(3, 3).npu()
y = torch.randn(3, 3).npu()
z = x + y
print(f"Result device: {z.device}")
```

---

## PyTorchæ¨¡å‹è¿ç§»

### ä»£ç è¿ç§»ç­–ç•¥

ä»CUDAåˆ°NPUçš„è¿ç§»ä¸»è¦æ¶‰åŠè®¾å¤‡é€‚é…ï¼š

| æ“ä½œ | CUDA (NVIDIA) | NPU (Ascend) |
|------|---------------|--------------|
| **æ£€æŸ¥è®¾å¤‡** | `torch.cuda.is_available()` | `torch.npu.is_available()` |
| **è®¾å¤‡æ•°é‡** | `torch.cuda.device_count()` | `torch.npu.device_count()` |
| **ç§»åŠ¨åˆ°è®¾å¤‡** | `.cuda()` æˆ– `.to('cuda')` | `.npu()` æˆ– `.to('npu')` |
| **åŒæ­¥** | `torch.cuda.synchronize()` | `torch.npu.synchronize()` |
| **æ¸…ç©ºç¼“å­˜** | `torch.cuda.empty_cache()` | `torch.npu.empty_cache()` |

### è‡ªåŠ¨åŒ–è¿ç§»è„šæœ¬

```python
def migrate_to_npu(model, device_type='auto'):
    """
    è‡ªåŠ¨å°†æ¨¡å‹è¿ç§»åˆ°NPU
    
    Args:
        model: PyTorchæ¨¡å‹
        device_type: 'auto', 'npu', 'cuda', 'cpu'
    """
    if device_type == 'auto':
        if torch.npu.is_available():
            device = 'npu:0'
        elif torch.cuda.is_available():
            device = 'cuda:0'
        else:
            device = 'cpu'
    else:
        device = device_type
    
    model = model.to(device)
    print(f"Model migrated to: {device}")
    return model, device

# ä½¿ç”¨ç¤ºä¾‹
from transformers import CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
model, device = migrate_to_npu(model)
```

### CLIPæ¨¡å‹è¿ç§»ç¤ºä¾‹

```python
import torch
import torch_npu
from transformers import CLIPModel, CLIPProcessor
from PIL import Image

# 1. åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 2. è¿ç§»åˆ°NPU
device = "npu:0"
model = model.to(device)
model.eval()

# 3. å‡†å¤‡è¾“å…¥
image = Image.open("example.jpg")
text = ["a photo of a cat", "a photo of a dog"]

inputs = processor(text=text, images=image, return_tensors="pt", padding=True)

# 4. ç§»åŠ¨è¾“å…¥åˆ°NPU
inputs = {k: v.to(device) for k, v in inputs.items()}

# 5. æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image
    probs = logits_per_image.softmax(dim=1)

print(f"Probabilities: {probs.cpu().numpy()}")
```

### å¸¸è§é€‚é…é—®é¢˜

#### 1. ä¸æ”¯æŒçš„ç®—å­

**é—®é¢˜**ï¼šæŸäº›PyTorchç®—å­åœ¨NPUä¸Šæœªå®ç°

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ–¹æ¡ˆ1ï¼šä½¿ç”¨CPU fallback
if not hasattr(torch.npu, 'some_op'):
    result = some_op(x.cpu()).npu()
else:
    result = torch.npu.some_op(x)

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨æ›¿ä»£å®ç°
# ä¾‹å¦‚ï¼štorch.nn.functional.grid_sample å¯èƒ½éœ€è¦æ›¿ä»£
```

#### 2. æ··åˆç²¾åº¦è®­ç»ƒ

```python
# NVIDIA (AMP)
from torch.cuda.amp import autocast, GradScaler

with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

# Ascend (ç±»ä¼¼ä½†æœ‰å·®å¼‚)
import torch_npu
from torch_npu.contrib import transfer_to_npu

# ä½¿ç”¨FP16
model = model.half().npu()
inputs = inputs.half().npu()
```

#### 3. å†…å­˜ç®¡ç†

```python
# å®šæœŸæ¸…ç†NPUå†…å­˜
if torch.npu.is_available():
    torch.npu.empty_cache()
    torch.npu.synchronize()
```

---

## æ¨¡å‹è½¬æ¢ä¸ä¼˜åŒ–

### ONNXè½¬æ¢

è™½ç„¶æ˜‡è…¾æ”¯æŒPyTorchç›´æ¥æ¨ç†ï¼Œä½†é€šè¿‡ONNXè½¬æ¢å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼š

```python
import torch
import torch.onnx

# 1. åŠ è½½PyTorchæ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
model.eval()

# 2. å‡†å¤‡ç¤ºä¾‹è¾“å…¥
dummy_input = {
    'input_ids': torch.randint(0, 1000, (1, 77)),
    'pixel_values': torch.randn(1, 3, 224, 224),
    'attention_mask': torch.ones(1, 77, dtype=torch.long)
}

# 3. å¯¼å‡ºONNX
torch.onnx.export(
    model,
    (dummy_input,),
    "clip_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input_ids', 'pixel_values', 'attention_mask'],
    output_names=['output'],
    dynamic_axes={
        'input_ids': {0: 'batch_size'},
        'pixel_values': {0: 'batch_size'},
        'attention_mask': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    }
)
```

### ATCæ¨¡å‹è½¬æ¢

ATCï¼ˆAscend Tensor Compilerï¼‰æ˜¯æ˜‡è…¾çš„æ¨¡å‹è½¬æ¢å·¥å…·ï¼š

```bash
# ONNX -> OM (Offline Model)
atc --model=clip_model.onnx \
    --framework=5 \
    --output=clip_model \
    --input_format=ND \
    --input_shape="input_ids:1,77;pixel_values:1,3,224,224;attention_mask:1,77" \
    --soc_version=Ascend910 \
    --log=error

# å‚æ•°è¯´æ˜ï¼š
# --model: è¾“å…¥æ¨¡å‹è·¯å¾„
# --framework: 5è¡¨ç¤ºONNX
# --output: è¾“å‡ºOMæ¨¡å‹åç§°
# --input_format: è¾“å…¥æ ¼å¼ï¼ˆND/NCHWç­‰ï¼‰
# --input_shape: è¾“å…¥å¼ é‡å½¢çŠ¶
# --soc_version: ç›®æ ‡èŠ¯ç‰‡ç‰ˆæœ¬
```

### åŠ¨æ€shapeæ”¯æŒ

```bash
# æ”¯æŒåŠ¨æ€batch size
atc --model=clip_model.onnx \
    --framework=5 \
    --output=clip_model_dynamic \
    --input_format=ND \
    --input_shape="input_ids:-1,77;pixel_values:-1,3,224,224" \
    --dynamic_dims="1;2;4;8" \
    --soc_version=Ascend910
```

### æ¨¡å‹ä¼˜åŒ–é€‰é¡¹

```bash
# å¯ç”¨å›¾ä¼˜åŒ–
atc --model=model.onnx \
    --framework=5 \
    --output=model_optimized \
    --soc_version=Ascend910 \
    --fusion_switch_file=fusion_switch.cfg \
    --insert_op_conf=insert_op.cfg \
    --op_select_implmode=high_performance
```

**fusion_switch.cfg** ç¤ºä¾‹ï¼ˆç®—å­èåˆé…ç½®ï¼‰ï¼š
```
UB_FUSION:on
OP_BANK_PATH:./op_bank
```

---

## æ¨ç†éƒ¨ç½²

### ä½¿ç”¨AscendCLè¿›è¡Œæ¨ç†

AscendCLæ˜¯æ˜‡è…¾çš„C/C++ APIï¼Œæ€§èƒ½æœ€ä¼˜ï¼š

```cpp
#include "acl/acl.h"

// 1. åˆå§‹åŒ–ACL
aclError ret = aclInit(nullptr);

// 2. è®¾ç½®è®¾å¤‡
int32_t deviceId = 0;
ret = aclrtSetDevice(deviceId);

// 3. åŠ è½½æ¨¡å‹
uint32_t modelId;
ret = aclmdlLoadFromFile("clip_model.om", &modelId);

// 4. åˆ›å»ºæ¨¡å‹æè¿°
aclmdlDesc *modelDesc = aclmdlCreateDesc();
ret = aclmdlGetDesc(modelDesc, modelId);

// 5. å‡†å¤‡è¾“å…¥è¾“å‡º
aclmdlDataset *input = aclmdlCreateDataset();
aclmdlDataset *output = aclmdlCreateDataset();

// 6. æ‰§è¡Œæ¨ç†
ret = aclmdlExecute(modelId, input, output);

// 7. è·å–ç»“æœ
// ... å¤„ç†è¾“å‡º ...

// 8. æ¸…ç†èµ„æº
aclmdlDestroyDataset(input);
aclmdlDestroyDataset(output);
aclmdlUnload(modelId);
aclrtResetDevice(deviceId);
aclFinalize();
```

### Pythonæ¨ç†ï¼ˆç®€åŒ–ç‰ˆï¼‰

ä½¿ç”¨`acl` PythonåŒ…è£…ï¼š

```python
import acl
import numpy as np

# 1. åˆå§‹åŒ–
ret = acl.init()
device_id = 0
ret = acl.rt.set_device(device_id)

# 2. åŠ è½½æ¨¡å‹
model_path = "clip_model.om"
model_id, ret = acl.mdl.load_from_file(model_path)

# 3. åˆ›å»ºæ¨¡å‹æè¿°
model_desc = acl.mdl.create_desc()
ret = acl.mdl.get_desc(model_desc, model_id)

# 4. è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
input_size = acl.mdl.get_num_inputs(model_desc)
output_size = acl.mdl.get_num_outputs(model_desc)

print(f"Model inputs: {input_size}, outputs: {output_size}")

# 5. å‡†å¤‡è¾“å…¥æ•°æ®
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
input_dataset = acl.mdl.create_dataset()

# åˆ›å»ºæ•°æ®buffer
data_buffer = acl.create_data_buffer(input_data.tobytes(), input_data.nbytes)
acl.mdl.add_dataset_buffer(input_dataset, data_buffer)

# 6. æ‰§è¡Œæ¨ç†
output_dataset = acl.mdl.create_dataset()
ret = acl.mdl.execute(model_id, input_dataset, output_dataset)

# 7. è·å–è¾“å‡º
output_buffer = acl.mdl.get_dataset_buffer(output_dataset, 0)
output_ptr = acl.get_data_buffer_addr(output_buffer)
output_size = acl.get_data_buffer_size(output_buffer)

# è½¬æ¢ä¸ºnumpyæ•°ç»„
output_data = acl.util.ptr_to_numpy(output_ptr, (1, 512), acl.NPY_FLOAT32)

print(f"Output shape: {output_data.shape}")

# 8. æ¸…ç†èµ„æº
acl.mdl.destroy_dataset(input_dataset)
acl.mdl.destroy_dataset(output_dataset)
acl.mdl.unload(model_id)
acl.rt.reset_device(device_id)
acl.finalize()
```

### ä½¿ç”¨MindX SDK

MindX SDKæä¾›äº†æ›´é«˜çº§çš„å°è£…ï¼š

```python
from mindx.sdk import Tensor, Model
import numpy as np

# 1. åŠ è½½æ¨¡å‹
model = Model("clip_model.om", device_id=0)

# 2. å‡†å¤‡è¾“å…¥
image_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
input_tensor = Tensor(image_data)

# 3. æ¨ç†
output = model.infer([input_tensor])

# 4. è·å–ç»“æœ
result = output[0].to_numpy()
print(f"Result shape: {result.shape}")
```

---

## æ€§èƒ½ä¼˜åŒ–

### ç®—å­èåˆ

CANNæ”¯æŒå¤šç§ç®—å­èåˆæ¨¡å¼ï¼š

**å¸¸è§èåˆæ¨¡å¼**ï¼š
- Conv + BN + ReLU â†’ å•ä¸ªèåˆç®—å­
- MatMul + BiasAdd â†’ GEMM
- Reduceæ“ä½œèåˆ

**é…ç½®èåˆè§„åˆ™**ï¼š
```python
# fusion_rules.json
{
    "fusion_rules": [
        {
            "rule_name": "ConvBNRelu",
            "pattern": ["Conv2D", "BatchNorm", "Relu"],
            "fusion_type": "buffer_fusion"
        }
    ]
}
```

### AIPPå›¾åƒé¢„å¤„ç†åŠ é€Ÿ

AIPPï¼ˆAI Pre-Processingï¼‰å¯ä»¥åœ¨NPUä¸ŠåŠ é€Ÿå›¾åƒé¢„å¤„ç†ï¼š

```bash
# é…ç½®AIPP
atc --model=model.onnx \
    --framework=5 \
    --output=model_aipp \
    --soc_version=Ascend910 \
    --insert_op_conf=aipp.cfg

# aipp.cfgç¤ºä¾‹
aipp_op {
    aipp_mode: static
    input_format: YUV420SP_U8
    src_image_size_w: 1920
    src_image_size_h: 1080
    crop: true
    load_start_pos_h: 0
    load_start_pos_w: 0
    crop_size_w: 224
    crop_size_h: 224
    csc_switch: true
    rbuv_swap_switch: false
    matrix_r0c0: 256
    matrix_r0c1: 0
    matrix_r0c2: 359
    # ... æ›´å¤šé…ç½® ...
}
```

### å¤šbatchæ¨ç†

```python
# æ‰¹é‡æ¨ç†ä¼˜åŒ–
def batch_inference_optimized(model, images, batch_size=8):
    """ä¼˜åŒ–çš„æ‰¹é‡æ¨ç†"""
    results = []
    
    for i in range(0, len(images), batch_size):
        batch = images[i:i+batch_size]
        
        # Pad to batch_size
        if len(batch) < batch_size:
            batch = batch + [batch[-1]] * (batch_size - len(batch))
        
        # è½¬æ¢ä¸ºtensor
        batch_tensor = torch.stack([preprocess(img) for img in batch]).npu()
        
        # æ¨ç†
        with torch.no_grad():
            output = model(batch_tensor)
        
        # åªä¿ç•™æœ‰æ•ˆç»“æœ
        results.extend(output[:len(images[i:i+batch_size])])
    
    return results
```

### æ··åˆç²¾åº¦æ¨ç†

```python
# FP16æ¨ç†
model = model.half().npu()

# æˆ–ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
import torch_npu
from torch_npu.contrib import transfer_to_npu

@torch.jit.script
def inference_fp16(model, inputs):
    with torch.npu.amp.autocast():
        return model(inputs)
```

### æ€§èƒ½åˆ†æå·¥å…·

```bash
# ä½¿ç”¨msprofåˆ†ææ€§èƒ½
msprof --application="python inference.py" \
       --output=./profiling_data \
       --ai-core=on \
       --aicpu=on

# æŸ¥çœ‹åˆ†ææŠ¥å‘Š
msprof --export=on \
       --output=./profiling_report
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ˜‡è…¾ç¡¬ä»¶ï¼Ÿ

**ç­”**ï¼šæ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©ï¼š

| åœºæ™¯ | æ¨èç¡¬ä»¶ | ç†ç”± |
|------|---------|------|
| äº‘ç«¯æ¨ç† | Atlas 300I | é«˜æ€§èƒ½ï¼Œé€‚åˆæ•°æ®ä¸­å¿ƒ |
| è¾¹ç¼˜æ¨ç† | Atlas 500 | åŠŸè€—ä½ï¼Œé€‚åˆè¾¹ç¼˜éƒ¨ç½² |
| è§†é¢‘åˆ†æ | Atlas 300V | ä¸“ä¸ºè§†é¢‘ä¼˜åŒ– |
| æ¨¡å‹è®­ç»ƒ | Atlas 800 | ç®—åŠ›å¼ºï¼Œæ”¯æŒå¤§è§„æ¨¡è®­ç»ƒ |

### Q2: PyTorchæ¨¡å‹è¿ç§»æœ‰å“ªäº›é™åˆ¶ï¼Ÿ

**ç­”**ï¼šä¸»è¦é™åˆ¶ï¼š

1. **ç®—å­æ”¯æŒ**ï¼šéƒ¨åˆ†ç®—å­å¯èƒ½æœªå®ç°ï¼Œéœ€è¦CPU fallback
2. **åŠ¨æ€shape**ï¼šæŸäº›æ“ä½œä¸æ”¯æŒåŠ¨æ€shape
3. **è‡ªå®šä¹‰ç®—å­**ï¼šéœ€è¦é‡æ–°å®ç°
4. **ç²¾åº¦**ï¼šFP16å¯èƒ½æœ‰ç²¾åº¦æŸå¤±

**æ£€æŸ¥ç®—å­æ”¯æŒ**ï¼š
```python
# æŸ¥çœ‹ä¸æ”¯æŒçš„ç®—å­
import torch
import torch_npu

model = ...
dummy_input = ...

try:
    model.npu()
    output = model(dummy_input.npu())
except RuntimeError as e:
    print(f"Error: {e}")
    # åˆ†æé”™è¯¯ä¿¡æ¯ï¼Œæ‰¾å‡ºä¸æ”¯æŒçš„ç®—å­
```

### Q3: æ€§èƒ½ä¸å¦‚é¢„æœŸæ€ä¹ˆåŠï¼Ÿ

**æ’æŸ¥æ­¥éª¤**ï¼š

1. **æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¼˜åŒ–**
   ```bash
   # ä½¿ç”¨profilingå·¥å…·åˆ†æ
   msprof --application="python your_script.py"
   ```

2. **å¯ç”¨ç®—å­èåˆ**
   ```bash
   atc --model=model.onnx --fusion_switch_file=fusion.cfg
   ```

3. **ä½¿ç”¨æ‰¹é‡æ¨ç†**
   - å¢å¤§batch sizeä»¥æé«˜ååé‡

4. **æ£€æŸ¥æ•°æ®ä¼ è¾“**
   - å‡å°‘CPU-NPUæ•°æ®ä¼ è¾“
   - ä½¿ç”¨pinned memory

5. **ä½¿ç”¨FP16**
   - åœ¨ç²¾åº¦å¯æ¥å—çš„æƒ…å†µä¸‹ä½¿ç”¨åŠç²¾åº¦

### Q4: å¦‚ä½•debug NPUç¨‹åºï¼Ÿ

**æ–¹æ³•**ï¼š

1. **æ—¥å¿—è¾“å‡º**ï¼š
   ```python
   import os
   os.environ['ASCEND_GLOBAL_LOG_LEVEL'] = '0'  # 0=debug, 1=info, 2=warning, 3=error
   ```

2. **æ•°æ®éªŒè¯**ï¼š
   ```python
   # å¯¹æ¯”CPUå’ŒNPUç»“æœ
   output_cpu = model_cpu(input_cpu)
   output_npu = model_npu(input_npu).cpu()
   diff = torch.abs(output_cpu - output_npu).max()
   print(f"Max difference: {diff}")
   ```

3. **ä½¿ç”¨profilingå·¥å…·**ï¼š
   ```bash
   msprof --application="python debug.py" --ai-core=on
   ```

---

## æœ€ä½³å®è·µ

### 1. æ¨¡å‹å¼€å‘æµç¨‹

```
1. åœ¨NVIDIA GPUä¸Šå¼€å‘å’Œè®­ç»ƒ
   â†“
2. éªŒè¯æ¨¡å‹åŠŸèƒ½å’Œç²¾åº¦
   â†“
3. å¯¼å‡ºONNXæ¨¡å‹
   â†“
4. åœ¨æ˜‡è…¾å¹³å°ä¸Šæµ‹è¯•PyTorchæ¨ç†
   â†“
5. ä½¿ç”¨ATCè½¬æ¢ä¸ºOMæ¨¡å‹
   â†“
6. æ€§èƒ½ä¼˜åŒ–å’Œprofiling
   â†“
7. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
```

### 2. ä»£ç å…¼å®¹æ€§è®¾è®¡

```python
class DeviceAgnosticModel:
    """è®¾å¤‡æ— å…³çš„æ¨¡å‹å°è£…"""
    
    def __init__(self, model_path, device='auto'):
        self.model = self.load_model(model_path)
        self.device = self.get_device(device)
        self.model = self.model.to(self.device)
    
    def get_device(self, device):
        """è‡ªåŠ¨é€‰æ‹©è®¾å¤‡"""
        if device == 'auto':
            if torch.npu.is_available():
                return 'npu:0'
            elif torch.cuda.is_available():
                return 'cuda:0'
            else:
                return 'cpu'
        return device
    
    def infer(self, inputs):
        """ç»Ÿä¸€çš„æ¨ç†æ¥å£"""
        inputs = self.to_device(inputs)
        with torch.no_grad():
            outputs = self.model(inputs)
        return outputs
    
    def to_device(self, data):
        """æ•°æ®è¿ç§»"""
        if isinstance(data, dict):
            return {k: v.to(self.device) for k, v in data.items()}
        elif isinstance(data, torch.Tensor):
            return data.to(self.device)
        else:
            return data
```

### 3. æ€§èƒ½ç›‘æ§

```python
import time
import torch
import torch_npu

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, device='npu'):
        self.device = device
        self.reset()
    
    def reset(self):
        self.times = []
        self.memory_usage = []
    
    def __enter__(self):
        if 'npu' in self.device:
            torch.npu.synchronize()
        self.start_time = time.time()
        return self
    
    def __exit__(self, *args):
        if 'npu' in self.device:
            torch.npu.synchronize()
        elapsed = time.time() - self.start_time
        self.times.append(elapsed)
        
        if 'npu' in self.device:
            memory = torch.npu.memory_allocated() / 1024**2  # MB
            self.memory_usage.append(memory)
    
    def report(self):
        avg_time = sum(self.times) / len(self.times) if self.times else 0
        max_memory = max(self.memory_usage) if self.memory_usage else 0
        
        print(f"Average inference time: {avg_time*1000:.2f}ms")
        print(f"Peak memory usage: {max_memory:.2f}MB")
        print(f"Throughput: {1/avg_time:.2f} samples/sec")

# ä½¿ç”¨
monitor = PerformanceMonitor(device='npu')

for i in range(100):
    with monitor:
        output = model(input_data)

monitor.report()
```

### 4. é”™è¯¯å¤„ç†

```python
def safe_npu_inference(model, inputs, fallback_to_cpu=True):
    """å®‰å…¨çš„NPUæ¨ç†ï¼Œæ”¯æŒCPU fallback"""
    try:
        # å°è¯•NPUæ¨ç†
        inputs_npu = {k: v.npu() for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(inputs_npu)
        return outputs.cpu()
    
    except Exception as e:
        print(f"NPU inference failed: {e}")
        
        if fallback_to_cpu:
            print("Falling back to CPU...")
            model_cpu = model.cpu()
            with torch.no_grad():
                outputs = model_cpu(inputs)
            return outputs
        else:
            raise
```

### 5. æ‰¹é‡éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy_to_ascend.sh - è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬

set -e

# é…ç½®
MODEL_NAME="clip_model"
MODEL_PATH="models/${MODEL_NAME}.onnx"
OUTPUT_DIR="./ascend_models"
SOC_VERSION="Ascend910"

# 1. æ£€æŸ¥ç¯å¢ƒ
echo "Checking Ascend environment..."
npu-smi info || { echo "NPU not available!"; exit 1; }

# 2. è½¬æ¢æ¨¡å‹
echo "Converting model to OM format..."
atc --model=${MODEL_PATH} \
    --framework=5 \
    --output=${OUTPUT_DIR}/${MODEL_NAME} \
    --soc_version=${SOC_VERSION} \
    --log=info

# 3. éªŒè¯æ¨¡å‹
echo "Validating converted model..."
python validate_model.py \
    --om_model=${OUTPUT_DIR}/${MODEL_NAME}.om \
    --onnx_model=${MODEL_PATH}

# 4. æ€§èƒ½æµ‹è¯•
echo "Running performance test..."
python benchmark.py \
    --model=${OUTPUT_DIR}/${MODEL_NAME}.om \
    --batch_size=1,4,8 \
    --iterations=100

echo "Deployment completed successfully!"
```

---

## æ€»ç»“

### å…³é”®è¦ç‚¹

1. **ç¯å¢ƒé…ç½®**ï¼šæ­£ç¡®å®‰è£…CANNå’Œtorch_npu
2. **ä»£ç è¿ç§»**ï¼šä¸»è¦æ˜¯è®¾å¤‡é€‚é…ï¼ˆcudaâ†’npuï¼‰
3. **æ¨¡å‹è½¬æ¢**ï¼šONNXâ†’OMï¼Œä½¿ç”¨ATCå·¥å…·
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šç®—å­èåˆã€AIPPã€æ··åˆç²¾åº¦
5. **å…¼å®¹æ€§è®¾è®¡**ï¼šç¼–å†™è®¾å¤‡æ— å…³çš„ä»£ç 

### ä¼˜åŠ¿ä¸å±€é™

**ä¼˜åŠ¿**ï¼š
- âœ… è‡ªä¸»å¯æ§ï¼Œå›½äº§åŒ–æ›¿ä»£
- âœ… æ€§èƒ½å¼ºåŠ²ï¼Œæˆæœ¬ä¼˜åŠ¿
- âœ… ç”Ÿæ€å®Œå–„ï¼Œå·¥å…·é“¾æˆç†Ÿ

**å±€é™**ï¼š
- âš ï¸ ç®—å­æ”¯æŒä¸å¦‚CUDAå®Œæ•´
- âš ï¸ ç¤¾åŒºç”Ÿæ€ä»åœ¨å‘å±•
- âš ï¸ å­¦ä¹ æ›²çº¿ç›¸å¯¹é™¡å³­

### é€‚ç”¨åœºæ™¯

æ¨èåœ¨ä»¥ä¸‹åœºæ™¯ä½¿ç”¨æ˜‡è…¾ï¼š

1. **å›½äº§åŒ–è¦æ±‚**ï¼šæ”¿åºœã€é‡‘èç­‰è¡Œä¸š
2. **å¤§è§„æ¨¡éƒ¨ç½²**ï¼šæ•°æ®ä¸­å¿ƒæ‰¹é‡æ¨ç†
3. **æˆæœ¬æ•æ„Ÿ**ï¼šè¿½æ±‚æ€§ä»·æ¯”çš„åœºæ™¯
4. **è§†é¢‘åˆ†æ**ï¼šæ™ºæ…§åŸå¸‚ã€å®‰é˜²ç­‰

---

## å‚è€ƒèµ„æº

- **å®˜æ–¹æ–‡æ¡£**ï¼šhttps://www.hiascend.com/document
- **CANNå¼€å‘æŒ‡å—**ï¼šhttps://www.hiascend.com/zh/software/cann
- **MindSporeæ•™ç¨‹**ï¼šhttps://www.mindspore.cn/tutorials
- **æ˜‡è…¾ç¤¾åŒº**ï¼šhttps://www.hiascend.com/forum
- **ModelZoo**ï¼šhttps://www.hiascend.com/zh/software/modelzoo

---

*æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œå¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿åé¦ˆï¼*

