# 01 - NVIDIAéƒ¨ç½²åŸºç¡€

> ğŸ“š **å­¦ä¹ ç›®æ ‡**  
> - äº†è§£NVIDIA GPUéƒ¨ç½²çš„åŸºç¡€çŸ¥è¯†
> - æŒæ¡PyTorchæ¨¡å‹çš„éƒ¨ç½²æ–¹æ³•
> - å­¦ä¼šä½¿ç”¨ONNXè¿›è¡Œæ¨¡å‹è½¬æ¢å’Œä¼˜åŒ–

> ğŸ¯ **å…ˆä¿®è¦æ±‚**  
> - å®Œæˆ [æ¨¡å‹å¾®è°ƒæŠ€æœ¯](../02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/) éƒ¨åˆ†
> - ç†Ÿæ‚‰PyTorchåŸºç¡€
> - æœ‰NVIDIA GPUç¯å¢ƒ

> â±ï¸ **é¢„è®¡å­¦ä¹ æ—¶é—´**: 60-90åˆ†é’Ÿ  
> ğŸ·ï¸ **éš¾åº¦**: â­â­â­â­â˜† é«˜çº§

> âœ… **ä»£ç å¯ç”¨æ€§**  
> æœ¬æ•™ç¨‹çš„ç¤ºä¾‹ä»£ç å°†åœ¨ä¸‹ä¸€æ­¥å®ç°ï¼š
> - éƒ¨ç½²è„šæœ¬: `code/04-deployment/nvidia/`
> - é…ç½®æ–‡ä»¶å’Œå·¥å…·

---

## ğŸ“– ç›®å½•

- [éƒ¨ç½²æ¦‚è¿°](#éƒ¨ç½²æ¦‚è¿°)
- [PyTorchéƒ¨ç½²](#pytorchéƒ¨ç½²)
- [ONNXè½¬æ¢ä¸ä¼˜åŒ–](#onnxè½¬æ¢ä¸ä¼˜åŒ–)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [éƒ¨ç½²æœ€ä½³å®è·µ](#éƒ¨ç½²æœ€ä½³å®è·µ)

---

## éƒ¨ç½²æ¦‚è¿°

### ä»€ä¹ˆæ˜¯æ¨¡å‹éƒ¨ç½²

**æ¨¡å‹éƒ¨ç½²ï¼ˆModel Deploymentï¼‰** æ˜¯å°†è®­ç»ƒå¥½çš„æ¨¡å‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œä¸ºå®é™…åº”ç”¨æä¾›æ¨ç†æœåŠ¡çš„è¿‡ç¨‹ã€‚

### éƒ¨ç½²æµç¨‹

```
è®­ç»ƒå¥½çš„æ¨¡å‹
    â†“
æ¨¡å‹è½¬æ¢/ä¼˜åŒ–
    â†“
éƒ¨ç½²åˆ°æ¨ç†æœåŠ¡å™¨
    â†“
æä¾›APIæœåŠ¡
    â†“
ç›‘æ§å’Œç»´æŠ¤
```

### NVIDIAéƒ¨ç½²æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | å¤æ‚åº¦ | æ€§èƒ½ | çµæ´»æ€§ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|--------|---------|
| **PyTorchç›´æ¥æ¨ç†** | â­ | â­â­â­ | â­â­â­â­â­ | å¼€å‘ã€åŸå‹ |
| **TorchScript** | â­â­ | â­â­â­â­ | â­â­â­â­ | ç”Ÿäº§ç¯å¢ƒ |
| **ONNX Runtime** | â­â­â­ | â­â­â­â­ | â­â­â­ | è·¨å¹³å° |
| **TensorRT** | â­â­â­â­ | â­â­â­â­â­ | â­â­ | é«˜æ€§èƒ½æ¨ç† |
| **Triton Server** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | ä¼ä¸šçº§éƒ¨ç½² |

---

## PyTorchéƒ¨ç½²

### 1. ç›´æ¥ä½¿ç”¨PyTorchæ¨¡å‹

**æœ€ç®€å•çš„éƒ¨ç½²æ–¹å¼**:

```python
import torch
from transformers import CLIPModel, CLIPProcessor
from PIL import Image

class CLIPInferenceService:
    """CLIPæ¨ç†æœåŠ¡"""
    
    def __init__(self, model_path: str, device: str = "cuda"):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        
        # åŠ è½½æ¨¡å‹
        self.model = CLIPModel.from_pretrained(model_path)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        
        # ç§»åŠ¨åˆ°GPU
        self.model = self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    @torch.no_grad()
    def predict(self, image_path: str, texts: list):
        """
        å›¾æ–‡åŒ¹é…æ¨ç†
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            texts: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            é¢„æµ‹ç»“æœå’Œæ¦‚ç‡
        """
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        # é¢„å¤„ç†
        inputs = self.processor(
            text=texts,
            images=image,
            return_tensors="pt",
            padding=True
        )
        
        # ç§»åŠ¨åˆ°GPU
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æ¨ç†
        outputs = self.model(**inputs)
        logits = outputs.logits_per_image
        probs = logits.softmax(dim=1)[0]
        
        # è¿”å›ç»“æœ
        results = [
            {"text": text, "probability": prob.item()}
            for text, prob in zip(texts, probs)
        ]
        
        return sorted(results, key=lambda x: x['probability'], reverse=True)

# ä½¿ç”¨ç¤ºä¾‹
service = CLIPInferenceService("openai/clip-vit-base-patch32")

results = service.predict(
    image_path="dog.jpg",
    texts=["a photo of a dog", "a photo of a cat", "a photo of a bird"]
)

for result in results:
    print(f"{result['text']}: {result['probability']:.4f}")
```

**ä¼˜ç‚¹**:
- âœ… ç®€å•æ˜“ç”¨
- âœ… å¼€å‘å¿«é€Ÿ
- âœ… è°ƒè¯•æ–¹ä¾¿

**ç¼ºç‚¹**:
- âŒ æ€§èƒ½ä¸€èˆ¬
- âŒ ä¾èµ–å®Œæ•´çš„PyTorchç¯å¢ƒ
- âŒ æ¨¡å‹æ–‡ä»¶è¾ƒå¤§

### 2. ä½¿ç”¨TorchScript

**TorchScript** å¯ä»¥å°†PyTorchæ¨¡å‹åºåˆ—åŒ–ä¸ºç‹¬ç«‹çš„ä¸­é—´è¡¨ç¤ºï¼Œæå‡æ€§èƒ½ã€‚

#### æ¨¡å‹è½¬æ¢

```python
import torch
from transformers import CLIPModel

# åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
model.eval()

# å‡†å¤‡ç¤ºä¾‹è¾“å…¥
dummy_pixel_values = torch.randn(1, 3, 224, 224)
dummy_input_ids = torch.randint(0, 49408, (1, 77))
dummy_attention_mask = torch.ones(1, 77, dtype=torch.long)

# æ–¹æ³•1: Tracingï¼ˆæ¨èç”¨äºè§†è§‰æ¨¡å‹ï¼‰
with torch.no_grad():
    traced_model = torch.jit.trace(
        model.vision_model,
        dummy_pixel_values
    )

# ä¿å­˜
traced_model.save("clip_vision_traced.pt")
print("âœ… TorchScriptæ¨¡å‹å·²ä¿å­˜")

# æ–¹æ³•2: Scriptingï¼ˆç”¨äºæœ‰æ§åˆ¶æµçš„æ¨¡å‹ï¼‰
try:
    scripted_model = torch.jit.script(model.vision_model)
    scripted_model.save("clip_vision_scripted.pt")
except Exception as e:
    print(f"âš ï¸  Scriptingå¤±è´¥: {e}")
```

#### åŠ è½½å’Œæ¨ç†

```python
# åŠ è½½TorchScriptæ¨¡å‹
loaded_model = torch.jit.load("clip_vision_traced.pt")
loaded_model.eval()
loaded_model = loaded_model.cuda()

# æ¨ç†
with torch.no_grad():
    outputs = loaded_model(dummy_pixel_values.cuda())
    print(f"è¾“å‡ºå½¢çŠ¶: {outputs.pooler_output.shape}")
```

**ä¼˜ç‚¹**:
- âœ… æ€§èƒ½æå‡10-20%
- âœ… å¯ä»¥åœ¨C++ä¸­ä½¿ç”¨
- âœ… æ¨¡å‹æ›´ç´§å‡‘

**ç¼ºç‚¹**:
- âŒ ä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒ
- âŒ è°ƒè¯•å›°éš¾
- âŒ ä»éœ€PyTorchè¿è¡Œæ—¶

### 3. æ¨¡å‹é‡åŒ–

**é‡åŒ–ï¼ˆQuantizationï¼‰** å¯ä»¥å‡å°‘æ¨¡å‹å¤§å°å’Œæå‡æ¨ç†é€Ÿåº¦ã€‚

#### åŠ¨æ€é‡åŒ–

```python
import torch
from torch.quantization import quantize_dynamic

# åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
model.eval()

# åŠ¨æ€é‡åŒ–ï¼ˆæ¨ç†æ—¶é‡åŒ–ï¼‰
quantized_model = quantize_dynamic(
    model.vision_model,
    {torch.nn.Linear},  # é‡åŒ–Linearå±‚
    dtype=torch.qint8   # ä½¿ç”¨int8
)

# ä¿å­˜
torch.save(quantized_model.state_dict(), "clip_vision_quantized.pth")

# æ¨ç†
with torch.no_grad():
    outputs = quantized_model(dummy_pixel_values)
```

**æ•ˆæœ**:
- æ¨¡å‹å¤§å°: å‡å°‘75%ï¼ˆFP32 â†’ INT8ï¼‰
- æ¨ç†é€Ÿåº¦: æå‡2-4xï¼ˆCPUï¼‰
- ç²¾åº¦æŸå¤±: <1%

---

## ONNXè½¬æ¢ä¸ä¼˜åŒ–

### ä»€ä¹ˆæ˜¯ONNX

**ONNXï¼ˆOpen Neural Network Exchangeï¼‰** æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ¨¡å‹è¡¨ç¤ºæ ¼å¼ï¼Œæ”¯æŒè·¨æ¡†æ¶å’Œè·¨å¹³å°éƒ¨ç½²ã€‚

### ä¸ºä»€ä¹ˆä½¿ç”¨ONNX

1. **è·¨å¹³å°**: ä¸€æ¬¡è½¬æ¢ï¼Œå¤šå¤„éƒ¨ç½²
2. **é«˜æ€§èƒ½**: ONNX Runtimeä¼˜åŒ–
3. **å¹¿æ³›æ”¯æŒ**: TensorRTã€OpenVINOç­‰
4. **ç”Ÿæ€ä¸°å¯Œ**: å·¥å…·é“¾å®Œå–„

### 1. æ¨¡å‹è½¬æ¢

#### è½¬æ¢CLIPè§†è§‰ç¼–ç å™¨

```python
import torch
from transformers import CLIPModel, CLIPProcessor

# åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
model.eval()

# å‡†å¤‡ç¤ºä¾‹è¾“å…¥
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
dummy_image = torch.randn(1, 3, 224, 224)

# å¯¼å‡ºä¸ºONNX
torch.onnx.export(
    model.vision_model,                    # æ¨¡å‹
    dummy_image,                           # ç¤ºä¾‹è¾“å…¥
    "clip_vision.onnx",                    # è¾“å‡ºæ–‡ä»¶
    input_names=['pixel_values'],          # è¾“å…¥åç§°
    output_names=['pooler_output'],        # è¾“å‡ºåç§°
    dynamic_axes={                         # åŠ¨æ€ç»´åº¦
        'pixel_values': {0: 'batch_size'},
        'pooler_output': {0: 'batch_size'}
    },
    opset_version=14,                      # ONNX opsetç‰ˆæœ¬
    do_constant_folding=True,              # å¸¸é‡æŠ˜å ä¼˜åŒ–
)

print("âœ… ONNXæ¨¡å‹å·²å¯¼å‡º")
```

#### è½¬æ¢æ–‡æœ¬ç¼–ç å™¨

```python
# å‡†å¤‡æ–‡æœ¬è¾“å…¥
dummy_input_ids = torch.randint(0, 49408, (1, 77))
dummy_attention_mask = torch.ones(1, 77, dtype=torch.long)

# å¯¼å‡ºæ–‡æœ¬ç¼–ç å™¨
torch.onnx.export(
    model.text_model,
    (dummy_input_ids, dummy_attention_mask),
    "clip_text.onnx",
    input_names=['input_ids', 'attention_mask'],
    output_names=['pooler_output'],
    dynamic_axes={
        'input_ids': {0: 'batch_size'},
        'attention_mask': {0: 'batch_size'},
        'pooler_output': {0: 'batch_size'}
    },
    opset_version=14,
)

print("âœ… æ–‡æœ¬ç¼–ç å™¨ONNXæ¨¡å‹å·²å¯¼å‡º")
```

### 2. ONNXæ¨¡å‹éªŒè¯

```python
import onnx
import onnxruntime as ort
import numpy as np

# éªŒè¯ONNXæ¨¡å‹
onnx_model = onnx.load("clip_vision.onnx")
onnx.checker.check_model(onnx_model)
print("âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
print("\næ¨¡å‹ä¿¡æ¯:")
print(f"  IRç‰ˆæœ¬: {onnx_model.ir_version}")
print(f"  Opsetç‰ˆæœ¬: {onnx_model.opset_import[0].version}")
print(f"  ç”Ÿäº§è€…: {onnx_model.producer_name}")

# æŸ¥çœ‹è¾“å…¥è¾“å‡º
print("\nè¾“å…¥:")
for input in onnx_model.graph.input:
    print(f"  {input.name}: {[d.dim_value for d in input.type.tensor_type.shape.dim]}")

print("\nè¾“å‡º:")
for output in onnx_model.graph.output:
    print(f"  {output.name}: {[d.dim_value for d in output.type.tensor_type.shape.dim]}")
```

### 3. ONNX Runtimeæ¨ç†

```python
import onnxruntime as ort
import numpy as np
from PIL import Image

class ONNXInferenceService:
    """ONNXæ¨ç†æœåŠ¡"""
    
    def __init__(self, onnx_path: str, use_gpu: bool = True):
        # é…ç½®providers
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if use_gpu else ['CPUExecutionProvider']
        
        # åˆ›å»ºæ¨ç†ä¼šè¯
        self.session = ort.InferenceSession(
            onnx_path,
            providers=providers
        )
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        
        print(f"âœ… ONNXæ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   Provider: {self.session.get_providers()}")
    
    def preprocess(self, image_path: str):
        """å›¾åƒé¢„å¤„ç†"""
        from torchvision import transforms
        
        transform = transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711]
            ),
        ])
        
        image = Image.open(image_path).convert('RGB')
        image_tensor = transform(image).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        return image_tensor.numpy()
    
    def predict(self, image_path: str):
        """æ¨ç†"""
        # é¢„å¤„ç†
        input_data = self.preprocess(image_path)
        
        # æ¨ç†
        outputs = self.session.run(
            [self.output_name],
            {self.input_name: input_data}
        )
        
        return outputs[0]

# ä½¿ç”¨ç¤ºä¾‹
service = ONNXInferenceService("clip_vision.onnx", use_gpu=True)
output = service.predict("dog.jpg")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
```

### 4. ONNXæ¨¡å‹ä¼˜åŒ–

```python
import onnx
from onnxruntime.transformers import optimizer

# åŠ è½½æ¨¡å‹
model = onnx.load("clip_vision.onnx")

# ä¼˜åŒ–
optimized_model = optimizer.optimize_model(
    "clip_vision.onnx",
    model_type='bert',  # ä½¿ç”¨BERTä¼˜åŒ–å™¨ï¼ˆTransformeræ¶æ„ï¼‰
    num_heads=12,
    hidden_size=768,
    optimization_options=None
)

# ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
optimized_model.save_model_to_file("clip_vision_optimized.onnx")
print("âœ… ONNXæ¨¡å‹å·²ä¼˜åŒ–")
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹å¤„ç†ï¼ˆBatchingï¼‰

```python
class BatchedInferenceService:
    """æ”¯æŒæ‰¹å¤„ç†çš„æ¨ç†æœåŠ¡"""
    
    def __init__(self, model_path: str, batch_size: int = 8):
        self.model = CLIPModel.from_pretrained(model_path).cuda()
        self.processor = CLIPProcessor.from_pretrained(model_path)
        self.batch_size = batch_size
        self.model.eval()
    
    @torch.no_grad()
    def predict_batch(self, image_paths: list):
        """æ‰¹é‡æ¨ç†"""
        results = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(image_paths), self.batch_size):
            batch_paths = image_paths[i:i + self.batch_size]
            
            # åŠ è½½å›¾åƒ
            images = [Image.open(path).convert('RGB') for path in batch_paths]
            
            # é¢„å¤„ç†
            inputs = self.processor(
                images=images,
                return_tensors="pt",
                padding=True
            )
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # æ¨ç†
            outputs = self.model.get_image_features(**inputs)
            results.append(outputs.cpu())
        
        return torch.cat(results, dim=0)

# ä½¿ç”¨
service = BatchedInferenceService("openai/clip-vit-base-patch32", batch_size=16)
features = service.predict_batch(image_paths)
```

**æ•ˆæœ**: æ‰¹å¤„ç†å¯ä»¥æå‡3-5xååé‡

### 2. æ··åˆç²¾åº¦æ¨ç†

```python
# ä½¿ç”¨FP16æ¨ç†
model = model.half()  # è½¬æ¢ä¸ºFP16
inputs = {k: v.half() if v.dtype == torch.float32 else v 
          for k, v in inputs.items()}

# æˆ–ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
with torch.cuda.amp.autocast():
    outputs = model(**inputs)
```

**æ•ˆæœ**: 
- æ˜¾å­˜å ç”¨å‡åŠ
- æ¨ç†é€Ÿåº¦æå‡1.5-2x
- ç²¾åº¦æŸå¤±<0.5%

### 3. æ¨¡å‹ç¼“å­˜

```python
from functools import lru_cache

class CachedInferenceService:
    """å¸¦ç¼“å­˜çš„æ¨ç†æœåŠ¡"""
    
    def __init__(self, model_path: str):
        self.model = CLIPModel.from_pretrained(model_path).cuda()
        self.processor = CLIPProcessor.from_pretrained(model_path)
        self.model.eval()
    
    @lru_cache(maxsize=1000)
    def get_text_features(self, text: str):
        """ç¼“å­˜æ–‡æœ¬ç‰¹å¾"""
        inputs = self.processor(text=text, return_tensors="pt", padding=True)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            features = self.model.get_text_features(**inputs)
        
        return features.cpu()
```

---

## éƒ¨ç½²æœ€ä½³å®è·µ

### 1. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

```python
# ä½¿ç”¨ç‰ˆæœ¬å·ç®¡ç†æ¨¡å‹
MODEL_VERSIONS = {
    'v1.0': 'models/clip_v1.0',
    'v1.1': 'models/clip_v1.1',
    'latest': 'models/clip_latest'
}

def load_model(version='latest'):
    model_path = MODEL_VERSIONS.get(version)
    return CLIPModel.from_pretrained(model_path)
```

### 2. å¥åº·æ£€æŸ¥

```python
def health_check():
    """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
    try:
        # æµ‹è¯•æ¨ç†
        dummy_input = torch.randn(1, 3, 224, 224).cuda()
        with torch.no_grad():
            _ = model(dummy_input)
        
        return {"status": "healthy", "gpu_available": torch.cuda.is_available()}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

### 3. æ€§èƒ½ç›‘æ§

```python
import time

class MonitoredInferenceService:
    """å¸¦ç›‘æ§çš„æ¨ç†æœåŠ¡"""
    
    def __init__(self, model_path: str):
        self.model = CLIPModel.from_pretrained(model_path).cuda()
        self.processor = CLIPProcessor.from_pretrained(model_path)
        self.model.eval()
        
        # ç›‘æ§æŒ‡æ ‡
        self.total_requests = 0
        self.total_time = 0.0
    
    @torch.no_grad()
    def predict(self, image_path: str, texts: list):
        start_time = time.time()
        
        # æ¨ç†
        image = Image.open(image_path).convert('RGB')
        inputs = self.processor(text=texts, images=image, return_tensors="pt", padding=True)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        outputs = self.model(**inputs)
        
        # æ›´æ–°ç›‘æ§æŒ‡æ ‡
        inference_time = time.time() - start_time
        self.total_requests += 1
        self.total_time += inference_time
        
        return outputs, inference_time
    
    def get_metrics(self):
        """è·å–ç›‘æ§æŒ‡æ ‡"""
        return {
            'total_requests': self.total_requests,
            'average_latency': self.total_time / max(self.total_requests, 1),
            'total_time': self.total_time
        }
```

### 4. é”™è¯¯å¤„ç†

```python
class RobustInferenceService:
    """å¥å£®çš„æ¨ç†æœåŠ¡"""
    
    def predict(self, image_path: str, texts: list):
        try:
            # éªŒè¯è¾“å…¥
            if not os.path.exists(image_path):
                raise ValueError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            
            if not texts or len(texts) == 0:
                raise ValueError("æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            
            # æ¨ç†
            image = Image.open(image_path).convert('RGB')
            inputs = self.processor(text=texts, images=image, return_tensors="pt", padding=True)
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            return {"success": True, "outputs": outputs}
        
        except Exception as e:
            return {"success": False, "error": str(e)}
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | å»¶è¿Ÿ | ååé‡ | æ˜¾å­˜ | æ¨¡å‹å¤§å° |
|------|------|--------|------|---------|
| **PyTorch FP32** | 20ms | 50 img/s | 2.5GB | 600MB |
| **PyTorch FP16** | 12ms | 80 img/s | 1.3GB | 600MB |
| **TorchScript** | 18ms | 55 img/s | 2.5GB | 600MB |
| **ONNX Runtime** | 15ms | 65 img/s | 2.0GB | 600MB |
| **ONNX + TensorRT** | 8ms | 120 img/s | 1.5GB | 400MB |

*æµ‹è¯•ç¯å¢ƒ: NVIDIA RTX 3090, Batch Size=1*

---

## â¡ï¸ ä¸‹ä¸€æ­¥

- [02-TensorRTä¼˜åŒ–](./02-TensorRTä¼˜åŒ–.md) - å­¦ä¹ TensorRTåŠ é€Ÿï¼ˆå¾…å¼€å‘ï¼‰
- [03-Tritonæ¨ç†æœåŠ¡å™¨](./03-Tritonæ¨ç†æœåŠ¡å™¨.md) - ä¼ä¸šçº§éƒ¨ç½²ï¼ˆå¾…å¼€å‘ï¼‰
- [ä»£ç å®ç°](../../code/04-deployment/nvidia/) - æŸ¥çœ‹å®Œæ•´ä»£ç 

---

## ğŸ“š å‚è€ƒèµ„æº

- [PyTorchéƒ¨ç½²æ–‡æ¡£](https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html)
- [ONNXå®˜æ–¹æ–‡æ¡£](https://onnx.ai/)
- [ONNX Runtimeæ–‡æ¡£](https://onnxruntime.ai/)
- [TensorRTæ–‡æ¡£](https://developer.nvidia.com/tensorrt)

