# 02 - æ¨¡å‹æœåŠ¡åŒ–

> ğŸ“š **å­¦ä¹ ç›®æ ‡**  
> - å­¦ä¼šå°†æ¨¡å‹å°è£…ä¸ºWebæœåŠ¡
> - æŒæ¡FastAPI/Flaskçš„ä½¿ç”¨
> - äº†è§£æœåŠ¡åŒ–éƒ¨ç½²çš„æœ€ä½³å®è·µ

> ğŸ¯ **å…ˆä¿®è¦æ±‚**  
> - å®Œæˆ [01-NVIDIAéƒ¨ç½²åŸºç¡€](./01-NVIDIAéƒ¨ç½²åŸºç¡€.md)
> - ç†Ÿæ‚‰Python Webå¼€å‘åŸºç¡€

> â±ï¸ **é¢„è®¡å­¦ä¹ æ—¶é—´**: 45-60åˆ†é’Ÿ  
> ğŸ·ï¸ **éš¾åº¦**: â­â­â­â˜†â˜† ä¸­çº§

---

## ğŸ“– ç›®å½•

- [ä¸ºä»€ä¹ˆéœ€è¦æœåŠ¡åŒ–](#ä¸ºä»€ä¹ˆéœ€è¦æœåŠ¡åŒ–)
- [ä½¿ç”¨FastAPIæ„å»ºæœåŠ¡](#ä½¿ç”¨fastapiæ„å»ºæœåŠ¡)
- [Dockerå®¹å™¨åŒ–](#dockerå®¹å™¨åŒ–)
- [ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²](#ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²)

---

## ä¸ºä»€ä¹ˆéœ€è¦æœåŠ¡åŒ–

### æœåŠ¡åŒ–çš„ä¼˜åŠ¿

1. **æ ‡å‡†åŒ–æ¥å£**: RESTful APIï¼Œæ˜“äºé›†æˆ
2. **è§£è€¦**: æ¨¡å‹å’Œåº”ç”¨åˆ†ç¦»
3. **å¯æ‰©å±•**: æ°´å¹³æ‰©å±•ï¼Œè´Ÿè½½å‡è¡¡
4. **æ˜“ç»´æŠ¤**: ç‹¬ç«‹æ›´æ–°ï¼Œç‰ˆæœ¬ç®¡ç†

---

## ä½¿ç”¨FastAPIæ„å»ºæœåŠ¡

### 1. åŸºç¡€æœåŠ¡

```python
from fastapi import FastAPI, File, UploadFile, Form
from fastapi.responses import JSONResponse
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor
import io

app = FastAPI(title="CLIPæ¨ç†æœåŠ¡", version="1.0.0")

# å…¨å±€æ¨¡å‹å®ä¾‹
model = None
processor = None

@app.on_event("startup")
async def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model, processor
    
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    
    model = model.cuda()
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "gpu_available": torch.cuda.is_available()
    }

@app.post("/predict")
async def predict(
    image: UploadFile = File(...),
    texts: str = Form(...)
):
    """
    å›¾æ–‡åŒ¹é…æ¨ç†
    
    Args:
        image: ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
        texts: é€—å·åˆ†éš”çš„å€™é€‰æ–‡æœ¬
    """
    try:
        # è§£ææ–‡æœ¬
        text_list = [t.strip() for t in texts.split(',')]
        
        # è¯»å–å›¾åƒ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # é¢„å¤„ç†
        inputs = processor(
            text=text_list,
            images=pil_image,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits_per_image
            probs = logits.softmax(dim=1)[0]
        
        # è¿”å›ç»“æœ
        results = [
            {"text": text, "probability": float(prob)}
            for text, prob in zip(text_list, probs)
        ]
        
        return JSONResponse({
            "success": True,
            "results": sorted(results, key=lambda x: x['probability'], reverse=True)
        })
    
    except Exception as e:
        return JSONResponse({
            "success": False,
            "error": str(e)
        }, status_code=500)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. å¯åŠ¨æœåŠ¡

```bash
# å®‰è£…ä¾èµ–
pip install fastapi uvicorn python-multipart

# å¯åŠ¨æœåŠ¡
python app.py

# æˆ–ä½¿ç”¨uvicorn
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

### 3. æµ‹è¯•API

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æ¨ç†è¯·æ±‚
curl -X POST "http://localhost:8000/predict" \
  -F "image=@dog.jpg" \
  -F "texts=a photo of a dog,a photo of a cat,a photo of a bird"
```

---

## Dockerå®¹å™¨åŒ–

### 1. Dockerfile

```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…Python
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 2. requirements.txt

```
torch==2.0.0
torchvision==0.15.0
transformers==4.35.0
fastapi==0.104.0
uvicorn[standard]==0.24.0
python-multipart==0.0.6
pillow==10.1.0
```

### 3. æ„å»ºå’Œè¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t clip-service:latest .

# è¿è¡Œå®¹å™¨
docker run --gpus all -p 8000:8000 clip-service:latest
```

---

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 1. ä½¿ç”¨Gunicorn + Uvicorn

```bash
# å®‰è£…
pip install gunicorn

# å¯åŠ¨ï¼ˆå¤šworkerï¼‰
gunicorn app:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000 \
  --timeout 120
```

### 2. Nginxåå‘ä»£ç†

```nginx
upstream clip_service {
    server localhost:8000;
    server localhost:8001;
    server localhost:8002;
}

server {
    listen 80;
    server_name api.example.com;
    
    location / {
        proxy_pass http://clip_service;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 3. ç›‘æ§å’Œæ—¥å¿—

```python
import logging
from prometheus_client import Counter, Histogram

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# PrometheusæŒ‡æ ‡
REQUEST_COUNT = Counter('requests_total', 'Total requests')
REQUEST_LATENCY = Histogram('request_latency_seconds', 'Request latency')

@app.post("/predict")
@REQUEST_LATENCY.time()
async def predict(...):
    REQUEST_COUNT.inc()
    logger.info(f"Received prediction request")
    # ... æ¨ç†é€»è¾‘
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

- [01-NVIDIAéƒ¨ç½²åŸºç¡€](./01-NVIDIAéƒ¨ç½²åŸºç¡€.md) - å›é¡¾éƒ¨ç½²åŸºç¡€
- [ä»£ç å®ç°](../../code/04-deployment/nvidia/) - æŸ¥çœ‹å®Œæ•´ä»£ç 

---

## ğŸ“š å‚è€ƒèµ„æº

- [FastAPIæ–‡æ¡£](https://fastapi.tiangolo.com/)
- [Dockeræ–‡æ¡£](https://docs.docker.com/)
- [Nginxæ–‡æ¡£](https://nginx.org/en/docs/)

