# æœ€ä½³å®è·µ

æœ¬æ–‡æ¡£æ€»ç»“è§†è§‰å¤§æ¨¡å‹å¼€å‘å’Œéƒ¨ç½²çš„æœ€ä½³å®è·µï¼Œå¸®åŠ©ä½ é¿å…å¸¸è§é™·é˜±ï¼Œæé«˜å¼€å‘æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

---

## ğŸ“‹ ç›®å½•

1. [æ•°æ®å‡†å¤‡æœ€ä½³å®è·µ](#æ•°æ®å‡†å¤‡æœ€ä½³å®è·µ)
2. [æ¨¡å‹è®­ç»ƒæœ€ä½³å®è·µ](#æ¨¡å‹è®­ç»ƒæœ€ä½³å®è·µ)
3. [æ¨¡å‹è¯„ä¼°æœ€ä½³å®è·µ](#æ¨¡å‹è¯„ä¼°æœ€ä½³å®è·µ)
4. [éƒ¨ç½²ä¼˜åŒ–æœ€ä½³å®è·µ](#éƒ¨ç½²ä¼˜åŒ–æœ€ä½³å®è·µ)
5. [ä»£ç ç»„ç»‡æœ€ä½³å®è·µ](#ä»£ç ç»„ç»‡æœ€ä½³å®è·µ)
6. [æ€§èƒ½ç›‘æ§æœ€ä½³å®è·µ](#æ€§èƒ½ç›‘æ§æœ€ä½³å®è·µ)

---

## æ•°æ®å‡†å¤‡æœ€ä½³å®è·µ

### 1. æ•°æ®è´¨é‡æ£€æŸ¥

```python
import os
from PIL import Image
from tqdm import tqdm

def validate_dataset(data_dir):
    """éªŒè¯æ•°æ®é›†è´¨é‡"""
    issues = {
        'corrupt_images': [],
        'small_images': [],
        'wrong_format': [],
        'missing_labels': []
    }
    
    for root, dirs, files in os.walk(data_dir):
        for file in tqdm(files):
            if not file.lower().endswith(('.jpg', '.jpeg', '.png')):
                continue
            
            filepath = os.path.join(root, file)
            
            try:
                # å°è¯•æ‰“å¼€å›¾ç‰‡
                img = Image.open(filepath)
                img.verify()  # éªŒè¯å›¾ç‰‡å®Œæ•´æ€§
                
                # é‡æ–°æ‰“å¼€ï¼ˆverifyåéœ€è¦é‡æ–°æ‰“å¼€ï¼‰
                img = Image.open(filepath)
                
                # æ£€æŸ¥å°ºå¯¸
                if img.size[0] < 224 or img.size[1] < 224:
                    issues['small_images'].append(filepath)
                
                # æ£€æŸ¥æ ¼å¼
                if img.mode not in ['RGB', 'L']:
                    issues['wrong_format'].append(filepath)
                
            except Exception as e:
                issues['corrupt_images'].append((filepath, str(e)))
    
    # æ‰“å°æŠ¥å‘Š
    print("\n=== æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š ===")
    for issue_type, items in issues.items():
        if items:
            print(f"\n{issue_type}: {len(items)}")
            for item in items[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - {item}")
    
    return issues

# ä½¿ç”¨
issues = validate_dataset("data/train")

# æ¸…ç†é—®é¢˜æ•°æ®
for filepath in issues['corrupt_images']:
    os.remove(filepath[0])
    print(f"Removed: {filepath[0]}")
```

### 2. æ•°æ®å¢å¼ºç­–ç•¥

```python
from torchvision import transforms
import albumentations as A
from albumentations.pytorch import ToTensorV2

def get_transforms(mode='train', use_advanced=False):
    """
    è·å–æ•°æ®å¢å¼ºç­–ç•¥
    
    Args:
        mode: 'train' or 'val'
        use_advanced: æ˜¯å¦ä½¿ç”¨é«˜çº§å¢å¼º
    """
    if mode == 'train':
        if use_advanced:
            # é«˜çº§å¢å¼ºï¼ˆä½¿ç”¨albumentationsï¼‰
            transform = A.Compose([
                A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),
                A.HorizontalFlip(p=0.5),
                A.ShiftScaleRotate(
                    shift_limit=0.1,
                    scale_limit=0.1,
                    rotate_limit=15,
                    p=0.5
                ),
                A.OneOf([
                    A.GaussNoise(p=1),
                    A.GaussianBlur(p=1),
                    A.MotionBlur(p=1),
                ], p=0.3),
                A.OneOf([
                    A.RandomBrightnessContrast(p=1),
                    A.HueSaturationValue(p=1),
                ], p=0.3),
                A.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                ),
                ToTensorV2()
            ])
        else:
            # åŸºç¡€å¢å¼ºï¼ˆä½¿ç”¨torchvisionï¼‰
            transform = transforms.Compose([
                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
                transforms.RandomHorizontalFlip(),
                transforms.ColorJitter(
                    brightness=0.2,
                    contrast=0.2,
                    saturation=0.2,
                    hue=0.1
                ),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
    else:
        # éªŒè¯é›†ï¼šåªåšå¿…è¦çš„é¢„å¤„ç†
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
    
    return transform
```

### 3. æ•°æ®åˆ’åˆ†ç­–ç•¥

```python
from sklearn.model_selection import train_test_split
import numpy as np

def split_dataset(data_dir, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,
                 random_state=42):
    """
    åˆ’åˆ†æ•°æ®é›†
    
    ç¡®ä¿ï¼š
    1. ç±»åˆ«å¹³è¡¡
    2. å¯é‡ç°
    3. æ²¡æœ‰æ•°æ®æ³„æ¼
    """
    assert train_ratio + val_ratio + test_ratio == 1.0
    
    # æ”¶é›†æ‰€æœ‰æ ·æœ¬
    samples = []
    labels = []
    
    for class_name in os.listdir(data_dir):
        class_path = os.path.join(data_dir, class_name)
        if not os.path.isdir(class_path):
            continue
        
        for img_name in os.listdir(class_path):
            if not img_name.lower().endswith(('.jpg', '.png')):
                continue
            
            samples.append(os.path.join(class_path, img_name))
            labels.append(class_name)
    
    # åˆ†å±‚åˆ’åˆ†ï¼ˆä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰
    train_samples, temp_samples, train_labels, temp_labels = train_test_split(
        samples, labels,
        test_size=(1 - train_ratio),
        stratify=labels,
        random_state=random_state
    )
    
    val_samples, test_samples, val_labels, test_labels = train_test_split(
        temp_samples, temp_labels,
        test_size=test_ratio / (val_ratio + test_ratio),
        stratify=temp_labels,
        random_state=random_state
    )
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"Total samples: {len(samples)}")
    print(f"Train: {len(train_samples)} ({len(train_samples)/len(samples):.1%})")
    print(f"Val: {len(val_samples)} ({len(val_samples)/len(samples):.1%})")
    print(f"Test: {len(test_samples)} ({len(test_samples)/len(samples):.1%})")
    
    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
    from collections import Counter
    print("\nClass distribution:")
    print("Train:", Counter(train_labels))
    print("Val:", Counter(val_labels))
    print("Test:", Counter(test_labels))
    
    return {
        'train': (train_samples, train_labels),
        'val': (val_samples, val_labels),
        'test': (test_samples, test_labels)
    }
```

### 4. æ•°æ®åŠ è½½å™¨ä¼˜åŒ–

```python
from torch.utils.data import DataLoader, Dataset
import multiprocessing

def get_optimal_dataloader(dataset, batch_size, mode='train'):
    """
    åˆ›å»ºä¼˜åŒ–çš„DataLoader
    
    æœ€ä½³å®è·µï¼š
    1. num_workers = CPUæ ¸å¿ƒæ•°
    2. pin_memory = Trueï¼ˆæœ‰GPUæ—¶ï¼‰
    3. persistent_workers = Trueï¼ˆå‡å°‘å¯åŠ¨å¼€é”€ï¼‰
    4. prefetch_factor = 2-4ï¼ˆé¢„å–æ•°æ®ï¼‰
    """
    num_workers = min(8, multiprocessing.cpu_count())  # æœ€å¤š8ä¸ªworker
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(mode == 'train'),
        num_workers=num_workers,
        pin_memory=True,  # åŠ é€Ÿæ•°æ®ä¼ è¾“åˆ°GPU
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=2 if num_workers > 0 else None,
        drop_last=(mode == 'train')  # è®­ç»ƒæ—¶ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batch
    )
    
    return dataloader
```

---

## æ¨¡å‹è®­ç»ƒæœ€ä½³å®è·µ

### 1. è®­ç»ƒé…ç½®ç®¡ç†

```python
import yaml
from dataclasses import dataclass, asdict

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ï¼ˆä½¿ç”¨dataclassä¾¿äºç®¡ç†ï¼‰"""
    
    # æ¨¡å‹é…ç½®
    model_name: str = "openai/clip-vit-base-patch32"
    num_classes: int = 10
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 32
    num_epochs: int = 50
    learning_rate: float = 1e-5
    weight_decay: float = 0.01
    warmup_steps: int = 500
    
    # æ•°æ®é…ç½®
    data_dir: str = "data/train"
    val_split: float = 0.1
    num_workers: int = 4
    
    # ä¼˜åŒ–é…ç½®
    use_mixed_precision: bool = True
    gradient_clip: float = 1.0
    accumulation_steps: int = 1
    
    # ä¿å­˜é…ç½®
    save_dir: str = "checkpoints"
    save_every: int = 5
    keep_last_n: int = 3
    
    # æ—¥å¿—é…ç½®
    log_every: int = 100
    use_tensorboard: bool = True
    use_wandb: bool = False
    
    def save(self, filepath):
        """ä¿å­˜é…ç½®åˆ°YAMLæ–‡ä»¶"""
        with open(filepath, 'w') as f:
            yaml.dump(asdict(self), f)
    
    @classmethod
    def load(cls, filepath):
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
        with open(filepath) as f:
            config_dict = yaml.safe_load(f)
        return cls(**config_dict)

# ä½¿ç”¨
config = TrainingConfig(
    model_name="openai/clip-vit-base-patch32",
    batch_size=64,
    learning_rate=2e-5
)

# ä¿å­˜é…ç½®
config.save("configs/experiment1.yaml")

# åŠ è½½é…ç½®
config = TrainingConfig.load("configs/experiment1.yaml")
```

### 2. å­¦ä¹ ç‡è°ƒåº¦

```python
import torch
from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR
import math

def get_lr_scheduler(optimizer, config, num_training_steps):
    """
    è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    æ¨èç­–ç•¥ï¼š
    1. å°æ•°æ®é›†ï¼šCosineAnnealing with warmup
    2. å¤§æ•°æ®é›†ï¼šOneCycleLR
    3. å¾®è°ƒï¼šLinear warmup + decay
    """
    
    # æ–¹æ¡ˆ1ï¼šCosine Annealing (æ¨èç”¨äºå¾®è°ƒ)
    def cosine_with_warmup(step):
        if step < config.warmup_steps:
            # Linear warmup
            return step / config.warmup_steps
        else:
            # Cosine decay
            progress = (step - config.warmup_steps) / (num_training_steps - config.warmup_steps)
            return 0.5 * (1.0 + math.cos(math.pi * progress))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(
        optimizer,
        lr_lambda=cosine_with_warmup
    )
    
    return scheduler
    
    # æ–¹æ¡ˆ2ï¼šOneCycleLR (æ¨èç”¨äºä»å¤´è®­ç»ƒ)
    # scheduler = OneCycleLR(
    #     optimizer,
    #     max_lr=config.learning_rate,
    #     total_steps=num_training_steps,
    #     pct_start=0.1  # 10%çš„æ­¥æ•°ç”¨äºwarmup
    # )
    
    # æ–¹æ¡ˆ3ï¼šReduceLROnPlateau (åŸºäºéªŒè¯æŒ‡æ ‡)
    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    #     optimizer,
    #     mode='max',
    #     factor=0.5,
    #     patience=3,
    #     verbose=True
    # )
```

### 3. æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†

```python
import torch
import os
from pathlib import Path

class CheckpointManager:
    """æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, save_dir, keep_last_n=3, save_best=True):
        """
        Args:
            save_dir: ä¿å­˜ç›®å½•
            keep_last_n: ä¿ç•™æœ€è¿‘Nä¸ªæ£€æŸ¥ç‚¹
            save_best: æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
        """
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        self.keep_last_n = keep_last_n
        self.save_best = save_best
        self.best_metric = float('-inf')
        
        self.checkpoints = []
    
    def save(self, model, optimizer, scheduler, epoch, metrics):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = self.save_dir / f"checkpoint_epoch_{epoch}.pth"
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'metrics': metrics
        }, checkpoint_path)
        
        self.checkpoints.append(checkpoint_path)
        print(f"Saved checkpoint: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if self.save_best and metrics.get('val_acc', 0) > self.best_metric:
            self.best_metric = metrics['val_acc']
            best_path = self.save_dir / "best_model.pth"
            torch.save(model.state_dict(), best_path)
            print(f"Saved best model: {best_path} (acc={self.best_metric:.4f})")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        if len(self.checkpoints) > self.keep_last_n:
            old_checkpoint = self.checkpoints.pop(0)
            if old_checkpoint.exists():
                old_checkpoint.unlink()
                print(f"Removed old checkpoint: {old_checkpoint}")
    
    def load_latest(self, model, optimizer=None, scheduler=None):
        """åŠ è½½æœ€æ–°æ£€æŸ¥ç‚¹"""
        if not self.checkpoints:
            # æŸ¥æ‰¾ç°æœ‰æ£€æŸ¥ç‚¹
            self.checkpoints = sorted(self.save_dir.glob("checkpoint_epoch_*.pth"))
        
        if not self.checkpoints:
            print("No checkpoint found")
            return 0
        
        latest_checkpoint = self.checkpoints[-1]
        checkpoint = torch.load(latest_checkpoint)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        if optimizer:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if scheduler and checkpoint.get('scheduler_state_dict'):
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        print(f"Loaded checkpoint: {latest_checkpoint}")
        return checkpoint['epoch']
    
    def load_best(self, model):
        """åŠ è½½æœ€ä½³æ¨¡å‹"""
        best_path = self.save_dir / "best_model.pth"
        if not best_path.exists():
            print("No best model found")
            return
        
        model.load_state_dict(torch.load(best_path))
        print(f"Loaded best model: {best_path}")

# ä½¿ç”¨
checkpoint_mgr = CheckpointManager(
    save_dir="checkpoints/experiment1",
    keep_last_n=3,
    save_best=True
)

# è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader, optimizer)
    val_acc = validate(model, val_loader)
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_mgr.save(
        model, optimizer, scheduler, epoch,
        metrics={'train_loss': train_loss, 'val_acc': val_acc}
    )
```

### 4. æ—©åœç­–ç•¥

```python
class EarlyStopping:
    """æ—©åœç­–ç•¥"""
    
    def __init__(self, patience=7, min_delta=0.001, mode='max'):
        """
        Args:
            patience: å®¹å¿çš„epochæ•°
            min_delta: æœ€å°æ”¹è¿›å¹…åº¦
            mode: 'max' (å‡†ç¡®ç‡) or 'min' (æŸå¤±)
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, metric):
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        Returns:
            should_stop: bool
        """
        score = metric if self.mode == 'max' else -metric
        
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0
        
        return self.early_stop

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=5, min_delta=0.001, mode='max')

for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_acc = validate()
    
    # æ£€æŸ¥æ˜¯å¦æ—©åœ
    if early_stopping(val_acc):
        print(f"Early stopping at epoch {epoch}")
        break
```

### 5. æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦

```python
from torch.cuda.amp import autocast, GradScaler

def train_one_epoch(model, dataloader, optimizer, scheduler, config):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    
    æœ€ä½³å®è·µï¼š
    1. ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿè®­ç»ƒ
    2. æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§batch size
    3. æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    """
    model.train()
    device = next(model.parameters()).device
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if config.use_mixed_precision else None
    
    running_loss = 0.0
    optimizer.zero_grad()
    
    for step, (images, labels) in enumerate(dataloader):
        images, labels = images.to(device), labels.to(device)
        
        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        if scaler:
            with autocast():
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss = loss / config.accumulation_steps
        else:
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss = loss / config.accumulation_steps
        
        # åå‘ä¼ æ’­
        if scaler:
            scaler.scale(loss).backward()
        else:
            loss.backward()
        
        # æ¢¯åº¦ç´¯ç§¯
        if (step + 1) % config.accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            if config.gradient_clip > 0:
                if scaler:
                    scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(
                    model.parameters(),
                    config.gradient_clip
                )
            
            # æ›´æ–°å‚æ•°
            if scaler:
                scaler.step(optimizer)
                scaler.update()
            else:
                optimizer.step()
            
            optimizer.zero_grad()
            
            if scheduler:
                scheduler.step()
        
        running_loss += loss.item() * config.accumulation_steps
        
        # æ—¥å¿—
        if (step + 1) % config.log_every == 0:
            avg_loss = running_loss / (step + 1)
            lr = optimizer.param_groups[0]['lr']
            print(f"Step {step+1}/{len(dataloader)}, "
                  f"Loss: {avg_loss:.4f}, LR: {lr:.6f}")
    
    return running_loss / len(dataloader)
```

---

## æ¨¡å‹è¯„ä¼°æœ€ä½³å®è·µ

### 1. å¤šæŒ‡æ ‡è¯„ä¼°

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, classification_report
)
import numpy as np

def comprehensive_evaluate(model, dataloader, class_names=None):
    """
    å…¨é¢è¯„ä¼°æ¨¡å‹
    
    è¿”å›å¤šä¸ªæŒ‡æ ‡ï¼š
    - Accuracy
    - Precision/Recall/F1
    - Confusion Matrix
    - Per-class metrics
    """
    model.eval()
    device = next(model.parameters()).device
    
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            outputs = model(images)
            
            probs = torch.softmax(outputs, dim=1)
            preds = outputs.argmax(dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())
            all_probs.extend(probs.cpu().numpy())
    
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = {
        'accuracy': accuracy_score(all_labels, all_preds),
        'precision_macro': precision_score(all_labels, all_preds, average='macro'),
        'recall_macro': recall_score(all_labels, all_preds, average='macro'),
        'f1_macro': f1_score(all_labels, all_preds, average='macro'),
        'precision_weighted': precision_score(all_labels, all_preds, average='weighted'),
        'recall_weighted': recall_score(all_labels, all_preds, average='weighted'),
        'f1_weighted': f1_score(all_labels, all_preds, average='weighted'),
    }
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    
    # åˆ†ç±»æŠ¥å‘Š
    report = classification_report(
        all_labels, all_preds,
        target_names=class_names,
        digits=4
    )
    
    # æ‰“å°ç»“æœ
    print("\n=== è¯„ä¼°ç»“æœ ===")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"Macro F1: {metrics['f1_macro']:.4f}")
    print(f"Weighted F1: {metrics['f1_weighted']:.4f}")
    print(f"\n{report}")
    
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
    if class_names:
        plot_confusion_matrix(cm, class_names)
    
    return metrics, cm, all_probs

def plot_confusion_matrix(cm, class_names):
    """å¯è§†åŒ–æ··æ·†çŸ©é˜µ"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names
    )
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    print("Confusion matrix saved to confusion_matrix.png")
```

### 2. äº¤å‰éªŒè¯

```python
from sklearn.model_selection import StratifiedKFold

def k_fold_cross_validation(model_fn, dataset, k=5, config=None):
    """
    KæŠ˜äº¤å‰éªŒè¯
    
    Args:
        model_fn: åˆ›å»ºæ¨¡å‹çš„å‡½æ•°
        dataset: å®Œæ•´æ•°æ®é›†
        k: æŠ˜æ•°
        config: è®­ç»ƒé…ç½®
    """
    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(
        range(len(dataset)),
        dataset.labels  # éœ€è¦datasetæä¾›labelså±æ€§
    )):
        print(f"\n=== Fold {fold + 1}/{k} ===")
        
        # åˆ›å»ºå­æ•°æ®é›†
        train_subset = torch.utils.data.Subset(dataset, train_idx)
        val_subset = torch.utils.data.Subset(dataset, val_idx)
        
        train_loader = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=config.batch_size)
        
        # åˆ›å»ºæ–°æ¨¡å‹
        model = model_fn()
        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
        
        # è®­ç»ƒ
        best_val_acc = 0
        for epoch in range(config.num_epochs):
            train_loss = train_one_epoch(model, train_loader, optimizer)
            val_acc = validate(model, val_loader)
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
        
        fold_results.append(best_val_acc)
        print(f"Fold {fold + 1} Best Val Acc: {best_val_acc:.4f}")
    
    # ç»Ÿè®¡
    mean_acc = np.mean(fold_results)
    std_acc = np.std(fold_results)
    
    print(f"\n=== Cross Validation Results ===")
    print(f"Mean Accuracy: {mean_acc:.4f} Â± {std_acc:.4f}")
    print(f"All folds: {fold_results}")
    
    return fold_results
```

---

## éƒ¨ç½²ä¼˜åŒ–æœ€ä½³å®è·µ

### 1. æ¨¡å‹å¯¼å‡º

```python
def export_model_for_production(model, save_dir="models/production"):
    """
    å¯¼å‡ºç”Ÿäº§ç¯å¢ƒæ¨¡å‹
    
    å¯¼å‡ºå¤šç§æ ¼å¼ï¼š
    1. PyTorchæƒé‡ï¼ˆ.pthï¼‰
    2. TorchScriptï¼ˆ.ptï¼‰
    3. ONNXï¼ˆ.onnxï¼‰
    """
    os.makedirs(save_dir, exist_ok=True)
    
    model.eval()
    device = next(model.parameters()).device
    
    # 1. PyTorchæƒé‡
    torch.save(model.state_dict(), f"{save_dir}/model_weights.pth")
    print(f"Saved PyTorch weights: {save_dir}/model_weights.pth")
    
    # 2. TorchScript (JITç¼–è¯‘)
    dummy_input = torch.randn(1, 3, 224, 224).to(device)
    traced_model = torch.jit.trace(model, dummy_input)
    traced_model.save(f"{save_dir}/model_traced.pt")
    print(f"Saved TorchScript: {save_dir}/model_traced.pt")
    
    # 3. ONNX
    torch.onnx.export(
        model,
        dummy_input,
        f"{save_dir}/model.onnx",
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    print(f"Saved ONNX: {save_dir}/model.onnx")
    
    # 4. ä¿å­˜æ¨¡å‹é…ç½®
    config = {
        'model_name': model.__class__.__name__,
        'input_size': [3, 224, 224],
        'num_classes': model.num_classes if hasattr(model, 'num_classes') else None,
        'preprocessing': {
            'mean': [0.485, 0.456, 0.406],
            'std': [0.229, 0.224, 0.225]
        }
    }
    
    import json
    with open(f"{save_dir}/config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\nModel exported successfully to {save_dir}/")
```

### 2. æ‰¹å¤„ç†æ¨ç†

```python
class BatchInferenceEngine:
    """æ‰¹å¤„ç†æ¨ç†å¼•æ“"""
    
    def __init__(self, model, batch_size=32, device="cuda"):
        self.model = model
        self.model.eval()
        self.batch_size = batch_size
        self.device = device
        self.queue = []
        self.results = {}
    
    def add_request(self, request_id, image):
        """æ·»åŠ æ¨ç†è¯·æ±‚"""
        self.queue.append((request_id, image))
        
        # å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œæ‰§è¡Œæ‰¹å¤„ç†
        if len(self.queue) >= self.batch_size:
            self._process_batch()
    
    def _process_batch(self):
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡"""
        if not self.queue:
            return
        
        # æ”¶é›†æ•°æ®
        request_ids = [item[0] for item in self.queue]
        images = torch.stack([item[1] for item in self.queue]).to(self.device)
        
        # æ‰¹é‡æ¨ç†
        with torch.no_grad():
            outputs = self.model(images)
        
        # åˆ†é…ç»“æœ
        for i, request_id in enumerate(request_ids):
            self.results[request_id] = outputs[i].cpu()
        
        # æ¸…ç©ºé˜Ÿåˆ—
        self.queue = []
    
    def get_result(self, request_id, timeout=5.0):
        """è·å–æ¨ç†ç»“æœ"""
        import time
        start_time = time.time()
        
        while request_id not in self.results:
            if time.time() - start_time > timeout:
                # è¶…æ—¶ï¼Œå¼ºåˆ¶å¤„ç†å½“å‰æ‰¹æ¬¡
                self._process_batch()
                break
            time.sleep(0.01)
        
        return self.results.pop(request_id, None)
    
    def flush(self):
        """å¤„ç†é˜Ÿåˆ—ä¸­å‰©ä½™çš„è¯·æ±‚"""
        self._process_batch()
```

### 3. APIé™æµå’Œç¼“å­˜

```python
from functools import lru_cache
import hashlib
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from fastapi import FastAPI, Request

# åˆ›å»ºé™æµå™¨
limiter = Limiter(key_func=get_remote_address)
app = FastAPI()
app.state.limiter = limiter
app.add_exception_handler(429, _rate_limit_exceeded_handler)

# ç»“æœç¼“å­˜
@lru_cache(maxsize=1000)
def cached_inference(image_hash):
    """ç¼“å­˜æ¨ç†ç»“æœ"""
    # å®é™…æ¨ç†é€»è¾‘
    pass

@app.post("/predict")
@limiter.limit("10/minute")  # é™æµï¼šæ¯åˆ†é’Ÿ10æ¬¡
async def predict(request: Request, file: UploadFile):
    """
    é¢„æµ‹æ¥å£
    
    æœ€ä½³å®è·µï¼š
    1. é™æµé˜²æ­¢æ»¥ç”¨
    2. ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
    3. å¼‚æ­¥å¤„ç†æé«˜åå
    """
    # è¯»å–å›¾ç‰‡
    image_bytes = await file.read()
    
    # è®¡ç®—hashï¼ˆç”¨äºç¼“å­˜ï¼‰
    image_hash = hashlib.md5(image_bytes).hexdigest()
    
    # å°è¯•ä»ç¼“å­˜è·å–
    result = cached_inference(image_hash)
    
    if result is None:
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæ¨ç†
        image = preprocess_image(image_bytes)
        with torch.no_grad():
            result = model(image)
    
    return {"prediction": result.tolist()}
```

---

## ä»£ç ç»„ç»‡æœ€ä½³å®è·µ

### 1. é¡¹ç›®ç»“æ„

```
project/
â”œâ”€â”€ configs/                # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ base.yaml
â”‚   â”œâ”€â”€ experiment1.yaml
â”‚   â””â”€â”€ production.yaml
â”œâ”€â”€ data/                   # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ splits/
â”œâ”€â”€ models/                 # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ clip_classifier.py
â”‚   â””â”€â”€ custom_model.py
â”œâ”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ scripts/                # è„šæœ¬
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ export.py
â”œâ”€â”€ tests/                  # æµ‹è¯•
â”‚   â”œâ”€â”€ test_data.py
â”‚   â””â”€â”€ test_models.py
â”œâ”€â”€ notebooks/              # Jupyter notebooks
â”œâ”€â”€ checkpoints/            # æ£€æŸ¥ç‚¹
â”œâ”€â”€ logs/                   # æ—¥å¿—
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### 2. é…ç½®æ–‡ä»¶ç®¡ç†

ä½¿ç”¨YAMLç®¡ç†æ‰€æœ‰é…ç½®ï¼Œé¿å…ç¡¬ç¼–ç ï¼š

```yaml
# configs/base.yaml
model:
  name: "openai/clip-vit-base-patch32"
  num_classes: 10

training:
  batch_size: 32
  num_epochs: 50
  learning_rate: 1e-5
  weight_decay: 0.01
  
data:
  train_dir: "data/train"
  val_dir: "data/val"
  num_workers: 4

logging:
  log_dir: "logs"
  save_dir: "checkpoints"
  log_every: 100
```

### 3. æ—¥å¿—è®°å½•

```python
import logging
from pathlib import Path

def setup_logging(log_dir="logs", experiment_name="default"):
    """è®¾ç½®æ—¥å¿—"""
    log_dir = Path(log_dir) / experiment_name
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_dir / "train.log"),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    return logger

# ä½¿ç”¨
logger = setup_logging()
logger.info("Training started")
logger.warning("Warning message")
logger.error("Error message")
```

---

## æ€§èƒ½ç›‘æ§æœ€ä½³å®è·µ

### 1. TensorBoardé›†æˆ

```python
from torch.utils.tensorboard import SummaryWriter

class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir="logs"):
        self.writer = SummaryWriter(log_dir)
        self.step = 0
    
    def log_scalars(self, metrics, prefix="train"):
        """è®°å½•æ ‡é‡æŒ‡æ ‡"""
        for key, value in metrics.items():
            self.writer.add_scalar(f"{prefix}/{key}", value, self.step)
    
    def log_images(self, images, predictions, labels, n=8):
        """è®°å½•å›¾ç‰‡å’Œé¢„æµ‹ç»“æœ"""
        import torchvision
        grid = torchvision.utils.make_grid(images[:n])
        self.writer.add_image('predictions', grid, self.step)
    
    def log_model_graph(self, model, input_size=(1, 3, 224, 224)):
        """è®°å½•æ¨¡å‹ç»“æ„"""
        dummy_input = torch.randn(input_size)
        self.writer.add_graph(model, dummy_input)
    
    def step_forward(self):
        """å‰è¿›ä¸€æ­¥"""
        self.step += 1

# ä½¿ç”¨
logger = TrainingLogger("runs/experiment1")

for epoch in range(num_epochs):
    train_metrics = train_one_epoch()
    val_metrics = validate()
    
    logger.log_scalars(train_metrics, prefix="train")
    logger.log_scalars(val_metrics, prefix="val")
    logger.step_forward()

# å¯åŠ¨TensorBoard:  tensorboard --logdir=runs
```

### 2. GPUç›‘æ§

```python
import pynvml

class GPUMonitor:
    """GPUç›‘æ§å™¨"""
    
    def __init__(self):
        pynvml.nvmlInit()
        self.device_count = pynvml.nvmlDeviceGetCount()
    
    def get_stats(self):
        """è·å–GPUç»Ÿè®¡ä¿¡æ¯"""
        stats = []
        
        for i in range(self.device_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            
            # å†…å­˜ä¿¡æ¯
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            
            # æ¸©åº¦
            temp = pynvml.nvmlDeviceGetTemperature(
                handle,
                pynvml.NVML_TEMPERATURE_GPU
            )
            
            # åˆ©ç”¨ç‡
            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            
            stats.append({
                'gpu_id': i,
                'memory_used_mb': mem_info.used / 1024 / 1024,
                'memory_total_mb': mem_info.total / 1024 / 1024,
                'memory_percent': mem_info.used / mem_info.total * 100,
                'temperature': temp,
                'gpu_utilization': util.gpu,
                'memory_utilization': util.memory
            })
        
        return stats
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        
        print("\n=== GPU Stats ===")
        for stat in stats:
            print(f"GPU {stat['gpu_id']}:")
            print(f"  Memory: {stat['memory_used_mb']:.0f}/{stat['memory_total_mb']:.0f} MB ({stat['memory_percent']:.1f}%)")
            print(f"  Temperature: {stat['temperature']}Â°C")
            print(f"  Utilization: {stat['gpu_utilization']}%")

# ä½¿ç”¨
monitor = GPUMonitor()
monitor.print_stats()
```

---

## ğŸ¯ æ€»ç»“

éµå¾ªè¿™äº›æœ€ä½³å®è·µå¯ä»¥ï¼š

1. **æé«˜å¼€å‘æ•ˆç‡**: æ ‡å‡†åŒ–çš„æµç¨‹å’Œå·¥å…·
2. **ä¿è¯æ¨¡å‹è´¨é‡**: ä¸¥æ ¼çš„éªŒè¯å’Œè¯„ä¼°
3. **ä¼˜åŒ–æ€§èƒ½**: è®­ç»ƒå’Œæ¨ç†åŠ é€Ÿ
4. **ä¾¿äºç»´æŠ¤**: æ¸…æ™°çš„ä»£ç ç»„ç»‡
5. **æ˜“äºéƒ¨ç½²**: å¤šæ ¼å¼å¯¼å‡ºå’Œä¼˜åŒ–

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [å¸¸è§é—®é¢˜FAQ](./03-å¸¸è§é—®é¢˜FAQ.md)
- [æ•…éšœæ’æŸ¥æŒ‡å—](./05-æ•…éšœæ’æŸ¥æŒ‡å—.md)
- [é«˜çº§ä¸»é¢˜](../07-é«˜çº§ä¸»é¢˜/)

---

**æœ€åæ›´æ–°**: 2025-11-05  
**ç‰ˆæœ¬**: v1.0

