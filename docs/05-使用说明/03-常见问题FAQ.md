# å¸¸è§é—®é¢˜FAQ

æœ¬æ–‡æ¡£æ±‡æ€»é¡¹ç›®ä½¿ç”¨è¿‡ç¨‹ä¸­çš„å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ä½ å¿«é€Ÿè§£å†³é‡åˆ°çš„é—®é¢˜ã€‚

---

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå®‰è£…é—®é¢˜](#ç¯å¢ƒå®‰è£…é—®é¢˜)
2. [æ¨¡å‹ä¸‹è½½é—®é¢˜](#æ¨¡å‹ä¸‹è½½é—®é¢˜)
3. [è®­ç»ƒç›¸å…³é—®é¢˜](#è®­ç»ƒç›¸å…³é—®é¢˜)
4. [æ¨ç†éƒ¨ç½²é—®é¢˜](#æ¨ç†éƒ¨ç½²é—®é¢˜)
5. [ç¡¬ä»¶ç›¸å…³é—®é¢˜](#ç¡¬ä»¶ç›¸å…³é—®é¢˜)
6. [æ•°æ®å¤„ç†é—®é¢˜](#æ•°æ®å¤„ç†é—®é¢˜)
7. [æ€§èƒ½ä¼˜åŒ–é—®é¢˜](#æ€§èƒ½ä¼˜åŒ–é—®é¢˜)

---

## ç¯å¢ƒå®‰è£…é—®é¢˜

### Q1: pip install æ—¶æŠ¥é”™ "No module named 'torch'"

**é—®é¢˜æè¿°**: å®‰è£…å…¶ä»–ä¾èµ–æ—¶æç¤ºæ‰¾ä¸åˆ°PyTorch

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å…ˆå®‰è£…PyTorchï¼ˆæ ¹æ®CUDAç‰ˆæœ¬ï¼‰
# CUDA 11.8
pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu121

# CPUç‰ˆæœ¬
pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cpu

# ç„¶åå®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

### Q2: CUDAç‰ˆæœ¬ä¸åŒ¹é…

**é—®é¢˜æè¿°**: 
```
RuntimeError: CUDA error: no kernel image is available for execution on the device
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version

# 2. æ£€æŸ¥PyTorch CUDAç‰ˆæœ¬
python -c "import torch; print(torch.version.cuda)"

# 3. é‡æ–°å®‰è£…åŒ¹é…çš„PyTorchç‰ˆæœ¬
# å¦‚æœç³»ç»ŸCUDAæ˜¯11.8
pip uninstall torch torchvision
pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118
```

### Q3: ImportError: libcudnn.so.8: cannot open shared object file

**é—®é¢˜æè¿°**: æ‰¾ä¸åˆ°cuDNNåº“

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³•1: å®‰è£…cuDNN
# Ubuntu/Debian
sudo apt-get install libcudnn8 libcudnn8-dev

# æ–¹æ³•2: æ·»åŠ åº“è·¯å¾„
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# æ–¹æ³•3: ä½¿ç”¨condaå®‰è£…ï¼ˆæ¨èï¼‰
conda install cudnn
```

### Q4: è™šæ‹Ÿç¯å¢ƒä¸­pip installå¾ˆæ…¢

**é—®é¢˜æè¿°**: ä½¿ç”¨pipå®‰è£…ä¾èµ–é€Ÿåº¦éå¸¸æ…¢

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨å›½å†…é•œåƒæº
# ä¸´æ—¶ä½¿ç”¨
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# æ°¸ä¹…é…ç½®
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# æˆ–ç¼–è¾‘ ~/.pip/pip.conf
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host = pypi.tuna.tsinghua.edu.cn
```

### Q5: AttributeError: module 'clip' has no attribute 'load'

**é—®é¢˜æè¿°**: CLIPåº“å®‰è£…ä¸æ­£ç¡®

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¯èƒ½æ˜¯å®‰è£…äº†é”™è¯¯çš„clipåŒ…
pip uninstall clip

# é‡æ–°å®‰è£…æ­£ç¡®çš„CLIP
pip install git+https://github.com/openai/CLIP.git

# æˆ–è€…
pip install ftfy regex tqdm
pip install git+https://github.com/openai/CLIP.git
```

---

## æ¨¡å‹ä¸‹è½½é—®é¢˜

### Q6: HuggingFaceæ¨¡å‹ä¸‹è½½å¤±è´¥

**é—®é¢˜æè¿°**: 
```
OSError: Can't load weights for 'openai/clip-vit-base-patch32'
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³•1: ä½¿ç”¨HuggingFaceé•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½å¹¶æŒ‡å®šè·¯å¾„
# ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
git clone https://hf-mirror.com/openai/clip-vit-base-patch32 models/clip-vit-base-patch32

# ä½¿ç”¨æœ¬åœ°è·¯å¾„
python code/xxx.py --model_path models/clip-vit-base-patch32

# æ–¹æ³•3: ä½¿ç”¨ç¦»çº¿æ¨¡å¼
export TRANSFORMERS_OFFLINE=1
# ç¡®ä¿æ¨¡å‹å·²ç¼“å­˜åœ¨ ~/.cache/huggingface/
```

### Q7: æ¨¡å‹ä¸‹è½½ä¸­æ–­ï¼Œå¦‚ä½•ç»§ç»­ä¸‹è½½ï¼Ÿ

**é—®é¢˜æè¿°**: ä¸‹è½½å¤§æ¨¡å‹æ—¶ç½‘ç»œä¸­æ–­

**è§£å†³æ–¹æ¡ˆ**:
```python
# HuggingFaceä¼šè‡ªåŠ¨æ–­ç‚¹ç»­ä¼ 
from transformers import CLIPModel

# ä¼šè‡ªåŠ¨ä»ç¼“å­˜ç»§ç»­ä¸‹è½½
model = CLIPModel.from_pretrained(
    "openai/clip-vit-base-patch32",
    resume_download=True  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
)
```

### Q8: ç£ç›˜ç©ºé—´ä¸è¶³

**é—®é¢˜æè¿°**: `/root/.cache/huggingface/` å ç”¨è¿‡å¤šç©ºé—´

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥çœ‹ç¼“å­˜å¤§å°
du -sh ~/.cache/huggingface/

# æ¸…ç†æ—§æ¨¡å‹
huggingface-cli delete-cache

# æˆ–æ‰‹åŠ¨åˆ é™¤ä¸éœ€è¦çš„æ¨¡å‹
rm -rf ~/.cache/huggingface/hub/models--xxx

# ä¿®æ”¹ç¼“å­˜ç›®å½•
export HF_HOME=/path/to/large/disk/huggingface
export TRANSFORMERS_CACHE=/path/to/large/disk/huggingface

# æ°¸ä¹…è®¾ç½®ï¼ˆæ·»åŠ åˆ°~/.bashrcï¼‰
echo 'export HF_HOME=/path/to/large/disk/huggingface' >> ~/.bashrc
```

---

## è®­ç»ƒç›¸å…³é—®é¢˜

### Q9: CUDA Out of Memory (OOM)

**é—®é¢˜æè¿°**: 
```
RuntimeError: CUDA out of memory. Tried to allocate XX.XX MiB
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å‡å°batch size
batch_size = 8  # ä»32å‡åˆ°8

# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
for i, (images, labels) in enumerate(dataloader):
    outputs = model(images)
    loss = criterion(outputs, labels) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    outputs = model(images)
    loss = criterion(outputs, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()

# 4. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²é€Ÿåº¦æ¢å†…å­˜ï¼‰
from torch.utils.checkpoint import checkpoint

output = checkpoint(model.layer, input)

# 5. ä½¿ç”¨LoRAè€Œä¸æ˜¯å…¨å‚æ•°å¾®è°ƒ
from peft import get_peft_model, LoraConfig

lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1)
model = get_peft_model(model, lora_config)
```

### Q10: è®­ç»ƒlossä¸ä¸‹é™

**é—®é¢˜æè¿°**: è®­ç»ƒå‡ ä¸ªepochålossæ²¡æœ‰æ˜æ˜¾ä¸‹é™

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥å­¦ä¹ ç‡
# è§†è§‰å¤§æ¨¡å‹å¾®è°ƒå»ºè®®ä½¿ç”¨å°å­¦ä¹ ç‡
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # è€Œä¸æ˜¯1e-3

# 2. ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)

# 3. æ£€æŸ¥æ•°æ®æ ‡ç­¾æ˜¯å¦æ­£ç¡®
for images, labels in dataloader:
    print(f"Labels: {labels}")
    print(f"Label range: {labels.min()} to {labels.max()}")
    break

# 4. ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡warm-up
from transformers import get_linear_schedule_with_warmup

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=total_steps
)

# 5. æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å†»æ›´å¤šå±‚
# åªå¾®è°ƒæœ€åå‡ å±‚å¯èƒ½ä¸å¤Ÿ
for name, param in model.named_parameters():
    if 'layer.10' in name or 'layer.11' in name or 'classifier' in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```

### Q11: losså˜æˆNaN

**é—®é¢˜æè¿°**: è®­ç»ƒè¿‡ç¨‹ä¸­lossçªç„¶å˜æˆNaN

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. é™ä½å­¦ä¹ ç‡
optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)  # æ›´å°çš„å­¦ä¹ ç‡

# 2. ä½¿ç”¨æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 3. æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰å¼‚å¸¸å€¼
# æ·»åŠ æ•°æ®éªŒè¯
def validate_batch(images, labels):
    assert not torch.isnan(images).any(), "Images contain NaN"
    assert not torch.isinf(images).any(), "Images contain Inf"
    assert (labels >= 0).all() and (labels < num_classes).all(), "Invalid labels"

# 4. ä½¿ç”¨æ··åˆç²¾åº¦æ—¶æ·»åŠ æ¢¯åº¦ç¼©æ”¾
from torch.cuda.amp import GradScaler

scaler = GradScaler()
# ... (å¦‚Q9æ‰€ç¤º)

# 5. æ£€æŸ¥æŸå¤±å‡½æ•°
# ç¡®ä¿ä½¿ç”¨åˆé€‚çš„æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss(reduction='mean')  # ä½¿ç”¨meanè€Œä¸æ˜¯sum
```

### Q12: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**é—®é¢˜æè¿°**: è®­ç»ƒä¸€ä¸ªepochè¦å¾ˆé•¿æ—¶é—´

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨å¤šGPUè®­ç»ƒ
import torch.nn as nn
model = nn.DataParallel(model, device_ids=[0, 1, 2, 3])

# 2. å¢åŠ num_workers
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,  # å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
    pin_memory=True  # åŠ é€Ÿæ•°æ®ä¼ è¾“åˆ°GPU
)

# 3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåŠ é€Ÿ2xï¼‰
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    outputs = model(images)
    loss = criterion(outputs, labels)

# 4. ä¼˜åŒ–æ•°æ®é¢„å¤„ç†
# é¢„å…ˆå¤„ç†æ•°æ®å¹¶ç¼“å­˜
# æˆ–ä½¿ç”¨æ›´å¿«çš„å›¾åƒè§£ç åº“ï¼ˆå¦‚turbojpegï¼‰

# 5. ä½¿ç”¨ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
model = torch.compile(model)

# 6. å‡å°‘æ—¥å¿—è¾“å‡ºé¢‘ç‡
if step % 100 == 0:  # æ¯100æ­¥æ‰“å°ä¸€æ¬¡ï¼Œè€Œä¸æ˜¯æ¯æ­¥
    print(f"Step {step}, Loss: {loss.item()}")
```

---

## æ¨ç†éƒ¨ç½²é—®é¢˜

### Q13: æ¨ç†é€Ÿåº¦æ…¢

**é—®é¢˜æè¿°**: å•å¼ å›¾ç‰‡æ¨ç†éœ€è¦å¾ˆé•¿æ—¶é—´

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨evalæ¨¡å¼
model.eval()

# 2. ç¦ç”¨æ¢¯åº¦è®¡ç®—
with torch.no_grad():
    outputs = model(images)

# 3. ä½¿ç”¨FP16æ¨ç†
model = model.half()
images = images.half()

# 4. ä½¿ç”¨æ‰¹å¤„ç†
# å°†å¤šä¸ªè¯·æ±‚åˆå¹¶æˆbatchå¤„ç†
batch_images = torch.stack([img1, img2, img3, ...])
with torch.no_grad():
    batch_outputs = model(batch_images)

# 5. è½¬æ¢ä¸ºONNX
import torch.onnx
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    opset_version=11
)

# ä½¿ç”¨ONNX Runtimeæ¨ç†
import onnxruntime as ort
session = ort.InferenceSession("model.onnx")
outputs = session.run(None, {"input": input_data})

# 6. ä½¿ç”¨TensorRTï¼ˆNVIDIA GPUï¼‰
import torch_tensorrt
trt_model = torch_tensorrt.compile(model, ...)
```

### Q14: APIæœåŠ¡å†…å­˜æ³„æ¼

**é—®é¢˜æè¿°**: FastAPIæœåŠ¡è¿è¡Œä¸€æ®µæ—¶é—´åå†…å­˜å ç”¨è¶Šæ¥è¶Šé«˜

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ç¡®ä¿ä½¿ç”¨torch.no_grad()
@app.post("/predict")
async def predict(file: UploadFile):
    image = load_image(file)
    
    with torch.no_grad():  # é‡è¦ï¼é˜²æ­¢å†…å­˜ç´¯ç§¯
        output = model(image)
    
    return {"result": output.tolist()}

# 2. åŠæ—¶é‡Šæ”¾å¤§å¯¹è±¡
@app.post("/predict")
async def predict(file: UploadFile):
    image = load_image(file)
    
    with torch.no_grad():
        output = model(image)
    
    result = output.cpu().tolist()
    
    # æ˜¾å¼åˆ é™¤
    del image, output
    torch.cuda.empty_cache()
    
    return {"result": result}

# 3. ä½¿ç”¨è¿›ç¨‹æ± è€Œä¸æ˜¯çº¿ç¨‹æ± 
from concurrent.futures import ProcessPoolExecutor

executor = ProcessPoolExecutor(max_workers=4)

# 4. å®šæœŸé‡å¯worker
# ä½¿ç”¨gunicornçš„--max-requestså‚æ•°
# gunicorn app:app --max-requests 1000 --max-requests-jitter 100

# 5. ç›‘æ§å†…å­˜ä½¿ç”¨
import psutil
import gc

@app.get("/health")
def health():
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    
    if memory_mb > 8000:  # è¶…è¿‡8GB
        gc.collect()
        torch.cuda.empty_cache()
    
    return {"memory_mb": memory_mb}
```

### Q15: Dockerå®¹å™¨ä¸­GPUä¸å¯ç”¨

**é—®é¢˜æè¿°**: 
```
RuntimeError: Found no NVIDIA driver on your system
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. å®‰è£…nvidia-container-toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# 2. ä½¿ç”¨--gpuså‚æ•°è¿è¡Œå®¹å™¨
docker run --gpus all -it your-image

# 3. ä½¿ç”¨docker-composeæ—¶æŒ‡å®šruntime
docker-compose.yml:
services:
  app:
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

# 4. æµ‹è¯•GPUæ˜¯å¦å¯ç”¨
docker run --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

---

## ç¡¬ä»¶ç›¸å…³é—®é¢˜

### Q16: å¤šGPUè®­ç»ƒæ—¶æ˜¾å­˜ä¸å‡è¡¡

**é—®é¢˜æè¿°**: GPU 0æ˜¾å­˜å ç”¨å¾ˆé«˜ï¼Œå…¶ä»–GPUå¾ˆä½

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨DistributedDataParallelè€Œä¸æ˜¯DataParallel
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
dist.init_process_group(backend='nccl')
model = DDP(model, device_ids=[local_rank])

# 2. ä½¿ç”¨balanced batch sampler
from torch.utils.data.distributed import DistributedSampler

sampler = DistributedSampler(dataset)
dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)

# 3. å¯åŠ¨è„šæœ¬
# torchrun --nproc_per_node=4 train.py

# 4. æˆ–ä½¿ç”¨torch.multiprocessing
import torch.multiprocessing as mp

def train_worker(rank, world_size):
    setup(rank, world_size)
    # ... training code

mp.spawn(train_worker, args=(world_size,), nprocs=world_size)
```

### Q17: åä¸ºæ˜‡è…¾NPUæ— æ³•è¯†åˆ«

**é—®é¢˜æè¿°**: `npu-smi info` æ˜¾ç¤ºno device

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥é©±åŠ¨å®‰è£…
ls /usr/local/Ascend/driver/

# 2. æ£€æŸ¥è®¾å¤‡
ls -l /dev/davinci*

# 3. è®¾ç½®ç¯å¢ƒå˜é‡
export LD_LIBRARY_PATH=/usr/local/Ascend/driver/lib64:$LD_LIBRARY_PATH
export ASCEND_HOME=/usr/local/Ascend/latest
export PATH=$ASCEND_HOME/bin:$PATH

# 4. é‡å¯é©±åŠ¨æœåŠ¡
sudo systemctl restart ascend-device-driver

# 5. æ£€æŸ¥ç”¨æˆ·æƒé™
sudo usermod -a -G HwHiAiUser $(whoami)
# é‡æ–°ç™»å½•ä½¿æƒé™ç”Ÿæ•ˆ

# 6. æ£€æŸ¥CANNç‰ˆæœ¬
cat /usr/local/Ascend/latest/version.cfg
```

### Q18: CPUæ¨ç†å¤ªæ…¢

**é—®é¢˜æè¿°**: åœ¨æ²¡æœ‰GPUçš„ç¯å¢ƒä¸‹æ¨ç†é€Ÿåº¦ä¸å¯æ¥å—

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨INT8é‡åŒ–
import torch
from torch.quantization import quantize_dynamic

model_int8 = quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# 2. ä½¿ç”¨ONNX Runtime
import onnxruntime as ort

session = ort.InferenceSession(
    "model.onnx",
    providers=['CPUExecutionProvider']
)

# 3. ä½¿ç”¨OpenVINOï¼ˆIntel CPUä¼˜åŒ–ï¼‰
# éœ€è¦å®‰è£…openvino
from openvino.runtime import Core

ie = Core()
model = ie.read_model("model.xml")
compiled_model = ie.compile_model(model, "CPU")

# 4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
# ViT-B/32 -> ViT-B/16 -> è’¸é¦å°æ¨¡å‹

# 5. æ‰¹å¤„ç†
# ç´¯ç§¯å¤šä¸ªè¯·æ±‚ä¸€èµ·å¤„ç†
```

---

## æ•°æ®å¤„ç†é—®é¢˜

### Q19: è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½å¤±è´¥

**é—®é¢˜æè¿°**: ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†æ—¶æŠ¥é”™

**è§£å†³æ–¹æ¡ˆ**:
```python
from torch.utils.data import Dataset
from PIL import Image
import os

class CustomDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.samples = []
        
        # åŠ è½½æ‰€æœ‰å›¾ç‰‡
        for class_name in os.listdir(data_dir):
            class_path = os.path.join(data_dir, class_name)
            if not os.path.isdir(class_path):
                continue
            
            for img_name in os.listdir(class_path):
                if not img_name.lower().endswith(('.jpg', '.png', '.jpeg')):
                    continue
                
                img_path = os.path.join(class_path, img_name)
                self.samples.append((img_path, class_name))
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, class_name = self.samples[idx]
        
        try:
            # åŠ è½½å›¾ç‰‡
            image = Image.open(img_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            return image, class_name
        
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            # è¿”å›ä¸€ä¸ªå ä½å›¾ç‰‡
            return torch.zeros(3, 224, 224), class_name

# ä½¿ç”¨
dataset = CustomDataset("data/images", transform=preprocess)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

### Q20: å›¾ç‰‡å°ºå¯¸ä¸ä¸€è‡´å¯¼è‡´batché”™è¯¯

**é—®é¢˜æè¿°**: 
```
RuntimeError: stack expects each tensor to be equal size
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. åœ¨transformä¸­ç»Ÿä¸€å°ºå¯¸
from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize(256),  # å…ˆç¼©æ”¾
    transforms.CenterCrop(224),  # å†è£å‰ªåˆ°å›ºå®šå¤§å°
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# 2. æˆ–ä½¿ç”¨è‡ªå®šä¹‰collate_fn
def custom_collate(batch):
    images, labels = zip(*batch)
    
    # ç»Ÿä¸€å°ºå¯¸
    images = [transforms.Resize((224, 224))(img) for img in images]
    images = torch.stack(images)
    
    return images, labels

dataloader = DataLoader(
    dataset,
    batch_size=32,
    collate_fn=custom_collate
)
```

---

## æ€§èƒ½ä¼˜åŒ–é—®é¢˜

### Q21: å¦‚ä½•æé«˜è®­ç»ƒé€Ÿåº¦ï¼Ÿ

**ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# 1. æ•°æ®åŠ è½½ä¼˜åŒ–
dataloader = DataLoader(
    dataset,
    batch_size=64,  # å°½å¯èƒ½å¤§ï¼ˆä¸OOMçš„æƒ…å†µä¸‹ï¼‰
    num_workers=8,  # CPUæ ¸å¿ƒæ•°
    pin_memory=True,  # åŠ é€Ÿæ•°æ®ä¼ è¾“åˆ°GPU
    persistent_workers=True,  # ä¿æŒworkerè¿›ç¨‹å­˜æ´»
    prefetch_factor=2  # é¢„å–æ•°æ®
)

# 2. æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for images, labels in dataloader:
    with autocast():
        outputs = model(images)
        loss = criterion(outputs, labels)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

# 3. æ¨¡å‹ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰
model = torch.compile(model, mode="reduce-overhead")

# 4. æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§batch sizeï¼‰
accumulation_steps = 4

for i, (images, labels) in enumerate(dataloader):
    outputs = model(images)
    loss = criterion(outputs, labels) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 5. ä½¿ç”¨LoRAè€Œä¸æ˜¯å…¨å‚æ•°å¾®è°ƒ
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(r=8, lora_alpha=16)
model = get_peft_model(model, lora_config)

# 6. å†»ç»“backboneï¼Œåªå¾®è°ƒhead
for param in model.visual.parameters():
    param.requires_grad = False
for param in model.classifier.parameters():
    param.requires_grad = True
```

### Q22: å¦‚ä½•å‡å°æ¨¡å‹ä½“ç§¯ï¼Ÿ

**æ¨¡å‹å‹ç¼©æ–¹æ¡ˆ**:
```python
# 1. æ¨¡å‹é‡åŒ–
import torch.quantization as quantization

# åŠ¨æ€é‡åŒ–ï¼ˆæœ€ç®€å•ï¼‰
model_int8 = quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# ä¿å­˜
torch.save(model_int8.state_dict(), "model_int8.pth")

# 2. æ¨¡å‹å‰ªæ
import torch.nn.utils.prune as prune

# å‰ªæ30%çš„æƒé‡
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.3)
        prune.remove(module, 'weight')

# 3. çŸ¥è¯†è’¸é¦
# ä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹
teacher_model = load_large_model()
student_model = load_small_model()

# ... è’¸é¦è®­ç»ƒä»£ç ï¼ˆå‚è€ƒé«˜çº§ä¸»é¢˜æ–‡æ¡£ï¼‰

# 4. åªä¿å­˜å¿…è¦çš„æƒé‡
# å»é™¤ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦ç­‰
torch.save(model.state_dict(), "model_weights_only.pth")

# 5. ä½¿ç”¨æ›´å°çš„æ¨¡å‹å˜ä½“
# ViT-L/14 -> ViT-B/32 -> ViT-B/16
```

### Q23: å¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Ÿ

**å®Œæ•´è¯„ä¼°æ–¹æ¡ˆ**:
```python
import time
import psutil
from torch.profiler import profile, ProfilerActivity

def comprehensive_evaluation(model, test_loader, device="cuda"):
    """å…¨é¢è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    # 1. å‡†ç¡®ç‡è¯„ä¼°
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    accuracy = 100. * correct / total
    print(f"Accuracy: {accuracy:.2f}%")
    
    # 2. é€Ÿåº¦è¯„ä¼°
    model.eval()
    torch.cuda.synchronize()
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(10):
            _ = model(torch.randn(1, 3, 224, 224).to(device))
    
    # æµ‹é€Ÿ
    times = []
    with torch.no_grad():
        for _ in range(100):
            start = time.time()
            _ = model(torch.randn(1, 3, 224, 224).to(device))
            torch.cuda.synchronize()
            times.append(time.time() - start)
    
    avg_time = sum(times) / len(times)
    print(f"Average inference time: {avg_time*1000:.2f} ms")
    print(f"Throughput: {1/avg_time:.2f} images/sec")
    
    # 3. å†…å­˜è¯„ä¼°
    torch.cuda.reset_peak_memory_stats()
    with torch.no_grad():
        _ = model(torch.randn(32, 3, 224, 224).to(device))
    
    peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
    print(f"Peak memory: {peak_memory:.2f} MB")
    
    # 4. æ¨¡å‹å¤§å°
    torch.save(model.state_dict(), "temp_model.pth")
    model_size = os.path.getsize("temp_model.pth") / 1024 / 1024
    os.remove("temp_model.pth")
    print(f"Model size: {model_size:.2f} MB")
    
    # 5. è¯¦ç»†æ€§èƒ½åˆ†æï¼ˆå¯é€‰ï¼‰
    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
        with torch.no_grad():
            _ = model(torch.randn(1, 3, 224, 224).to(device))
    
    print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
    
    return {
        "accuracy": accuracy,
        "avg_inference_time_ms": avg_time * 1000,
        "throughput": 1 / avg_time,
        "peak_memory_mb": peak_memory,
        "model_size_mb": model_size
    }

# ä½¿ç”¨
results = comprehensive_evaluation(model, test_loader)
```

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœä»¥ä¸ŠFAQæ²¡æœ‰è§£å†³ä½ çš„é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–å¸®åŠ©ï¼š

### 1. æŸ¥çœ‹æ–‡æ¡£
- [ç¯å¢ƒå®‰è£…æŒ‡å—](./01-ç¯å¢ƒå®‰è£…æŒ‡å—.md)
- [å¿«é€Ÿå¼€å§‹](./02-å¿«é€Ÿå¼€å§‹.md)
- [æœ€ä½³å®è·µ](./04-æœ€ä½³å®è·µ.md)
- [æ•…éšœæ’æŸ¥æŒ‡å—](./05-æ•…éšœæ’æŸ¥æŒ‡å—.md)

### 2. æäº¤Issue
- GitHub Issues: https://github.com/YourRepo/Large-Model-Tutorial/issues
- æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå¤ç°æ­¥éª¤

### 3. ç¤¾åŒºè®¨è®º
- GitHub Discussions: ä¸å…¶ä»–ç”¨æˆ·äº¤æµç»éªŒ
- Stack Overflow: ä½¿ç”¨ `clip` `vision-transformer` æ ‡ç­¾

### 4. æŸ¥çœ‹æ—¥å¿—
```bash
# æŸ¥çœ‹é¡¹ç›®æ—¥å¿—
cat logs/training.log

# æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—
dmesg | grep -i cuda
dmesg | grep -i nvidia
```

---

## ğŸ”„ æŒç»­æ›´æ–°

æœ¬FAQä¼šæŒç»­æ›´æ–°ï¼Œæ·»åŠ æ›´å¤šå¸¸è§é—®é¢˜ã€‚å¦‚æœä½ é‡åˆ°äº†æ–°çš„é—®é¢˜å¹¶æ‰¾åˆ°äº†è§£å†³æ–¹æ¡ˆï¼Œæ¬¢è¿è´¡çŒ®åˆ°é¡¹ç›®ä¸­ï¼

**æœ€åæ›´æ–°**: 2025-11-05  
**ç‰ˆæœ¬**: v1.0

