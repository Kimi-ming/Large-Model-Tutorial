# å¿«é€Ÿå¼€å§‹

æ¬¢è¿å¼€å§‹æ‚¨çš„è§†è§‰å¤§æ¨¡å‹å­¦ä¹ ä¹‹æ—…ï¼æœ¬æ–‡æ¡£å°†å¸¦æ‚¨å¿«é€Ÿä¸Šæ‰‹ï¼Œåœ¨10åˆ†é’Ÿå†…è¿è¡Œæ‚¨çš„ç¬¬ä¸€ä¸ªè§†è§‰å¤§æ¨¡å‹æ¨ç†ç¤ºä¾‹ã€‚

## ğŸ“‹ å‰ç½®æ¡ä»¶

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ï¼š

- âœ… å·²å®Œæˆ[ç¯å¢ƒå®‰è£…](./01-ç¯å¢ƒå®‰è£…æŒ‡å—.md)
- âœ… Python 3.8+ å·²å®‰è£…
- âœ… å·²å®‰è£…PyTorchå’ŒTransformersåº“
- âœ… ï¼ˆæ¨èï¼‰å·²ä¸‹è½½CLIPæ¨¡å‹ï¼ˆæˆ–å°†è‡ªåŠ¨ä¸‹è½½ï¼‰

---

## ğŸš€ ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼šCLIPå›¾æ–‡åŒ¹é…

CLIP (Contrastive Language-Image Pre-training) æ˜¯OpenAIå¼€å‘çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¯ä»¥ç†è§£å›¾åƒå’Œæ–‡æœ¬çš„å…³ç³»ã€‚

### æ­¥éª¤1ï¼šä¸‹è½½ç¤ºä¾‹å›¾ç‰‡

åˆ›å»ºæµ‹è¯•å›¾ç‰‡æˆ–ä½¿ç”¨ä»»æ„å›¾ç‰‡ï¼š

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºç¤ºä¾‹ç›®å½•
mkdir -p examples/images

# ä¸‹è½½ç¤ºä¾‹å›¾ç‰‡ï¼ˆæˆ–ä½¿ç”¨æ‚¨è‡ªå·±çš„å›¾ç‰‡ï¼‰
# è¿™é‡Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬
```

### æ­¥éª¤2ï¼šåˆ›å»ºæ¨ç†è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `quick_start_clip.py`ï¼š

```python
#!/usr/bin/env python3
"""
CLIPå¿«é€Ÿå¼€å§‹ç¤ºä¾‹

åŠŸèƒ½ï¼šä½¿ç”¨CLIPæ¨¡å‹è¿›è¡Œå›¾æ–‡åŒ¹é…
"""

import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

# é…ç½®
MODEL_NAME = "openai/clip-vit-base-patch32"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("=" * 60)
print("CLIP å›¾æ–‡åŒ¹é…ç¤ºä¾‹")
print("=" * 60)
print(f"\nä½¿ç”¨è®¾å¤‡: {DEVICE}")

# 1. åŠ è½½æ¨¡å‹
print(f"\n[1/4] åŠ è½½æ¨¡å‹: {MODEL_NAME}")
print("æç¤º: é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦600MBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...")

model = CLIPModel.from_pretrained(MODEL_NAME)
processor = CLIPProcessor.from_pretrained(MODEL_NAME)
model = model.to(DEVICE)
model.eval()

print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# 2. å‡†å¤‡å›¾åƒ
print("\n[2/4] å‡†å¤‡æµ‹è¯•å›¾åƒ")

# æ–¹å¼1ï¼šä»URLåŠ è½½å›¾ç‰‡
from PIL import Image
import requests
from io import BytesIO

image_url = "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"
try:
    response = requests.get(image_url, timeout=10)
    image = Image.open(BytesIO(response.content))
    print(f"âœ“ ä»ç½‘ç»œåŠ è½½å›¾ç‰‡: {image.size}")
except:
    # æ–¹å¼2ï¼šåˆ›å»ºçº¯è‰²å›¾ç‰‡ä½œä¸ºç¤ºä¾‹
    print("âš  æ— æ³•ä¸‹è½½ç¤ºä¾‹å›¾ç‰‡ï¼Œä½¿ç”¨æœ¬åœ°åˆ›å»ºçš„æµ‹è¯•å›¾ç‰‡")
    image = Image.new('RGB', (224, 224), color='red')
    print("âœ“ åˆ›å»ºæµ‹è¯•å›¾ç‰‡: (224, 224)")

# 3. å‡†å¤‡æ–‡æœ¬å€™é€‰
print("\n[3/4] å‡†å¤‡æ–‡æœ¬å€™é€‰")
text_candidates = [
    "ä¸€åªçŒ«",
    "ä¸€åªç‹—",
    "ä¸€ç¾¤é¹¦é¹‰",
    "ä¸€è¾†æ±½è½¦",
    "ä¸€åº§å»ºç­‘",
]

print(f"å€™é€‰æ–‡æœ¬æ•°é‡: {len(text_candidates)}")
for i, text in enumerate(text_candidates, 1):
    print(f"  {i}. {text}")

# 4. è¿›è¡Œæ¨ç†
print("\n[4/4] æ‰§è¡Œæ¨ç†...")

# å¤„ç†è¾“å…¥
inputs = processor(
    text=text_candidates,
    images=image,
    return_tensors="pt",
    padding=True
)

# ç§»åŠ¨åˆ°è®¾å¤‡
inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

# æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image
    probs = logits_per_image.softmax(dim=1)

print("âœ“ æ¨ç†å®Œæˆï¼")

# 5. æ˜¾ç¤ºç»“æœ
print("\n" + "=" * 60)
print("ğŸ“Š åŒ¹é…ç»“æœ")
print("=" * 60)

# æ’åºç»“æœ
sorted_indices = probs[0].argsort(descending=True)

for rank, idx in enumerate(sorted_indices, 1):
    text = text_candidates[idx]
    confidence = probs[0][idx].item() * 100
    bar_length = int(confidence / 2)  # 50ä¸ªå­—ç¬¦æ»¡æ ¼
    bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
    
    print(f"\n{rank}. {text}")
    print(f"   {bar} {confidence:.2f}%")

# æœ€ä½³åŒ¹é…
best_idx = sorted_indices[0]
best_text = text_candidates[best_idx]
best_confidence = probs[0][best_idx].item() * 100

print("\n" + "=" * 60)
print(f"ğŸ¯ æœ€ä½³åŒ¹é…: {best_text} ({best_confidence:.2f}%)")
print("=" * 60)

print("\nâœ¨ æ­å–œï¼æ‚¨å·²æˆåŠŸè¿è¡Œç¬¬ä¸€ä¸ªè§†è§‰å¤§æ¨¡å‹ç¤ºä¾‹ï¼")
print("\nğŸ“š ä¸‹ä¸€æ­¥:")
print("  - å°è¯•ç”¨è‡ªå·±çš„å›¾ç‰‡è¿›è¡Œæµ‹è¯•")
print("  - ä¿®æ”¹å€™é€‰æ–‡æœ¬åˆ—è¡¨")
print("  - æŸ¥çœ‹æ›´å¤šç¤ºä¾‹: code/01-model-evaluation/examples/")
```

### æ­¥éª¤3ï¼šè¿è¡Œç¤ºä¾‹

```bash
python quick_start_clip.py
```

é¢„æœŸè¾“å‡ºï¼š

```
============================================================
CLIP å›¾æ–‡åŒ¹é…ç¤ºä¾‹
============================================================

ä½¿ç”¨è®¾å¤‡: cuda

[1/4] åŠ è½½æ¨¡å‹: openai/clip-vit-base-patch32
æç¤º: é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦600MBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...
âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼

[2/4] å‡†å¤‡æµ‹è¯•å›¾åƒ
âœ“ ä»ç½‘ç»œåŠ è½½å›¾ç‰‡: (768, 512)

[3/4] å‡†å¤‡æ–‡æœ¬å€™é€‰
å€™é€‰æ–‡æœ¬æ•°é‡: 5
  1. ä¸€åªçŒ«
  2. ä¸€åªç‹—
  3. ä¸€ç¾¤é¹¦é¹‰
  4. ä¸€è¾†æ±½è½¦
  5. ä¸€åº§å»ºç­‘

[4/4] æ‰§è¡Œæ¨ç†...
âœ“ æ¨ç†å®Œæˆï¼

============================================================
ğŸ“Š åŒ¹é…ç»“æœ
============================================================

1. ä¸€ç¾¤é¹¦é¹‰
   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 98.75%

2. ä¸€åªçŒ«
   â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.80%

3. ä¸€åªç‹—
   â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.30%

4. ä¸€è¾†æ±½è½¦
   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.10%

5. ä¸€åº§å»ºç­‘
   â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.05%

============================================================
ğŸ¯ æœ€ä½³åŒ¹é…: ä¸€ç¾¤é¹¦é¹‰ (98.75%)
============================================================

âœ¨ æ­å–œï¼æ‚¨å·²æˆåŠŸè¿è¡Œç¬¬ä¸€ä¸ªè§†è§‰å¤§æ¨¡å‹ç¤ºä¾‹ï¼

ğŸ“š ä¸‹ä¸€æ­¥:
  - å°è¯•ç”¨è‡ªå·±çš„å›¾ç‰‡è¿›è¡Œæµ‹è¯•
  - ä¿®æ”¹å€™é€‰æ–‡æœ¬åˆ—è¡¨
  - æŸ¥çœ‹æ›´å¤šç¤ºä¾‹: code/01-model-evaluation/examples/
```

---

## ğŸ¨ ä½¿ç”¨æ‚¨è‡ªå·±çš„å›¾ç‰‡

### æ–¹æ³•1ï¼šæœ¬åœ°å›¾ç‰‡

```python
from PIL import Image

# åŠ è½½æœ¬åœ°å›¾ç‰‡
image = Image.open("path/to/your/image.jpg")

# å…¶ä½™ä»£ç ä¿æŒä¸å˜
```

### æ–¹æ³•2ï¼šä»URLåŠ è½½

```python
import requests
from PIL import Image
from io import BytesIO

# ä»URLåŠ è½½
url = "https://example.com/image.jpg"
response = requests.get(url)
image = Image.open(BytesIO(response.content))
```

---

## ğŸ“š æ›´å¤šç¤ºä¾‹

### ç¤ºä¾‹2ï¼šä½¿ç”¨å·¥å…·åº“ç®€åŒ–ä»£ç 

ä½¿ç”¨æˆ‘ä»¬æä¾›çš„å·¥å…·åº“ï¼Œä»£ç å¯ä»¥æ›´ç®€æ´ï¼š

```python
from code.utils import load_model_simple, ImageProcessor

# åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼‰
model, processor = load_model_simple('clip')

# å¤„ç†å›¾åƒ
img_processor = ImageProcessor(size=224)
image = img_processor.load_image("path/to/image.jpg")

# æ¨ç†
inputs = processor(text=["a cat", "a dog"], images=image, return_tensors="pt")
outputs = model(**inputs)
probs = outputs.logits_per_image.softmax(dim=1)

print(f"åŒ¹é…æ¦‚ç‡: {probs[0].tolist()}")
```

### ç¤ºä¾‹3ï¼šæ‰¹é‡å¤„ç†å¤šå¼ å›¾ç‰‡

```python
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

# åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# å‡†å¤‡å¤šå¼ å›¾ç‰‡
images = [
    Image.open("image1.jpg"),
    Image.open("image2.jpg"),
    Image.open("image3.jpg"),
]

# å‡†å¤‡æ–‡æœ¬
texts = ["a photo of a cat", "a photo of a dog"]

# æ‰¹é‡å¤„ç†
inputs = processor(text=texts, images=images, return_tensors="pt", padding=True)

# æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits_per_image
    probs = logits.softmax(dim=1)

# æ˜¾ç¤ºæ¯å¼ å›¾ç‰‡çš„åŒ¹é…ç»“æœ
for i, prob in enumerate(probs):
    print(f"\nå›¾ç‰‡ {i+1}:")
    for j, text in enumerate(texts):
        print(f"  {text}: {prob[j].item():.2%}")
```

---

## ğŸ› ï¸ å¸¸è§é—®é¢˜

### Q1: é¦–æ¬¡è¿è¡Œå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼ˆçº¦600MBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚åç»­è¿è¡Œä¼šä½¿ç”¨ç¼“å­˜ï¼Œé€Ÿåº¦ä¼šå¿«å¾ˆå¤šã€‚

å›½å†…ç”¨æˆ·å¯ä»¥ä½¿ç”¨é•œåƒæºåŠ é€Ÿï¼š

```bash
export HF_ENDPOINT=https://hf-mirror.com
python quick_start_clip.py
```

### Q2: é‡åˆ° "out of memory" é”™è¯¯ï¼Ÿ

**A**: æ˜¾å­˜ä¸è¶³ã€‚å¯ä»¥ï¼š

1. ä½¿ç”¨CPUæ¨¡å¼ï¼ˆè‡ªåŠ¨å›é€€ï¼‰
2. å‡å°æ‰¹æ¬¡å¤§å°
3. ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹

### Q3: å¦‚ä½•æé«˜æ¨ç†é€Ÿåº¦ï¼Ÿ

**A**:

1. ä½¿ç”¨GPUï¼ˆæ¯”CPUå¿«10-100å€ï¼‰
2. å¯ç”¨æ··åˆç²¾åº¦ï¼š
   ```python
   with torch.autocast('cuda'):
       outputs = model(**inputs)
   ```
3. æ‰¹é‡å¤„ç†å¤šå¼ å›¾ç‰‡

### Q4: æ”¯æŒå“ªäº›å›¾ç‰‡æ ¼å¼ï¼Ÿ

**A**: æ”¯æŒæ‰€æœ‰PILæ”¯æŒçš„æ ¼å¼ï¼ŒåŒ…æ‹¬ï¼š
- JPEG (.jpg, .jpeg)
- PNG (.png)
- BMP (.bmp)
- TIFF (.tif, .tiff)
- WebP (.webp)

---

## ğŸ¯ å­¦ä¹ è·¯å¾„

å®Œæˆå¿«é€Ÿå¼€å§‹åï¼Œå»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå­¦ä¹ ï¼š

### 1. åŸºç¡€é˜¶æ®µ
- âœ… **å¿«é€Ÿå¼€å§‹** â† æ‚¨åœ¨è¿™é‡Œ
- ğŸ“– [æ¨¡å‹ä¸‹è½½æŒ‡å—](./03-æ¨¡å‹ä¸‹è½½æŒ‡å—.md)
- ğŸ“– [å¸¸ç”¨åŠŸèƒ½ç¤ºä¾‹](./04-å¸¸ç”¨åŠŸèƒ½ç¤ºä¾‹.md)

### 2. è¿›é˜¶é˜¶æ®µ
- ğŸ“– æ¨¡å‹è°ƒç ”ä¸é€‰å‹
- ğŸ“– æ¨¡å‹å¾®è°ƒæŠ€æœ¯
- ğŸ“– æ•°æ®é›†å‡†å¤‡

### 3. å®æˆ˜é˜¶æ®µ
- ğŸ“– å¤šå¹³å°éƒ¨ç½²
- ğŸ“– å®é™…åº”ç”¨åœºæ™¯
- ğŸ“– é«˜çº§ä¸»é¢˜

---

## ğŸ’¡ æç¤ºä¸æŠ€å·§

### æŠ€å·§1ï¼šä¿å­˜å¤„ç†åçš„å›¾ç‰‡

```python
from PIL import Image
import torch

# å¤„ç†å¹¶ä¿å­˜
image = Image.open("input.jpg")
# ... å¤„ç† ...
image.save("output.jpg")
```

### æŠ€å·§2ï¼šå¯è§†åŒ–æ³¨æ„åŠ›å›¾

```python
import matplotlib.pyplot as plt

# è·å–æ³¨æ„åŠ›æƒé‡
attention = outputs.attentions[-1]  # æœ€åä¸€å±‚

# å¯è§†åŒ–
plt.imshow(attention[0, 0].cpu().numpy())
plt.colorbar()
plt.savefig("attention_map.png")
```

### æŠ€å·§3ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶

```python
from code.utils import load_config

# åŠ è½½é…ç½®
config = load_config("configs/models/clip.yaml")

# ä½¿ç”¨é…ç½®
model_name = config['model']['name']
device = config['model']['device']
```

---

## ğŸ“– ç›¸å…³èµ„æº

- **ä»£ç ç¤ºä¾‹**ï¼š`code/01-model-evaluation/examples/`
- **é…ç½®æ–‡ä»¶**ï¼š`configs/models/`
- **æ–‡æ¡£é¦–é¡µ**ï¼š[README.md](../../README.md)
- **å¸¸è§é—®é¢˜**ï¼š[FAQ.md](../../FAQ.md)

---

## ğŸ‰ æ­å–œï¼

æ‚¨å·²ç»æˆåŠŸï¼š
- âœ… è¿è¡Œäº†ç¬¬ä¸€ä¸ªè§†è§‰å¤§æ¨¡å‹
- âœ… ç†è§£äº†åŸºæœ¬çš„æ¨ç†æµç¨‹
- âœ… å­¦ä¼šäº†å¦‚ä½•ä¿®æ”¹ä»£ç 

ç»§ç»­æ¢ç´¢æ›´å¤šåŠŸèƒ½ï¼Œç¥å­¦ä¹ æ„‰å¿«ï¼ğŸš€

---

**ä¸Šæ¬¡æ›´æ–°**ï¼š2025-11-01  
**éš¾åº¦**ï¼šâ­â˜†â˜†â˜†â˜†ï¼ˆå…¥é—¨çº§ï¼‰  
**é¢„è®¡ç”¨æ—¶**ï¼š10-15åˆ†é’Ÿ

