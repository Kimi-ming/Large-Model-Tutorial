"""
SAMæ¨¡å‹å¾®è°ƒè®­ç»ƒè„šæœ¬

æ”¯æŒçš„å¾®è°ƒç­–ç•¥ï¼š
1. Full Fine-tuningï¼šå¾®è°ƒæ‰€æœ‰å‚æ•°
2. Adapter Tuningï¼šæ·»åŠ adapterå±‚
3. LoRAï¼šä½ç§©é€‚åº”

ä½œè€…ï¼šLarge-Model-Tutorial
è®¸å¯ï¼šMIT
"""

import argparse
import os
import random
import sys
import time
from pathlib import Path
from typing import Dict, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import yaml
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æ•°æ®é›†
from dataset import create_sam_dataloader


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def check_sam_installation():
    """æ£€æŸ¥SAMæ˜¯å¦å®‰è£…"""
    try:
        import segment_anything
        return True
    except ImportError:
        print("âŒ segment_anythingæœªå®‰è£…")
        print("å®‰è£…æ–¹æ³•: pip install git+https://github.com/facebookresearch/segment-anything.git")
        return False


class SAMTrainer:
    """SAMè®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        
        # è®¾ç½®è®¾å¤‡
        self.device = self._setup_device()
        
        # è®¾ç½®éšæœºç§å­
        set_seed(config['seed'])
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config['output']['output_dir']) / config['output']['experiment_name']
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        with open(self.output_dir / 'config.yaml', 'w') as f:
            yaml.dump(config, f)
        
        # TensorBoard
        if config['output']['use_tensorboard']:
            self.writer = SummaryWriter(
                log_dir=str(self.output_dir / config['output']['tensorboard_dir'])
            )
        else:
            self.writer = None
        
        # åŠ è½½æ¨¡å‹
        print("\n=== åŠ è½½SAMæ¨¡å‹ ===")
        self.model = self._load_model()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("\n=== åˆ›å»ºæ•°æ®åŠ è½½å™¨ ===")
        self.train_loader, self.val_loader = self._create_dataloaders()
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        self.criterion = self._create_criterion()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = torch.cuda.amp.GradScaler() if config['device']['use_amp'] else None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.best_metric = 0.0
        
        print(f"\nâœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        if self.config['device']['use_cuda'] and torch.cuda.is_available():
            device_id = self.config['device']['cuda_device']
            if ',' in str(device_id):
                # å¤šGPUï¼ˆæš‚ä¸æ”¯æŒï¼‰
                device = torch.device(f"cuda:{device_id.split(',')[0]}")
                print(f"ä½¿ç”¨GPU: {device_id} (å¤šGPUæš‚ä¸æ”¯æŒï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª)")
            else:
                device = torch.device(f"cuda:{device_id}")
                print(f"ä½¿ç”¨GPU: {device_id}")
        else:
            device = torch.device("cpu")
            print("ä½¿ç”¨CPU")
        
        return device
    
    def _load_model(self) -> nn.Module:
        """åŠ è½½SAMæ¨¡å‹"""
        from segment_anything import sam_model_registry
        
        model_type = self.config['model']['type']
        checkpoint_path = self.config['model']['checkpoint']
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
        
        print(f"åŠ è½½æ¨¡å‹: {model_type}")
        print(f"æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        sam = sam_model_registry[model_type](checkpoint=checkpoint_path)
        sam = sam.to(self.device)
        
        # åº”ç”¨å¾®è°ƒç­–ç•¥
        strategy = self.config['finetuning']['strategy']
        print(f"å¾®è°ƒç­–ç•¥: {strategy}")
        
        if strategy == 'full':
            # å…¨å‚æ•°å¾®è°ƒ
            self._setup_full_finetuning(sam)
        elif strategy == 'adapter':
            # Adapterå¾®è°ƒ
            self._setup_adapter_finetuning(sam)
        elif strategy == 'lora':
            # LoRAå¾®è°ƒ
            self._setup_lora_finetuning(sam)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¾®è°ƒç­–ç•¥: {strategy}")
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        total_params = sum(p.numel() for p in sam.parameters())
        trainable_params = sum(p.numel() for p in sam.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        
        return sam
    
    def _setup_full_finetuning(self, model: nn.Module):
        """è®¾ç½®å…¨å‚æ•°å¾®è°ƒ"""
        # æ ¹æ®é…ç½®å†»ç»“éƒ¨åˆ†ç»„ä»¶
        if self.config['model']['freeze_image_encoder']:
            print("  å†»ç»“å›¾åƒç¼–ç å™¨")
            for param in model.image_encoder.parameters():
                param.requires_grad = False
        
        if self.config['model']['freeze_prompt_encoder']:
            print("  å†»ç»“æç¤ºç¼–ç å™¨")
            for param in model.prompt_encoder.parameters():
                param.requires_grad = False
        
        if self.config['model']['freeze_mask_decoder']:
            print("  å†»ç»“æ©ç è§£ç å™¨")
            for param in model.mask_decoder.parameters():
                param.requires_grad = False
    
    def _setup_adapter_finetuning(self, model: nn.Module):
        """è®¾ç½®Adapterå¾®è°ƒ"""
        print("  Adapterå¾®è°ƒï¼ˆç®€åŒ–å®ç°ï¼Œå†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼‰")
        
        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in model.parameters():
            param.requires_grad = False
        
        # åªå¾®è°ƒæ©ç è§£ç å™¨ï¼ˆä½œä¸ºadapterï¼‰
        for param in model.mask_decoder.parameters():
            param.requires_grad = True
        
        print("  æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–çš„Adapterå®ç°ï¼Œå®é™…åº”åœ¨Transformerå±‚æ·»åŠ adapteræ¨¡å—")
    
    def _setup_lora_finetuning(self, model: nn.Module):
        """è®¾ç½®LoRAå¾®è°ƒ"""
        try:
            from peft import LoraConfig, get_peft_model
            
            print("  é…ç½®LoRA...")
            lora_config_dict = self.config['finetuning']['lora']
            
            # æ³¨æ„ï¼šSAMçš„PEFTæ”¯æŒå¯èƒ½éœ€è¦é¢å¤–çš„é€‚é…
            print("  æ³¨æ„ï¼šSAMçš„LoRAå¾®è°ƒéœ€è¦ç‰¹æ®Šé€‚é…ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–å®ç°")
            
            # ç®€åŒ–å®ç°ï¼šåªå¾®è°ƒmask_decoder
            for param in model.parameters():
                param.requires_grad = False
            
            for param in model.mask_decoder.parameters():
                param.requires_grad = True
            
        except ImportError:
            print("  âš ï¸ peftåº“æœªå®‰è£…ï¼Œå›é€€åˆ°adapteræ¨¡å¼")
            self._setup_adapter_finetuning(model)
    
    def _create_dataloaders(self) -> tuple:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        data_config = self.config['data']
        
        # è®­ç»ƒé›†
        train_loader = create_sam_dataloader(
            data_dir=data_config['data_dir'],
            split=data_config['train_split'],
            batch_size=data_config['batch_size'],
            num_workers=data_config['num_workers'],
            dataset_type=data_config['dataset_type'],
            image_size=data_config['image_size'],
            prompt_mode=data_config['prompt_mode'],
            num_points=data_config.get('num_points', 1),
            augment=data_config['augment'],
        )
        
        # éªŒè¯é›†
        val_loader = create_sam_dataloader(
            data_dir=data_config['data_dir'],
            split=data_config['val_split'],
            batch_size=data_config['batch_size'],
            num_workers=data_config['num_workers'],
            dataset_type=data_config['dataset_type'],
            image_size=data_config['image_size'],
            prompt_mode=data_config['prompt_mode'],
            num_points=data_config.get('num_points', 1),
            augment=False,  # éªŒè¯é›†ä¸å¢å¼º
        )
        
        print(f"è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
        
        return train_loader, val_loader
    
    def _create_optimizer(self) -> torch.optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_config = self.config['training']['optimizer']
        optimizer_type = optimizer_config['type']
        
        # è·å–å¯è®­ç»ƒå‚æ•°
        params = [p for p in self.model.parameters() if p.requires_grad]
        
        if optimizer_type == 'adamw':
            optimizer = torch.optim.AdamW(
                params,
                lr=self.config['training']['learning_rate'],
                betas=optimizer_config['betas'],
                eps=optimizer_config['eps'],
                weight_decay=self.config['training']['weight_decay']
            )
        elif optimizer_type == 'sgd':
            optimizer = torch.optim.SGD(
                params,
                lr=self.config['training']['learning_rate'],
                momentum=0.9,
                weight_decay=self.config['training']['weight_decay']
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_type}")
        
        print(f"ä¼˜åŒ–å™¨: {optimizer_type}")
        print(f"å­¦ä¹ ç‡: {self.config['training']['learning_rate']}")
        
        return optimizer
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_config = self.config['training']['lr_scheduler']
        scheduler_type = scheduler_config['type']
        
        num_training_steps = len(self.train_loader) * self.config['training']['num_epochs']
        num_warmup_steps = int(len(self.train_loader) * self.config['training']['warmup_epochs'])
        
        if scheduler_type == 'cosine':
            from torch.optim.lr_scheduler import CosineAnnealingLR
            scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=num_training_steps - num_warmup_steps,
                eta_min=scheduler_config.get('min_lr', 1e-6)
            )
        elif scheduler_type == 'linear':
            from torch.optim.lr_scheduler import LinearLR
            scheduler = LinearLR(
                self.optimizer,
                start_factor=1.0,
                end_factor=scheduler_config.get('min_lr', 1e-6) / self.config['training']['learning_rate'],
                total_iters=num_training_steps
            )
        else:
            scheduler = None
        
        print(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_type}")
        return scheduler
    
    def _create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        loss_config = self.config['loss']
        
        class SAMLoss(nn.Module):
            def __init__(self, config):
                super().__init__()
                self.config = config
                self.seg_loss_type = config['segmentation_loss']['type']
                self.dice_weight = config['segmentation_loss'].get('dice_weight', 1.0)
                self.bce_weight = config['segmentation_loss'].get('bce_weight', 1.0)
                self.iou_weight = config['iou_loss']['weight']
            
            def dice_loss(self, pred, target, smooth=1.0):
                """DiceæŸå¤±"""
                pred = torch.sigmoid(pred)
                intersection = (pred * target).sum(dim=(2, 3))
                union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))
                dice = (2.0 * intersection + smooth) / (union + smooth)
                return 1.0 - dice.mean()
            
            def bce_loss(self, pred, target):
                """äºŒå…ƒäº¤å‰ç†µæŸå¤±"""
                return F.binary_cross_entropy_with_logits(pred, target.float())
            
            def forward(self, pred_masks, pred_iou, target_masks):
                """
                è®¡ç®—æ€»æŸå¤±
                
                Args:
                    pred_masks: (B, N, H, W) é¢„æµ‹çš„æ©ç logits
                    pred_iou: (B, N) é¢„æµ‹çš„IoUåˆ†æ•°
                    target_masks: (B, H, W) ç›®æ ‡æ©ç 
                """
                # æ‰©å±•target_masksä»¥åŒ¹é…é¢„æµ‹çš„æ•°é‡
                target_masks = target_masks.unsqueeze(1)  # (B, 1, H, W)
                target_masks = target_masks.expand_as(pred_masks)  # (B, N, H, W)
                
                # åˆ†å‰²æŸå¤±
                if self.seg_loss_type == 'dice':
                    seg_loss = self.dice_loss(pred_masks, target_masks)
                elif self.seg_loss_type == 'bce':
                    seg_loss = self.bce_loss(pred_masks, target_masks)
                elif self.seg_loss_type == 'dice_bce':
                    dice = self.dice_loss(pred_masks, target_masks)
                    bce = self.bce_loss(pred_masks, target_masks)
                    seg_loss = self.dice_weight * dice + self.bce_weight * bce
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {self.seg_loss_type}")
                
                # IoUæŸå¤±ï¼ˆMAEï¼‰
                with torch.no_grad():
                    pred_masks_binary = (torch.sigmoid(pred_masks) > 0.5).float()
                    intersection = (pred_masks_binary * target_masks).sum(dim=(2, 3))
                    union = (pred_masks_binary + target_masks).clamp(0, 1).sum(dim=(2, 3))
                    target_iou = intersection / (union + 1e-6)
                
                iou_loss = F.l1_loss(pred_iou, target_iou)
                
                # æ€»æŸå¤±
                total_loss = seg_loss + self.iou_weight * iou_loss
                
                return {
                    'total_loss': total_loss,
                    'seg_loss': seg_loss,
                    'iou_loss': iou_loss
                }
        
        return SAMLoss(loss_config)
    
    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_losses = {
            'total_loss': 0.0,
            'seg_loss': 0.0,
            'iou_loss': 0.0
        }
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.config['training']['num_epochs']}")
        
        for step, batch in enumerate(pbar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            images = batch['image'].to(self.device)
            masks = batch['mask'].to(self.device)
            
            # å‡†å¤‡æç¤º
            prompts = {}
            if 'boxes' in batch:
                prompts['boxes'] = batch['boxes'].to(self.device)
            if 'points' in batch:
                prompts['points'] = batch['points'].to(self.device)
                prompts['point_labels'] = batch['point_labels'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(enabled=self.config['device']['use_amp']):
                # å›¾åƒç¼–ç 
                image_embeddings = self.model.image_encoder(images)
                
                # æç¤ºç¼–ç 
                if 'boxes' in prompts:
                    # ä½¿ç”¨æ¡†æç¤º
                    sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
                        points=None,
                        boxes=prompts['boxes'],
                        masks=None,
                    )
                elif 'points' in prompts:
                    # ä½¿ç”¨ç‚¹æç¤º
                    coords = prompts['points']
                    labels = prompts['point_labels']
                    sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
                        points=(coords, labels),
                        boxes=None,
                        masks=None,
                    )
                else:
                    # æ— æç¤º
                    sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
                        points=None,
                        boxes=None,
                        masks=None,
                    )
                
                # æ©ç è§£ç 
                low_res_masks, iou_predictions = self.model.mask_decoder(
                    image_embeddings=image_embeddings,
                    image_pe=self.model.prompt_encoder.get_dense_pe(),
                    sparse_prompt_embeddings=sparse_embeddings,
                    dense_prompt_embeddings=dense_embeddings,
                    multimask_output=True,
                )
                
                # ä¸Šé‡‡æ ·åˆ°åŸå§‹åˆ†è¾¨ç‡
                masks_pred = F.interpolate(
                    low_res_masks,
                    size=(self.config['data']['image_size'], self.config['data']['image_size']),
                    mode='bilinear',
                    align_corners=False
                )
                
                # è®¡ç®—æŸå¤±
                losses = self.criterion(masks_pred, iou_predictions, masks)
                loss = losses['total_loss']
            
            # åå‘ä¼ æ’­
            if self.scaler is not None:
                self.scaler.scale(loss).backward()
                
                if (step + 1) % self.config['training']['gradient_accumulation_steps'] == 0:
                    # æ¢¯åº¦è£å‰ª
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config['training']['max_grad_norm']
                    )
                    
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
                    
                    if self.scheduler is not None:
                        self.scheduler.step()
            else:
                loss.backward()
                
                if (step + 1) % self.config['training']['gradient_accumulation_steps'] == 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config['training']['max_grad_norm']
                    )
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                    
                    if self.scheduler is not None:
                        self.scheduler.step()
            
            # æ›´æ–°ç»Ÿè®¡
            for key in epoch_losses:
                epoch_losses[key] += losses[key].item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'lr': f"{self.optimizer.param_groups[0]['lr']:.2e}"
            })
            
            # è®°å½•æ—¥å¿—
            if (step + 1) % self.config['output']['logging_steps'] == 0:
                if self.writer is not None:
                    self.writer.add_scalar('train/total_loss', loss.item(), self.global_step)
                    self.writer.add_scalar('train/seg_loss', losses['seg_loss'].item(), self.global_step)
                    self.writer.add_scalar('train/iou_loss', losses['iou_loss'].item(), self.global_step)
                    self.writer.add_scalar('train/lr', self.optimizer.param_groups[0]['lr'], self.global_step)
            
            self.global_step += 1
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= len(self.train_loader)
        
        return epoch_losses
    
    @torch.no_grad()
    def validate(self, epoch: int):
        """éªŒè¯"""
        self.model.eval()
        
        val_losses = {
            'total_loss': 0.0,
            'seg_loss': 0.0,
            'iou_loss': 0.0
        }
        
        metrics = {
            'iou': 0.0,
            'dice': 0.0,
            'pixel_accuracy': 0.0
        }
        
        pbar = tqdm(self.val_loader, desc="Validation")
        
        for batch in pbar:
            images = batch['image'].to(self.device)
            masks = batch['mask'].to(self.device)
            
            prompts = {}
            if 'boxes' in batch:
                prompts['boxes'] = batch['boxes'].to(self.device)
            if 'points' in batch:
                prompts['points'] = batch['points'].to(self.device)
                prompts['point_labels'] = batch['point_labels'].to(self.device)
            
            # å‰å‘ä¼ æ’­ï¼ˆä¸è®­ç»ƒç›¸åŒï¼‰
            image_embeddings = self.model.image_encoder(images)
            
            if 'boxes' in prompts:
                sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
                    points=None,
                    boxes=prompts['boxes'],
                    masks=None,
                )
            elif 'points' in prompts:
                coords = prompts['points']
                labels = prompts['point_labels']
                sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
                    points=(coords, labels),
                    boxes=None,
                    masks=None,
                )
            else:
                sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
                    points=None,
                    boxes=None,
                    masks=None,
                )
            
            low_res_masks, iou_predictions = self.model.mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.model.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=True,
            )
            
            masks_pred = F.interpolate(
                low_res_masks,
                size=(self.config['data']['image_size'], self.config['data']['image_size']),
                mode='bilinear',
                align_corners=False
            )
            
            # è®¡ç®—æŸå¤±
            losses = self.criterion(masks_pred, iou_predictions, masks)
            
            for key in val_losses:
                val_losses[key] += losses[key].item()
            
            # è®¡ç®—æŒ‡æ ‡
            masks_pred_binary = (torch.sigmoid(masks_pred[:, 0]) > 0.5).float()  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ©ç 
            masks_target = masks.float()
            
            # IoU
            intersection = (masks_pred_binary * masks_target).sum(dim=(1, 2))
            union = (masks_pred_binary + masks_target).clamp(0, 1).sum(dim=(1, 2))
            iou = (intersection / (union + 1e-6)).mean()
            metrics['iou'] += iou.item()
            
            # Dice
            dice = (2.0 * intersection / (masks_pred_binary.sum(dim=(1, 2)) + masks_target.sum(dim=(1, 2)) + 1e-6)).mean()
            metrics['dice'] += dice.item()
            
            # Pixel Accuracy
            correct = (masks_pred_binary == masks_target).sum()
            total = masks_target.numel()
            metrics['pixel_accuracy'] += (correct / total).item()
        
        # è®¡ç®—å¹³å‡
        for key in val_losses:
            val_losses[key] /= len(self.val_loader)
        
        for key in metrics:
            metrics[key] /= len(self.val_loader)
        
        # è®°å½•åˆ°TensorBoard
        if self.writer is not None:
            self.writer.add_scalar('val/total_loss', val_losses['total_loss'], epoch)
            self.writer.add_scalar('val/iou', metrics['iou'], epoch)
            self.writer.add_scalar('val/dice', metrics['dice'], epoch)
            self.writer.add_scalar('val/pixel_accuracy', metrics['pixel_accuracy'], epoch)
        
        return val_losses, metrics
    
    def save_checkpoint(self, epoch: int, metrics: Dict, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'metrics': metrics,
            'config': self.config,
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = self.output_dir / f'checkpoint_epoch_{epoch}.pth'
        torch.save(checkpoint, checkpoint_path)
        print(f"âœ… ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä¼˜æ£€æŸ¥ç‚¹
        if is_best:
            best_path = self.output_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
            print(f"ğŸŒŸ ä¿å­˜æœ€ä¼˜æ¨¡å‹: {best_path}")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        save_total_limit = self.config['output']['save_total_limit']
        checkpoints = sorted(self.output_dir.glob('checkpoint_epoch_*.pth'))
        if len(checkpoints) > save_total_limit:
            for old_ckpt in checkpoints[:-save_total_limit]:
                old_ckpt.unlink()
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_ckpt}")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "="*60)
        print("å¼€å§‹è®­ç»ƒ")
        print("="*60)
        
        num_epochs = self.config['training']['num_epochs']
        eval_every = self.config['evaluation']['eval_every_n_epochs']
        
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_losses = self.train_epoch(epoch)
            
            print(f"\nEpoch {epoch+1}/{num_epochs} è®­ç»ƒå®Œæˆ:")
            print(f"  Loss: {train_losses['total_loss']:.4f}")
            print(f"  Seg Loss: {train_losses['seg_loss']:.4f}")
            print(f"  IoU Loss: {train_losses['iou_loss']:.4f}")
            
            # éªŒè¯
            if (epoch + 1) % eval_every == 0 or epoch == num_epochs - 1:
                val_losses, metrics = self.validate(epoch)
                
                print(f"\néªŒè¯ç»“æœ:")
                print(f"  Val Loss: {val_losses['total_loss']:.4f}")
                print(f"  IoU: {metrics['iou']:.4f}")
                print(f"  Dice: {metrics['dice']:.4f}")
                print(f"  Pixel Acc: {metrics['pixel_accuracy']:.4f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                is_best = metrics['iou'] > self.best_metric
                if is_best:
                    self.best_metric = metrics['iou']
                
                if not self.config['output']['save_best_only'] or is_best:
                    self.save_checkpoint(epoch, metrics, is_best)
        
        print("\n" + "="*60)
        print("è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³IoU: {self.best_metric:.4f}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print("="*60)
        
        if self.writer is not None:
            self.writer.close()


def main():
    parser = argparse.ArgumentParser(description="SAMå¾®è°ƒè®­ç»ƒ")
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥SAMæ˜¯å¦å®‰è£…
    if not check_sam_installation():
        sys.exit(1)
    
    # åŠ è½½é…ç½®
    print(f"åŠ è½½é…ç½®: {args.config}")
    config = load_config(args.config)
    
    # æ¢å¤è®­ç»ƒ
    if args.resume:
        config['resume']['enabled'] = True
        config['resume']['checkpoint_path'] = args.resume
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = SAMTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main()

