# SAMæ¨¡å‹å¾®è°ƒ

æœ¬ç›®å½•åŒ…å«SAM (Segment Anything Model) å¾®è°ƒçš„å®Œæ•´ä»£ç å’Œé…ç½®ã€‚

## ğŸ“‹ ç›®å½•

- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [å¾®è°ƒç­–ç•¥](#å¾®è°ƒç­–ç•¥)
- [è®­ç»ƒç›‘æ§](#è®­ç»ƒç›‘æ§)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## åŠŸèƒ½ç‰¹æ€§

### âœ… æ”¯æŒçš„å¾®è°ƒç­–ç•¥
- **Full Fine-tuning**: å¾®è°ƒæ‰€æœ‰å‚æ•°
- **Adapter Tuning**: æ·»åŠ adapterå±‚ï¼ˆå‚æ•°é«˜æ•ˆï¼‰
- **LoRA**: ä½ç§©é€‚åº”ï¼ˆå‚æ•°é«˜æ•ˆï¼‰

### âœ… æ”¯æŒçš„æ•°æ®æ ¼å¼
- **ç›®å½•æ ¼å¼**: `images/` å’Œ `masks/` åˆ†åˆ«å­˜æ”¾å›¾åƒå’Œæ©ç 
- **COCOæ ¼å¼**: æ ‡å‡†COCOå®ä¾‹åˆ†å‰²æ•°æ®é›†

### âœ… æ”¯æŒçš„æç¤ºæ¨¡å¼
- **Box**: è¾¹ç•Œæ¡†æç¤º
- **Point**: ç‚¹æç¤º
- **Both**: æ¡†+ç‚¹ç»„åˆæç¤º

### âœ… å…¶ä»–ç‰¹æ€§
- æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
- æ¢¯åº¦ç´¯ç§¯
- å­¦ä¹ ç‡è°ƒåº¦ï¼ˆCosine/Linearï¼‰
- TensorBoardå¯è§†åŒ–
- è‡ªåŠ¨ä¿å­˜æœ€ä¼˜æ¨¡å‹
- æ•°æ®å¢å¼º

---

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# SAM
pip install git+https://github.com/facebookresearch/segment-anything.git

# å…¶ä»–ä¾èµ–
pip install opencv-python pillow pyyaml tqdm tensorboard

# å¯é€‰ï¼šPEFTï¼ˆç”¨äºLoRAï¼‰
pip install peft

# å¯é€‰ï¼šCOCO APIï¼ˆç”¨äºCOCOæ•°æ®é›†ï¼‰
pip install pycocotools
```

### 2. ä¸‹è½½SAMé¢„è®­ç»ƒæƒé‡

```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p models/sam

# ä¸‹è½½ViT-Bæ¨¡å‹ï¼ˆçº¦375MBï¼‰
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -P models/sam/

# æˆ–è€…ViT-Læ¨¡å‹ï¼ˆçº¦1.2GBï¼‰
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth -P models/sam/

# æˆ–è€…ViT-Hæ¨¡å‹ï¼ˆçº¦2.4GBï¼‰
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P models/sam/
```

---

## æ•°æ®å‡†å¤‡

### æ–¹å¼1ï¼šç›®å½•æ ¼å¼ï¼ˆæ¨èï¼‰

ç»„ç»‡ä½ çš„æ•°æ®å¦‚ä¸‹ï¼š

```
data/segmentation/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”‚   â”œâ”€â”€ img2.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ val/
â”‚       â”œâ”€â”€ img3.jpg
â”‚       â””â”€â”€ ...
â””â”€â”€ masks/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ img1.png  # äºŒå€¼æ©ç ï¼Œ0=èƒŒæ™¯ï¼Œ255=å‰æ™¯
    â”‚   â”œâ”€â”€ img2.png
    â”‚   â””â”€â”€ ...
    â””â”€â”€ val/
        â”œâ”€â”€ img3.png
        â””â”€â”€ ...
```

**æ©ç æ ¼å¼è¦æ±‚**ï¼š
- å•é€šé“ç°åº¦å›¾åƒ
- 0ï¼ˆé»‘è‰²ï¼‰= èƒŒæ™¯
- 255ï¼ˆç™½è‰²ï¼‰= å‰æ™¯
- æ–‡ä»¶åä¸å¯¹åº”çš„å›¾åƒç›¸åŒï¼ˆæ‰©å±•åä¸º.pngï¼‰

### æ–¹å¼2ï¼šCOCOæ ¼å¼

å¦‚æœä½ æœ‰COCOæ ¼å¼çš„æ•°æ®é›†ï¼š

```
data/coco/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train2017/
â”‚   â””â”€â”€ val2017/
â””â”€â”€ annotations/
    â”œâ”€â”€ instances_train2017.json
    â””â”€â”€ instances_val2017.json
```

---

## å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡é…ç½®æ–‡ä»¶

å¤åˆ¶å¹¶ä¿®æ”¹ `config.yaml`ï¼š

```bash
cp config.yaml my_config.yaml
```

å…³é”®é…ç½®é¡¹ï¼š

```yaml
model:
  type: "vit_b"  # é€‰æ‹©æ¨¡å‹å¤§å°
  checkpoint: "models/sam/sam_vit_b_01ec64.pth"

data:
  data_dir: "data/segmentation"  # ä½ çš„æ•°æ®ç›®å½•
  dataset_type: "directory"      # directory æˆ– coco
  prompt_mode: "box"             # box, point, both
  batch_size: 2                  # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´

training:
  num_epochs: 50
  learning_rate: 1.0e-4

output:
  output_dir: "outputs/sam_finetuning"
  experiment_name: "my_experiment"
```

### 2. æµ‹è¯•æ•°æ®åŠ è½½

åœ¨è®­ç»ƒå‰ï¼Œå…ˆæµ‹è¯•æ•°æ®é›†æ˜¯å¦æ­£ç¡®ï¼š

```bash
# æµ‹è¯•ç›®å½•æ ¼å¼æ•°æ®é›†
python dataset.py \
    --data_dir data/segmentation \
    --dataset_type directory \
    --split train \
    --visualize

# æµ‹è¯•COCOæ ¼å¼æ•°æ®é›†
python dataset.py \
    --data_dir data/coco \
    --dataset_type coco \
    --split train \
    --visualize
```

è¿™å°†ç”Ÿæˆ `dataset_sample.png` å¯è§†åŒ–ç»“æœã€‚

### 3. å¼€å§‹è®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
python train.py --config config.yaml

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python train.py --config my_config.yaml

# æ¢å¤è®­ç»ƒ
python train.py --config my_config.yaml --resume outputs/sam_finetuning/my_experiment/checkpoint_epoch_10.pth
```

### 4. ç›‘æ§è®­ç»ƒ

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- å®æ—¶losså’Œå­¦ä¹ ç‡
- æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
- TensorBoardæ—¥å¿—

å¯åŠ¨TensorBoardï¼š

```bash
tensorboard --logdir outputs/sam_finetuning/my_experiment/runs
```

ç„¶åè®¿é—® `http://localhost:6006`

---

## é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

```yaml
model:
  type: "vit_b"  # vit_b, vit_l, vit_h
  checkpoint: "path/to/sam_checkpoint.pth"
  freeze_image_encoder: true   # å†»ç»“å›¾åƒç¼–ç å™¨ï¼ˆæ¨èï¼‰
  freeze_prompt_encoder: false # å†»ç»“æç¤ºç¼–ç å™¨
  freeze_mask_decoder: false   # å†»ç»“æ©ç è§£ç å™¨
```

**å»ºè®®**ï¼š
- å†»ç»“å›¾åƒç¼–ç å™¨å¯ä»¥èŠ‚çœæ˜¾å­˜å’ŒåŠ é€Ÿè®­ç»ƒ
- ä¸»è¦å¾®è°ƒæ©ç è§£ç å™¨å³å¯è·å¾—è‰¯å¥½æ•ˆæœ

### æ•°æ®é…ç½®

```yaml
data:
  data_dir: "data/segmentation"
  dataset_type: "directory"  # directory æˆ– coco
  train_split: "train"
  val_split: "val"
  image_size: 1024           # SAMæ ‡å‡†è¾“å…¥å¤§å°
  prompt_mode: "box"         # box, point, both
  num_points: 3              # ç‚¹æç¤ºæ•°é‡
  batch_size: 2              # batchå¤§å°
  num_workers: 4             # æ•°æ®åŠ è½½çº¿ç¨‹
  augment: true              # æ•°æ®å¢å¼º
```

**æç¤ºæ¨¡å¼é€‰æ‹©**ï¼š
- `box`: ä½¿ç”¨è¾¹ç•Œæ¡†æç¤ºï¼ˆæ¨èï¼Œç¨³å®šï¼‰
- `point`: ä½¿ç”¨ç‚¹æç¤ºï¼ˆçµæ´»ï¼‰
- `both`: åŒæ—¶ä½¿ç”¨æ¡†å’Œç‚¹ï¼ˆæœ€ä½³æ•ˆæœï¼Œä½†æ…¢ï¼‰

### å¾®è°ƒç­–ç•¥

```yaml
finetuning:
  strategy: "adapter"  # full, adapter, lora
  
  # Adapteré…ç½®
  adapter:
    hidden_dim: 256
    adapter_layers: [6, 12, 18, 24]
  
  # LoRAé…ç½®
  lora:
    r: 8
    lora_alpha: 32
    target_modules: ["qkv", "proj"]
    lora_dropout: 0.1
```

**ç­–ç•¥å¯¹æ¯”**ï¼š

| ç­–ç•¥ | å¯è®­ç»ƒå‚æ•° | æ˜¾å­˜éœ€æ±‚ | è®­ç»ƒé€Ÿåº¦ | æ•ˆæœ |
|------|-----------|---------|---------|------|
| Full | 100% | é«˜ | æ…¢ | æœ€ä½³ |
| Adapter | ~5% | ä¸­ | ä¸­ | è‰¯å¥½ |
| LoRA | ~1% | ä½ | å¿« | è‰¯å¥½ |

### è®­ç»ƒé…ç½®

```yaml
training:
  num_epochs: 50
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_epochs: 2
  gradient_accumulation_steps: 4  # æ¨¡æ‹Ÿæ›´å¤§batch
  max_grad_norm: 1.0
  
  lr_scheduler:
    type: "cosine"
    min_lr: 1.0e-6
  
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
```

**è¶…å‚æ•°å»ºè®®**ï¼š
- `learning_rate`: 1e-4 ~ 5e-4ï¼ˆadapter/LoRAï¼‰ï¼Œ1e-5 ~ 1e-4ï¼ˆfullï¼‰
- `batch_size`: 2~4ï¼ˆå—é™äºæ˜¾å­˜ï¼‰
- `gradient_accumulation_steps`: 4~8ï¼ˆæ¨¡æ‹Ÿbatch_size=8~32ï¼‰

### æŸå¤±å‡½æ•°

```yaml
loss:
  segmentation_loss:
    type: "dice_bce"  # dice, bce, dice_bce, focal
    dice_weight: 1.0
    bce_weight: 1.0
  
  iou_loss:
    weight: 1.0
```

**æŸå¤±ç±»å‹**ï¼š
- `dice`: DiceæŸå¤±ï¼ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é²æ£’ï¼‰
- `bce`: äºŒå…ƒäº¤å‰ç†µï¼ˆæ ‡å‡†ï¼‰
- `dice_bce`: ç»„åˆæŸå¤±ï¼ˆæ¨èï¼‰
- `focal`: FocalæŸå¤±ï¼ˆå¤„ç†å›°éš¾æ ·æœ¬ï¼‰

---

## å¾®è°ƒç­–ç•¥

### 1. Full Fine-tuning

å¾®è°ƒæ‰€æœ‰å‚æ•°ï¼Œæ•ˆæœæœ€å¥½ä½†èµ„æºéœ€æ±‚é«˜ã€‚

```yaml
finetuning:
  strategy: "full"

model:
  freeze_image_encoder: true  # å»ºè®®å†»ç»“ä»¥èŠ‚çœèµ„æº
  freeze_prompt_encoder: false
  freeze_mask_decoder: false
```

**é€‚ç”¨åœºæ™¯**ï¼š
- æœ‰å……è¶³çš„GPUèµ„æºï¼ˆ24GB+ï¼‰
- æ•°æ®é›†è¾ƒå¤§ï¼ˆ>10Kæ ·æœ¬ï¼‰
- ä»»åŠ¡ä¸é¢„è®­ç»ƒå·®å¼‚å¤§

### 2. Adapter Tuning

åœ¨æ¨¡å‹ä¸­æ·»åŠ å°çš„adapteræ¨¡å—ï¼Œåªè®­ç»ƒadapterå‚æ•°ã€‚

```yaml
finetuning:
  strategy: "adapter"
  adapter:
    hidden_dim: 256
    adapter_layers: [6, 12, 18, 24]
```

**é€‚ç”¨åœºæ™¯**ï¼š
- GPUèµ„æºæœ‰é™ï¼ˆ12GB+ï¼‰
- æ•°æ®é›†ä¸­ç­‰è§„æ¨¡ï¼ˆ1K~10Kï¼‰
- å¹³è¡¡æ•ˆæœå’Œæ•ˆç‡

### 3. LoRA (Low-Rank Adaptation)

ä½¿ç”¨ä½ç§©çŸ©é˜µè¿‘ä¼¼å‚æ•°æ›´æ–°ï¼Œå‚æ•°é‡æœ€å°ã€‚

```yaml
finetuning:
  strategy: "lora"
  lora:
    r: 8               # ç§©ï¼ˆè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†å‚æ•°è¶Šå¤šï¼‰
    lora_alpha: 32     # ç¼©æ”¾å› å­
    target_modules: ["qkv", "proj"]
    lora_dropout: 0.1
```

**é€‚ç”¨åœºæ™¯**ï¼š
- GPUèµ„æºç´§å¼ ï¼ˆ8GB+ï¼‰
- æ•°æ®é›†è¾ƒå°ï¼ˆ<1Kï¼‰
- å¿«é€Ÿå®éªŒå’Œè¿­ä»£

---

## è®­ç»ƒç›‘æ§

### ç»ˆç«¯è¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå®æ—¶æ˜¾ç¤ºï¼š

```
Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [05:30<00:00,  1.32s/it, loss=0.234, lr=9.5e-05]

Epoch 10/50 è®­ç»ƒå®Œæˆ:
  Loss: 0.2345
  Seg Loss: 0.2100
  IoU Loss: 0.0245

éªŒè¯ç»“æœ:
  Val Loss: 0.1987
  IoU: 0.8123
  Dice: 0.8956
  Pixel Acc: 0.9234

âœ… ä¿å­˜æ£€æŸ¥ç‚¹: outputs/sam_finetuning/my_experiment/checkpoint_epoch_10.pth
ğŸŒŸ ä¿å­˜æœ€ä¼˜æ¨¡å‹: outputs/sam_finetuning/my_experiment/best_model.pth
```

### TensorBoard

å¯åŠ¨TensorBoardåå¯æŸ¥çœ‹ï¼š
- è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
- å„é¡¹æŒ‡æ ‡å˜åŒ–
- å­¦ä¹ ç‡å˜åŒ–

---

## å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³ï¼ˆOut of Memoryï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å°batch_sizeï¼š
   ```yaml
   data:
     batch_size: 1  # ä»2æ”¹ä¸º1
   ```

2. å¢å¤§æ¢¯åº¦ç´¯ç§¯ï¼š
   ```yaml
   training:
     gradient_accumulation_steps: 8  # ä»4æ”¹ä¸º8
   ```

3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š
   ```yaml
   device:
     use_amp: true
   ```

4. å†»ç»“å›¾åƒç¼–ç å™¨ï¼š
   ```yaml
   model:
     freeze_image_encoder: true
   ```

5. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š
   ```yaml
   model:
     type: "vit_b"  # ä¸ç”¨vit_læˆ–vit_h
   ```

### Q2: è®­ç»ƒé€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¯åŠ é€Ÿ2xï¼‰
2. å¢åŠ num_workersï¼ˆæ•°æ®åŠ è½½å¹¶è¡Œï¼‰
3. ä½¿ç”¨SSDå­˜å‚¨æ•°æ®
4. å†»ç»“å›¾åƒç¼–ç å™¨
5. ä½¿ç”¨adapteræˆ–LoRAç­–ç•¥

### Q3: éªŒè¯æŒ‡æ ‡ä¸æå‡

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½ï¼ˆä½¿ç”¨`--visualize`ï¼‰
2. é™ä½å­¦ä¹ ç‡
3. å¢åŠ è®­ç»ƒepoch
4. æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒlossä½ä½†éªŒè¯lossé«˜ï¼‰
5. å°è¯•ä¸åŒçš„æŸå¤±å‡½æ•°

### Q4: å¦‚ä½•åœ¨è‡ªå·±çš„æ•°æ®ä¸Šæµ‹è¯•ï¼Ÿ

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨æ¨ç†è„šæœ¬ï¼š

```python
from segment_anything import sam_model_registry, SamPredictor
import torch

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
sam = sam_model_registry["vit_b"](checkpoint="path/to/checkpoint.pth")
checkpoint = torch.load("outputs/sam_finetuning/my_experiment/best_model.pth")
sam.load_state_dict(checkpoint['model_state_dict'])
sam.eval()

predictor = SamPredictor(sam)

# ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æç¤ºæ–¹å¼
# ...
```

### Q5: æ”¯æŒå¤šGPUè®­ç»ƒå—ï¼Ÿ

å½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒå¤šGPUï¼ˆDataParallel/DDPï¼‰ã€‚

è®¡åˆ’åœ¨åç»­ç‰ˆæœ¬æ·»åŠ ã€‚

---

## æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `dataset.py` | æ•°æ®é›†ç±»ï¼ˆæ”¯æŒç›®å½•å’ŒCOCOæ ¼å¼ï¼‰ |
| `train.py` | è®­ç»ƒè„šæœ¬ |
| `config.yaml` | é…ç½®æ–‡ä»¶æ¨¡æ¿ |
| `README.md` | æœ¬æ–‡æ¡£ |

---

## æ€§èƒ½å‚è€ƒ

åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ˆ~1Kæ ·æœ¬ï¼‰ä¸Šçš„æ€§èƒ½ï¼š

| ç­–ç•¥ | å¯è®­ç»ƒå‚æ•° | è®­ç»ƒæ—¶é—´ | éªŒè¯IoU | æ˜¾å­˜ |
|------|-----------|---------|---------|------|
| Full (freeze encoder) | ~8M | 3h | 0.82 | 18GB |
| Adapter | ~0.5M | 2h | 0.80 | 12GB |
| LoRA | ~0.1M | 1.5h | 0.78 | 10GB |

**ç¡¬ä»¶**: NVIDIA RTX 3090 (24GB)

---

## ç¤ºä¾‹å‘½ä»¤

### åŒ»å­¦å›¾åƒåˆ†å‰²

```bash
# é…ç½®
# - ä½¿ç”¨ViT-Bæ¨¡å‹
# - Adapterå¾®è°ƒ
# - Boxæç¤º
# - 50ä¸ªepoch

python train.py --config configs/medical_segmentation.yaml
```

### é¥æ„Ÿå›¾åƒåˆ†å‰²

```bash
# é…ç½®
# - ä½¿ç”¨ViT-Læ¨¡å‹
# - Fullå¾®è°ƒï¼ˆå†»ç»“ç¼–ç å™¨ï¼‰
# - Bothæç¤ºï¼ˆæ¡†+ç‚¹ï¼‰
# - 100ä¸ªepoch

python train.py --config configs/remote_sensing.yaml
```

---

## å¼•ç”¨

å¦‚æœæœ¬ä»£ç å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨SAMåŸæ–‡ï¼š

```bibtex
@article{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}
```

---

## åç»­è®¡åˆ’

- [ ] æ”¯æŒå¤šGPUè®­ç»ƒï¼ˆDDPï¼‰
- [ ] æ”¯æŒæ›´å¤šæ•°æ®å¢å¼º
- [ ] æ·»åŠ è¯„ä¼°è„šæœ¬
- [ ] æ·»åŠ æ¨ç†è„šæœ¬
- [ ] æ”¯æŒè§†é¢‘åˆ†å‰²
- [ ] æ”¯æŒäº¤äº’å¼æ ‡æ³¨å·¥å…·

---

## è”ç³»ä¸è´¡çŒ®

- GitHub: [Large-Model-Tutorial](https://github.com/your-repo)
- é—®é¢˜åé¦ˆ: [Issues](https://github.com/your-repo/issues)
- è´¡çŒ®ä»£ç : [Pull Requests](https://github.com/your-repo/pulls)

æ¬¢è¿æå‡ºé—®é¢˜å’Œè´¡çŒ®ä»£ç ï¼

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€

