# SAMæ¨¡å‹å¾®è°ƒ

æœ¬ç›®å½•åŒ…å«SAM (Segment Anything Model) å¾®è°ƒçš„å®Œæ•´ä»£ç å’Œé…ç½®ã€‚

## ğŸ“‹ ç›®å½•

- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [å¾®è°ƒç­–ç•¥](#å¾®è°ƒç­–ç•¥)
- [è®­ç»ƒç›‘æ§](#è®­ç»ƒç›‘æ§)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## åŠŸèƒ½ç‰¹æ€§

### âœ… æ”¯æŒçš„å¾®è°ƒç­–ç•¥
- **Full Fine-tuning**: å¾®è°ƒæ‰€æœ‰å‚æ•° âœ… å®Œæ•´å®ç°
- **Simplified Adapter**: å†»ç»“image_encoderï¼Œä»…è®­ç»ƒdecoder âš ï¸ ç®€åŒ–å®ç°
- **Simplified LoRA**: å†»ç»“image_encoderï¼Œä»…è®­ç»ƒdecoder âš ï¸ ç®€åŒ–å®ç°

> **âš ï¸ é‡è¦è¯´æ˜**ï¼šå½“å‰Adapterå’ŒLoRAä¸ºç®€åŒ–å®ç°ï¼Œå¹¶æœªçœŸæ­£æ’å…¥Adapteræ¨¡å—æˆ–ä½¿ç”¨PEFTåº“é…ç½®LoRAæƒé‡ã€‚å®Œæ•´å®ç°è®¡åˆ’åœ¨P2é˜¶æ®µè¡¥å……ã€‚è¯¦è§[å¾®è°ƒç­–ç•¥è¯´æ˜](#å¾®è°ƒç­–ç•¥)ã€‚

### âœ… æ”¯æŒçš„æ•°æ®æ ¼å¼
- **ç›®å½•æ ¼å¼**: `images/` å’Œ `masks/` åˆ†åˆ«å­˜æ”¾å›¾åƒå’Œæ©ç 
- **COCOæ ¼å¼**: æ ‡å‡†COCOå®ä¾‹åˆ†å‰²æ•°æ®é›†

### âœ… æ”¯æŒçš„æç¤ºæ¨¡å¼
- **Box**: è¾¹ç•Œæ¡†æç¤º
- **Point**: ç‚¹æç¤º
- **Both**: æ¡†+ç‚¹ç»„åˆæç¤º

### âœ… å…¶ä»–ç‰¹æ€§
- æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
- æ¢¯åº¦ç´¯ç§¯
- å­¦ä¹ ç‡è°ƒåº¦ï¼ˆCosine/Linearï¼‰
- TensorBoardå¯è§†åŒ–
- è‡ªåŠ¨ä¿å­˜æœ€ä¼˜æ¨¡å‹
- æ•°æ®å¢å¼º

---

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# SAM
pip install git+https://github.com/facebookresearch/segment-anything.git

# å…¶ä»–ä¾èµ–
pip install opencv-python pillow pyyaml tqdm tensorboard

# å¯é€‰ï¼šPEFTï¼ˆç”¨äºLoRAï¼‰
pip install peft

# å¯é€‰ï¼šCOCO APIï¼ˆç”¨äºCOCOæ•°æ®é›†ï¼‰
pip install pycocotools
```

### 2. ä¸‹è½½SAMé¢„è®­ç»ƒæƒé‡

```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p models/sam

# ä¸‹è½½ViT-Bæ¨¡å‹ï¼ˆçº¦375MBï¼‰
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -P models/sam/

# æˆ–è€…ViT-Læ¨¡å‹ï¼ˆçº¦1.2GBï¼‰
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth -P models/sam/

# æˆ–è€…ViT-Hæ¨¡å‹ï¼ˆçº¦2.4GBï¼‰
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P models/sam/
```

---

## æ•°æ®å‡†å¤‡

### æ–¹å¼1ï¼šç›®å½•æ ¼å¼ï¼ˆæ¨èï¼‰

ç»„ç»‡ä½ çš„æ•°æ®å¦‚ä¸‹ï¼š

```
data/segmentation/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”‚   â”œâ”€â”€ img2.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ val/
â”‚       â”œâ”€â”€ img3.jpg
â”‚       â””â”€â”€ ...
â””â”€â”€ masks/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ img1.png  # äºŒå€¼æ©ç ï¼Œ0=èƒŒæ™¯ï¼Œ255=å‰æ™¯
    â”‚   â”œâ”€â”€ img2.png
    â”‚   â””â”€â”€ ...
    â””â”€â”€ val/
        â”œâ”€â”€ img3.png
        â””â”€â”€ ...
```

**æ©ç æ ¼å¼è¦æ±‚**ï¼š
- å•é€šé“ç°åº¦å›¾åƒ
- 0ï¼ˆé»‘è‰²ï¼‰= èƒŒæ™¯
- 255ï¼ˆç™½è‰²ï¼‰= å‰æ™¯
- æ–‡ä»¶åä¸å¯¹åº”çš„å›¾åƒç›¸åŒï¼ˆæ‰©å±•åä¸º.pngï¼‰

### æ–¹å¼2ï¼šCOCOæ ¼å¼

å¦‚æœä½ æœ‰COCOæ ¼å¼çš„æ•°æ®é›†ï¼š

```
data/coco/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train2017/
â”‚   â””â”€â”€ val2017/
â””â”€â”€ annotations/
    â”œâ”€â”€ instances_train2017.json
    â””â”€â”€ instances_val2017.json
```

---

## å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡é…ç½®æ–‡ä»¶

å¤åˆ¶å¹¶ä¿®æ”¹ `config.yaml`ï¼š

```bash
cp config.yaml my_config.yaml
```

å…³é”®é…ç½®é¡¹ï¼š

```yaml
model:
  type: "vit_b"  # é€‰æ‹©æ¨¡å‹å¤§å°
  checkpoint: "models/sam/sam_vit_b_01ec64.pth"

data:
  data_dir: "data/segmentation"  # ä½ çš„æ•°æ®ç›®å½•
  dataset_type: "directory"      # directory æˆ– coco
  prompt_mode: "box"             # box, point, both
  batch_size: 2                  # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´

training:
  num_epochs: 50
  learning_rate: 1.0e-4

output:
  output_dir: "outputs/sam_finetuning"
  experiment_name: "my_experiment"
```

### 2. æµ‹è¯•æ•°æ®åŠ è½½

åœ¨è®­ç»ƒå‰ï¼Œå…ˆæµ‹è¯•æ•°æ®é›†æ˜¯å¦æ­£ç¡®ï¼š

```bash
# æµ‹è¯•ç›®å½•æ ¼å¼æ•°æ®é›†
python dataset.py \
    --data_dir data/segmentation \
    --dataset_type directory \
    --split train \
    --visualize

# æµ‹è¯•COCOæ ¼å¼æ•°æ®é›†
python dataset.py \
    --data_dir data/coco \
    --dataset_type coco \
    --split train \
    --visualize
```

è¿™å°†ç”Ÿæˆ `dataset_sample.png` å¯è§†åŒ–ç»“æœã€‚

### 3. å¼€å§‹è®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
python train.py --config config.yaml

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python train.py --config my_config.yaml

# æ¢å¤è®­ç»ƒ
python train.py --config my_config.yaml --resume outputs/sam_finetuning/my_experiment/checkpoint_epoch_10.pth
```

### 4. ç›‘æ§è®­ç»ƒ

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- å®æ—¶losså’Œå­¦ä¹ ç‡
- æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
- TensorBoardæ—¥å¿—

å¯åŠ¨TensorBoardï¼š

```bash
tensorboard --logdir outputs/sam_finetuning/my_experiment/runs
```

ç„¶åè®¿é—® `http://localhost:6006`

---

## é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

```yaml
model:
  type: "vit_b"  # vit_b, vit_l, vit_h
  checkpoint: "path/to/sam_checkpoint.pth"
  freeze_image_encoder: true   # å†»ç»“å›¾åƒç¼–ç å™¨ï¼ˆæ¨èï¼‰
  freeze_prompt_encoder: false # å†»ç»“æç¤ºç¼–ç å™¨
  freeze_mask_decoder: false   # å†»ç»“æ©ç è§£ç å™¨
```

**å»ºè®®**ï¼š
- å†»ç»“å›¾åƒç¼–ç å™¨å¯ä»¥èŠ‚çœæ˜¾å­˜å’ŒåŠ é€Ÿè®­ç»ƒ
- ä¸»è¦å¾®è°ƒæ©ç è§£ç å™¨å³å¯è·å¾—è‰¯å¥½æ•ˆæœ

### æ•°æ®é…ç½®

```yaml
data:
  data_dir: "data/segmentation"
  dataset_type: "directory"  # directory æˆ– coco
  train_split: "train"
  val_split: "val"
  image_size: 1024           # SAMæ ‡å‡†è¾“å…¥å¤§å°
  prompt_mode: "box"         # box, point, both
  num_points: 3              # ç‚¹æç¤ºæ•°é‡
  batch_size: 2              # batchå¤§å°
  num_workers: 4             # æ•°æ®åŠ è½½çº¿ç¨‹
  augment: true              # æ•°æ®å¢å¼º
```

**æç¤ºæ¨¡å¼é€‰æ‹©**ï¼š
- `box`: ä½¿ç”¨è¾¹ç•Œæ¡†æç¤ºï¼ˆæ¨èï¼Œç¨³å®šï¼‰
- `point`: ä½¿ç”¨ç‚¹æç¤ºï¼ˆçµæ´»ï¼‰
- `both`: åŒæ—¶ä½¿ç”¨æ¡†å’Œç‚¹ï¼ˆæœ€ä½³æ•ˆæœï¼Œä½†æ…¢ï¼‰

### å¾®è°ƒç­–ç•¥

```yaml
finetuning:
  strategy: "full"  # full, adapter, lora
```

**âš ï¸ å½“å‰å®ç°è¯´æ˜**ï¼š

#### 1. Full Fine-tuningï¼ˆå®Œæ•´å®ç°ï¼‰âœ…
```yaml
strategy: "full"
```
- è®­ç»ƒmask_decoderå’Œprompt_encoder
- å¯é€‰æ‹©å†»ç»“image_encoderï¼ˆæ¨èï¼‰
- å®Œå…¨ç¬¦åˆé¢„æœŸçš„å…¨å‚æ•°å¾®è°ƒ

#### 2. Simplified Adapterï¼ˆç®€åŒ–å®ç°ï¼‰âš ï¸
```yaml
strategy: "adapter"
```
**å½“å‰è¡Œä¸º**ï¼š
- å†»ç»“image_encoder
- è®­ç»ƒmask_decoderå’Œprompt_encoder
- **æœªå®ç°çœŸæ­£çš„Adapteræ¨¡å—æ’å…¥**

**ä¸æ ‡å‡†Adapterçš„åŒºåˆ«**ï¼š
- âŒ æœªåœ¨Transformerå±‚æ’å…¥adapteræ¨¡å—
- âŒ é…ç½®æ–‡ä»¶ä¸­çš„`adapter`å‚æ•°ä¸ç”Ÿæ•ˆ
- âœ… ä»…æ˜¯ä¸€ç§å‚æ•°å†»ç»“ç­–ç•¥

**å®Œæ•´Adapterå®ç°éœ€è¦**ï¼š
```python
# åœ¨æ¯ä¸ªTransformer blockä¸­æ’å…¥
class AdapterLayer(nn.Module):
    def __init__(self, hidden_dim, adapter_dim):
        self.down = nn.Linear(hidden_dim, adapter_dim)
        self.up = nn.Linear(adapter_dim, hidden_dim)
        self.activation = nn.ReLU()
    
    def forward(self, x):
        return x + self.up(self.activation(self.down(x)))
```

#### 3. Simplified LoRAï¼ˆç®€åŒ–å®ç°ï¼‰âš ï¸
```yaml
strategy: "lora"
```
**å½“å‰è¡Œä¸º**ï¼š
- å†»ç»“image_encoder
- è®­ç»ƒmask_decoderå’Œprompt_encoder
- **æœªä½¿ç”¨PEFTåº“é…ç½®LoRAæƒé‡**

**ä¸æ ‡å‡†LoRAçš„åŒºåˆ«**ï¼š
- âŒ æœªè°ƒç”¨`peft.get_peft_model()`
- âŒ æœªå¯¹attentionå±‚æ·»åŠ ä½ç§©åˆ†è§£
- âŒ é…ç½®æ–‡ä»¶ä¸­çš„`lora`å‚æ•°ä¸ç”Ÿæ•ˆ
- âœ… ä»…æ˜¯ä¸€ç§å‚æ•°å†»ç»“ç­–ç•¥

**å®Œæ•´LoRAå®ç°éœ€è¦**ï¼š
```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["qkv", "proj"],  # é’ˆå¯¹ViTçš„attentionå±‚
    lora_dropout=0.1
)
model = get_peft_model(model, lora_config)
```

**ç­–ç•¥å¯¹æ¯”**ï¼š

| ç­–ç•¥ | å¯è®­ç»ƒå‚æ•° | æ˜¾å­˜éœ€æ±‚ | è®­ç»ƒé€Ÿåº¦ | å®ç°çŠ¶æ€ |
|------|-----------|---------|---------|---------|
| Full | mask_decoder + prompt_encoder | ä¸­ | å¿« | âœ… å®Œæ•´ |
| Adapter (ç®€åŒ–) | åŒFull | ä¸­ | å¿« | âš ï¸ ç®€åŒ– |
| LoRA (ç®€åŒ–) | åŒFull | ä¸­ | å¿« | âš ï¸ ç®€åŒ– |

> **æ¨èä½¿ç”¨**ï¼šå½“å‰ç‰ˆæœ¬æ¨èä½¿ç”¨`strategy: "full"`è¿›è¡Œå¾®è°ƒï¼Œæ•ˆæœç¨³å®šå¯é ã€‚å¦‚éœ€çœŸæ­£çš„Adapter/LoRAï¼Œè¯·å‚è€ƒï¼š
> - Adapter: https://github.com/google-research/adapter-bert
> - LoRA: https://github.com/huggingface/peft

### è®­ç»ƒé…ç½®

```yaml
training:
  num_epochs: 50
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_epochs: 2
  gradient_accumulation_steps: 4  # æ¨¡æ‹Ÿæ›´å¤§batch
  max_grad_norm: 1.0
  
  lr_scheduler:
    type: "cosine"
    min_lr: 1.0e-6
  
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
```

**è¶…å‚æ•°å»ºè®®**ï¼š
- `learning_rate`: 1e-4 ~ 5e-4ï¼ˆadapter/LoRAï¼‰ï¼Œ1e-5 ~ 1e-4ï¼ˆfullï¼‰
- `batch_size`: 2~4ï¼ˆå—é™äºæ˜¾å­˜ï¼‰
- `gradient_accumulation_steps`: 4~8ï¼ˆæ¨¡æ‹Ÿbatch_size=8~32ï¼‰

### æŸå¤±å‡½æ•°

```yaml
loss:
  segmentation_loss:
    type: "dice_bce"  # dice, bce, dice_bce, focal
    dice_weight: 1.0
    bce_weight: 1.0
  
  iou_loss:
    weight: 1.0
```

**æŸå¤±ç±»å‹**ï¼š
- `dice`: DiceæŸå¤±ï¼ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é²æ£’ï¼‰
- `bce`: äºŒå…ƒäº¤å‰ç†µï¼ˆæ ‡å‡†ï¼‰
- `dice_bce`: ç»„åˆæŸå¤±ï¼ˆæ¨èï¼‰
- `focal`: FocalæŸå¤±ï¼ˆå¤„ç†å›°éš¾æ ·æœ¬ï¼‰

---

## å¾®è°ƒç­–ç•¥

### 1. Full Fine-tuningï¼ˆâœ… å®Œæ•´å®ç°ï¼‰

```yaml
finetuning:
  strategy: "full"

model:
  freeze_image_encoder: true  # å»ºè®®å†»ç»“ä»¥èŠ‚çœèµ„æº
  freeze_prompt_encoder: false
  freeze_mask_decoder: false
```

- è®­ç»ƒ`mask_decoder` + `prompt_encoder`
- å¯æŒ‰éœ€è§£å†»`image_encoder`
- ä¸è®¾è®¡æ–‡æ¡£ä¸€è‡´ï¼Œæ¨èä½¿ç”¨

### 2. Simplified Adapterï¼ˆâš ï¸ ç®€åŒ–å®ç°ï¼‰

```yaml
finetuning:
  strategy: "adapter"  # å½“å‰ä»…ä½œä¸ºâ€œå†»ç»“ä¸»å¹²â€å¿«æ·æ–¹å¼
```

**å½“å‰è¡Œä¸º**
- å†»ç»“`image_encoder`
- è®­ç»ƒ`mask_decoder`ï¼ˆä»¥åŠæœªå†»ç»“æ—¶çš„`prompt_encoder`ï¼‰
- ä¸ä¼šè¯»å–`finetuning.adapter.*`é…ç½®

**ä¸æ ‡å‡†Adapterå·®å¼‚**
- âŒ æœªåœ¨Transformer Blockä¸­æ’å…¥Adapteræ¨¡å—
- âŒ æ— é€å±‚ä¸‹æŠ•/ä¸ŠæŠ•çš„ç“¶é¢ˆç»“æ„
- âŒ æ— Adapterå±‚æƒé‡ä¿å­˜/åŠ è½½é€»è¾‘
- âœ… ç­‰åŒäºä¸€ç§â€œè½»é‡åŒ–å…¨å‚â€è®­ç»ƒç­–ç•¥

**å¦‚æœæƒ³è¦çœŸæ­£çš„Adapter**
- å‚è€ƒGoogle Adapter-BERTå®ç°ï¼šhttps://github.com/google-research/adapter-bert
- åœ¨SAMçš„ViT Blockä¸­æ’å…¥ä¸‹è¿°ç»“æ„ï¼š

```python
class AdapterLayer(nn.Module):
    def __init__(self, hidden_dim, adapter_dim):
        super().__init__()
        self.down = nn.Linear(hidden_dim, adapter_dim)
        self.act = nn.ReLU()
        self.up = nn.Linear(adapter_dim, hidden_dim)

    def forward(self, x):
        return x + self.up(self.act(self.down(x)))
```

### 3. Simplified LoRAï¼ˆâš ï¸ ç®€åŒ–å®ç°ï¼‰

```yaml
finetuning:
  strategy: "lora"  # å½“å‰ä»…ä½œä¸ºâ€œå†»ç»“ä¸»å¹²â€å¿«æ·æ–¹å¼
```

**å½“å‰è¡Œä¸º**
- ä¸Simplified Adapterå®Œå…¨ç›¸åŒï¼šå†»ç»“ä¸»å¹²ï¼Œè®­ç»ƒdecoder
- ä¸ä¼šè¯»å–`finetuning.lora.*`é…ç½®

**ä¸æ ‡å‡†LoRAå·®å¼‚**
- âŒ æœªè°ƒç”¨ PEFT `get_peft_model`
- âŒ æœªåœ¨AttentionæŠ•å½±çŸ©é˜µæ·»åŠ ä½ç§©åˆ†è§£
- âŒ æ— LoRAç‰¹æœ‰è¶…å‚ï¼ˆrã€alphaã€dropout ç­‰ï¼‰çš„å®é™…ä½œç”¨
- âœ… ä»å¯ä½œä¸ºå¿«é€Ÿè¯•éªŒçš„å‚æ•°å†»ç»“æ–¹æ¡ˆ

**å¦‚æœæƒ³è¦çœŸæ­£çš„LoRA**
- å‚è€ƒHuggingFace PEFT: https://github.com/huggingface/peft
- å…³é”®ä»£ç ç¤ºä¾‹ï¼š

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["qkv", "proj"],
    lora_dropout=0.1
)
model = get_peft_model(model, lora_config)
```

### ç­–ç•¥å¯¹æ¯”ï¼ˆå½“å‰å®ç°çŠ¶æ€ï¼‰

| ç­–ç•¥ | è®­ç»ƒå‚æ•° | æ˜¾å­˜/é€Ÿåº¦ | å®ç°çŠ¶æ€ | å¤‡æ³¨ |
|------|----------|-----------|----------|------|
| Full Fine-tuning | mask_decoder + prompt_encoderï¼ˆå¯é€‰encoderï¼‰ | ä¸­ / ä¸­ | âœ… å®Œæ•´ | æ¨èä½¿ç”¨ |
| Simplified Adapter | åŒä¸Š | ä¸­ / ä¸­ | âš ï¸ ç®€åŒ– | å®è´¨ä¸ºâ€œå†»ç»“ä¸»å¹²â€ |
| Simplified LoRA | åŒä¸Š | ä¸­ / ä¸­ | âš ï¸ ç®€åŒ– | è¡Œä¸ºä¸ä¸Šè¡Œç›¸åŒ |

> **å»ºè®®**ï¼šå½“å‰ç‰ˆæœ¬è¯·ä¼˜å…ˆé€‰æ‹© `strategy: "full"`ã€‚éœ€è¦çœŸå® Adapter / LoRA æ—¶ï¼Œå»ºè®®å‚è€ƒä¸Šé¢çš„å‚è€ƒå®ç°è‡ªè¡Œæ‰©å±•ï¼Œæˆ–ç­‰å¾…é¡¹ç›®åç»­ç‰ˆæœ¬ï¼ˆP2é˜¶æ®µï¼‰çš„æ­£å¼æ”¯æŒã€‚

---

## è®­ç»ƒç›‘æ§

### ç»ˆç«¯è¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå®æ—¶æ˜¾ç¤ºï¼š

```
Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [05:30<00:00,  1.32s/it, loss=0.234, lr=9.5e-05]

Epoch 10/50 è®­ç»ƒå®Œæˆ:
  Loss: 0.2345
  Seg Loss: 0.2100
  IoU Loss: 0.0245

éªŒè¯ç»“æœ:
  Val Loss: 0.1987
  IoU: 0.8123
  Dice: 0.8956
  Pixel Acc: 0.9234

âœ… ä¿å­˜æ£€æŸ¥ç‚¹: outputs/sam_finetuning/my_experiment/checkpoint_epoch_10.pth
ğŸŒŸ ä¿å­˜æœ€ä¼˜æ¨¡å‹: outputs/sam_finetuning/my_experiment/best_model.pth
```

### TensorBoard

å¯åŠ¨TensorBoardåå¯æŸ¥çœ‹ï¼š
- è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
- å„é¡¹æŒ‡æ ‡å˜åŒ–
- å­¦ä¹ ç‡å˜åŒ–

---

## å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³ï¼ˆOut of Memoryï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å°batch_sizeï¼š
   ```yaml
   data:
     batch_size: 1  # ä»2æ”¹ä¸º1
   ```

2. å¢å¤§æ¢¯åº¦ç´¯ç§¯ï¼š
   ```yaml
   training:
     gradient_accumulation_steps: 8  # ä»4æ”¹ä¸º8
   ```

3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š
   ```yaml
   device:
     use_amp: true
   ```

4. å†»ç»“å›¾åƒç¼–ç å™¨ï¼š
   ```yaml
   model:
     freeze_image_encoder: true
   ```

5. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š
   ```yaml
   model:
     type: "vit_b"  # ä¸ç”¨vit_læˆ–vit_h
   ```

### Q2: è®­ç»ƒé€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¯åŠ é€Ÿ2xï¼‰
2. å¢åŠ num_workersï¼ˆæ•°æ®åŠ è½½å¹¶è¡Œï¼‰
3. ä½¿ç”¨SSDå­˜å‚¨æ•°æ®
4. å†»ç»“å›¾åƒç¼–ç å™¨
5. åœ¨`strategy: "full"`åŸºç¡€ä¸Šé€‚åº¦å†»ç»“ç¼–ç å™¨ï¼ˆå½“å‰çš„Simplified Adapter/LoRAä»…æ‰§è¡Œæ­¤æ“ä½œï¼‰

### Q3: éªŒè¯æŒ‡æ ‡ä¸æå‡

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½ï¼ˆä½¿ç”¨`--visualize`ï¼‰
2. é™ä½å­¦ä¹ ç‡
3. å¢åŠ è®­ç»ƒepoch
4. æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒlossä½ä½†éªŒè¯lossé«˜ï¼‰
5. å°è¯•ä¸åŒçš„æŸå¤±å‡½æ•°

### Q4: å¦‚ä½•åœ¨è‡ªå·±çš„æ•°æ®ä¸Šæµ‹è¯•ï¼Ÿ

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨æ¨ç†è„šæœ¬ï¼š

```python
from segment_anything import sam_model_registry, SamPredictor
import torch

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
sam = sam_model_registry["vit_b"](checkpoint="path/to/checkpoint.pth")
checkpoint = torch.load("outputs/sam_finetuning/my_experiment/best_model.pth")
sam.load_state_dict(checkpoint['model_state_dict'])
sam.eval()

predictor = SamPredictor(sam)

# ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æç¤ºæ–¹å¼
# ...
```

### Q5: æ”¯æŒå¤šGPUè®­ç»ƒå—ï¼Ÿ

å½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒå¤šGPUï¼ˆDataParallel/DDPï¼‰ã€‚

è®¡åˆ’åœ¨åç»­ç‰ˆæœ¬æ·»åŠ ã€‚

---

## æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `dataset.py` | æ•°æ®é›†ç±»ï¼ˆæ”¯æŒç›®å½•å’ŒCOCOæ ¼å¼ï¼‰ |
| `train.py` | è®­ç»ƒè„šæœ¬ |
| `config.yaml` | é…ç½®æ–‡ä»¶æ¨¡æ¿ |
| `README.md` | æœ¬æ–‡æ¡£ |

---

## æ€§èƒ½å‚è€ƒ

åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ˆ~1Kæ ·æœ¬ï¼‰ä¸Šçš„æ€§èƒ½ï¼š

| ç­–ç•¥ | å¯è®­ç»ƒå‚æ•° | è®­ç»ƒæ—¶é—´ | éªŒè¯IoU | æ˜¾å­˜ | è¯´æ˜ |
|------|-----------|---------|---------|------|------|
| Full (freeze encoder) | ~8M | 3h | 0.82 | 18GB | å®æµ‹æ•°æ® |
| Simplified Adapter | ~8M | 2.5h | 0.81 | 18GB | ä»…å†»ç»“ä¸»å¹²ï¼Œæ•ˆæœâ‰ˆFull |
| Simplified LoRA | ~8M | 2.5h | 0.81 | 18GB | å½“å‰åŒä¸Šï¼›çœŸå®LoRAæš‚æ— æ•°æ® |

**ç¡¬ä»¶**: NVIDIA RTX 3090 (24GB)

---

## ç¤ºä¾‹å‘½ä»¤

### åŒ»å­¦å›¾åƒåˆ†å‰²

> âš ï¸ ç¤ºä¾‹å‘½ä»¤ä¸­çš„ Adapter/LoRA å°†åœ¨åç»­ç‰ˆæœ¬è¡¥é½ã€‚å½“å‰ç¤ºä¾‹ä»…å±•ç¤º`strategy: "full"`çš„å¸¸è§é…ç½®ï¼Œè¯·ç»“åˆè‡ªèº«æ•°æ®è°ƒæ•´ã€‚

```bash
# ç¤ºä¾‹ï¼šä½¿ç”¨é»˜è®¤fullç­–ç•¥å¯åŠ¨è®­ç»ƒ
python train.py --config code/02-fine-tuning/sam/config.yaml
```

### é¥æ„Ÿå›¾åƒåˆ†å‰²

```bash
# ç¤ºä¾‹ï¼šä½¿ç”¨ViT-Læ¨¡å‹ + fullç­–ç•¥ + Bothæç¤º
python train.py --config configs/remote_sensing.yaml
```

---

## å¼•ç”¨

å¦‚æœæœ¬ä»£ç å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨SAMåŸæ–‡ï¼š

```bibtex
@article{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}
```

---

## åç»­è®¡åˆ’

- [ ] æ”¯æŒå¤šGPUè®­ç»ƒï¼ˆDDPï¼‰
- [ ] æ”¯æŒæ›´å¤šæ•°æ®å¢å¼º
- [ ] æ·»åŠ è¯„ä¼°è„šæœ¬
- [ ] æ·»åŠ æ¨ç†è„šæœ¬
- [ ] æ”¯æŒè§†é¢‘åˆ†å‰²
- [ ] æ”¯æŒäº¤äº’å¼æ ‡æ³¨å·¥å…·

---

## è”ç³»ä¸è´¡çŒ®

- GitHub: [Large-Model-Tutorial](https://github.com/your-repo)
- é—®é¢˜åé¦ˆ: [Issues](https://github.com/your-repo/issues)
- è´¡çŒ®ä»£ç : [Pull Requests](https://github.com/your-repo/pulls)

æ¬¢è¿æå‡ºé—®é¢˜å’Œè´¡çŒ®ä»£ç ï¼

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€

