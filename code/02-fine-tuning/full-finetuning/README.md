# å…¨å‚æ•°å¾®è°ƒç¤ºä¾‹ä»£ç 

å¯¹CLIPæ¨¡å‹è¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰ï¼Œæ›´æ–°æ‰€æœ‰æ¨¡å‹å‚æ•°ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
code/02-fine-tuning/full-finetuning/
â”œâ”€â”€ __init__.py           # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ config.yaml           # é…ç½®æ–‡ä»¶
â”œâ”€â”€ train.py              # è®­ç»ƒè„šæœ¬
â””â”€â”€ README.md             # æœ¬æ–‡æ¡£
```

**æ³¨æ„**: è¯„ä¼°å’Œæ¨ç†å¯ä»¥å¤ç”¨LoRAçš„è„šæœ¬ï¼Œå› ä¸ºæ¨¡å‹ç»“æ„ç›¸åŒã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®é›†

```bash
# ä½¿ç”¨ä¸LoRAç›¸åŒçš„æ•°æ®å‡†å¤‡è„šæœ¬
python scripts/prepare_dog_dataset.py --output_dir data/dogs --num_classes 10

# æ‰‹åŠ¨æ·»åŠ å›¾åƒåˆ°å¯¹åº”ç›®å½•
# data/dogs/train/golden_retriever/*.jpg
# data/dogs/train/labrador/*.jpg
# ...
```

### 2. è®­ç»ƒæ¨¡å‹

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python code/02-fine-tuning/full-finetuning/train.py

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python code/02-fine-tuning/full-finetuning/train.py \
    --config code/02-fine-tuning/full-finetuning/config.yaml \
    --data_dir data/dogs \
    --output_dir outputs/full_finetuning
```

**è®­ç»ƒç›‘æ§**:
```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir logs/full_finetuning
```

### 3. è¯„ä¼°æ¨¡å‹

```bash
# å¤ç”¨LoRAçš„è¯„ä¼°è„šæœ¬
python code/02-fine-tuning/lora/evaluate.py \
    --checkpoint outputs/full_finetuning/checkpoint-epoch-20 \
    --data_dir data/dogs \
    --split test \
    --output_dir outputs/evaluation_full
```

### 4. æ¨ç†é¢„æµ‹

```bash
# å¤ç”¨LoRAçš„æ¨ç†è„šæœ¬
python code/02-fine-tuning/lora/inference.py \
    --checkpoint outputs/full_finetuning/checkpoint-epoch-20 \
    --image path/to/dog.jpg \
    --top_k 5
```

## âš™ï¸ é…ç½®è¯´æ˜

### å…³é”®é…ç½®å·®å¼‚ï¼ˆvs LoRAï¼‰

| é…ç½®é¡¹ | å…¨å‚æ•°å¾®è°ƒ | LoRAå¾®è°ƒ |
|--------|-----------|---------|
| **batch_size** | 16ï¼ˆæ›´å°ï¼‰| 32 |
| **learning_rate** | 1e-5ï¼ˆæ›´å°ï¼‰| 5e-4 |
| **gradient_accumulation** | 2ï¼ˆæ›´å¤§ï¼‰| 1 |
| **num_epochs** | 20ï¼ˆæ›´å¤šï¼‰| 10 |
| **æ˜¾å­˜éœ€æ±‚** | 24GB+ | 8GB+ |
| **è®­ç»ƒæ—¶é—´** | é•¿ï¼ˆæ•°å°æ—¶ï¼‰| çŸ­ï¼ˆæ•°ååˆ†é’Ÿï¼‰|

### åˆ†å±‚å­¦ä¹ ç‡

```yaml
training:
  layerwise_lr:
    enabled: true        # å¯ç”¨åˆ†å±‚å­¦ä¹ ç‡
    decay_rate: 0.95     # å±‚é—´è¡°å‡ç‡
```

**åŸç†**: åº•å±‚ï¼ˆæ¥è¿‘è¾“å…¥ï¼‰å­¦ä¹ ç‡æ›´å°ï¼Œé¡¶å±‚ï¼ˆæ¥è¿‘è¾“å‡ºï¼‰å­¦ä¹ ç‡æ›´å¤§ã€‚

**æ•ˆæœ**: 
- ä¿æŠ¤é¢„è®­ç»ƒçš„åº•å±‚ç‰¹å¾
- å…è®¸é¡¶å±‚å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
- æé«˜è®­ç»ƒç¨³å®šæ€§

### æ¸è¿›å¼è§£å†»ï¼ˆå¯é€‰ï¼‰

```yaml
training:
  progressive_unfreezing:
    enabled: true
    unfreeze_schedule:
      - epoch: 0
        layers: ["classifier"]  # å…ˆè®­ç»ƒåˆ†ç±»å¤´
      - epoch: 2
        layers: ["vision_model.encoder.layers.11"]  # é€æ­¥è§£å†»
      - epoch: 6
        layers: ["all"]  # æœ€åå…¨éƒ¨è§£å†»
```

**ä¼˜ç‚¹**:
- é¿å…ç¾éš¾æ€§é—å¿˜
- æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹
- å¯èƒ½è·å¾—æ›´å¥½çš„æ•ˆæœ

**ç¼ºç‚¹**:
- è®­ç»ƒæ—¶é—´æ›´é•¿
- éœ€è¦æ›´å¤šè½®æ•°

## ğŸ“Š æ€§èƒ½åŸºå‡†

### ç¡¬ä»¶è¦æ±‚

| é…ç½® | æœ€ä½ | æ¨è |
|------|------|------|
| GPU | 24GB (RTX 3090/4090) | 40GB+ (A100) |
| å†…å­˜ | 32GB | 64GB |
| ç¡¬ç›˜ | 20GB | 50GB (SSD) |

### è®­ç»ƒæ—¶é—´ä¼°ç®—

| æ•°æ®é›†å¤§å° | GPU | è®­ç»ƒæ—¶é—´ (20 epochs) |
|-----------|-----|---------------------|
| 1K å›¾åƒ | RTX 3090 | ~1å°æ—¶ |
| 5K å›¾åƒ | RTX 3090 | ~4å°æ—¶ |
| 10K å›¾åƒ | A100 | ~6å°æ—¶ |

### æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | é¢„è®­ç»ƒCLIP | LoRAå¾®è°ƒ | å…¨å‚æ•°å¾®è°ƒ |
|------|-----------|---------|-----------|
| Top-1å‡†ç¡®ç‡ | ~60% | ~85% | ~88-90% |
| Top-5å‡†ç¡®ç‡ | ~85% | ~95% | ~97-98% |
| å¯è®­ç»ƒå‚æ•° | - | <1% | 100% |
| æ˜¾å­˜éœ€æ±‚ | - | 8-16GB | 24-40GB |
| è®­ç»ƒæ—¶é—´ | - | çŸ­ | é•¿ |

## ğŸ”§ å¸¸è§é—®é¢˜

### 1. CUDAå†…å­˜ä¸è¶³

**é—®é¢˜**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:

**æ–¹æ¡ˆ1: å‡å°batch size + å¢åŠ æ¢¯åº¦ç´¯ç§¯**
```yaml
data:
  batch_size: 8  # æˆ– 4

training:
  gradient_accumulation_steps: 4  # æˆ– 8
```

**æ–¹æ¡ˆ2: å¯ç”¨æ··åˆç²¾åº¦**
```yaml
hardware:
  mixed_precision: true
```

**æ–¹æ¡ˆ3: ä½¿ç”¨æ¸è¿›å¼è§£å†»**
```yaml
training:
  progressive_unfreezing:
    enabled: true
```

### 2. è®­ç»ƒä¸ç¨³å®š

**é—®é¢˜**: æŸå¤±éœ‡è¡æˆ–å‘æ•£

**è§£å†³æ–¹æ¡ˆ**:
- é™ä½å­¦ä¹ ç‡ï¼ˆ1e-6 ~ 5e-6ï¼‰
- å¢åŠ warmupæ¯”ä¾‹ï¼ˆ0.1 â†’ 0.2ï¼‰
- å¯ç”¨æ¢¯åº¦è£å‰ªï¼ˆmax_grad_norm: 0.5ï¼‰
- ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡

### 3. è¿‡æ‹Ÿåˆ

**é—®é¢˜**: è®­ç»ƒé›†å‡†ç¡®ç‡é«˜ï¼ŒéªŒè¯é›†å‡†ç¡®ç‡ä½

**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ weight_decayï¼ˆ0.01 â†’ 0.05ï¼‰
- ä½¿ç”¨æ•°æ®å¢å¼º
- æ—©åœï¼ˆpatience: 3-5ï¼‰
- å‡å°‘è®­ç»ƒè½®æ•°

### 4. è®­ç»ƒæ—¶é—´å¤ªé•¿

**é—®é¢˜**: è®­ç»ƒé€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆ**:
- å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
- ä½¿ç”¨å¤šGPUè®­ç»ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
- è€ƒè™‘ä½¿ç”¨LoRAä»£æ›¿å…¨å‚æ•°å¾®è°ƒ
- å‡å°‘æ•°æ®é›†å¤§å°è¿›è¡Œå¿«é€Ÿå®éªŒ

### 5. ä½•æ—¶ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒï¼Ÿ

**æ¨èä½¿ç”¨**:
- âœ… æœ‰å¤§é‡æ ‡æ³¨æ•°æ®ï¼ˆ10K+æ ·æœ¬ï¼‰
- âœ… ä»»åŠ¡ä¸é¢„è®­ç»ƒå·®å¼‚å¤§
- âœ… è¿½æ±‚æœ€ä½³æ€§èƒ½
- âœ… æœ‰å……è¶³çš„è®¡ç®—èµ„æºï¼ˆ24GB+æ˜¾å­˜ï¼‰

**ä¸æ¨èä½¿ç”¨**:
- âŒ æ•°æ®é‡å°‘ï¼ˆ<1Kæ ·æœ¬ï¼‰â†’ ä½¿ç”¨LoRA
- âŒ èµ„æºæœ‰é™ï¼ˆ<16GBæ˜¾å­˜ï¼‰â†’ ä½¿ç”¨QLoRA
- âŒ å¿«é€Ÿè¿­ä»£ â†’ ä½¿ç”¨LoRA
- âŒ è¾¹ç¼˜è®¾å¤‡éƒ¨ç½² â†’ ä½¿ç”¨LoRA

## ğŸ’¡ ä¼˜åŒ–æŠ€å·§

### 1. åˆ†å±‚å­¦ä¹ ç‡

```python
# åº•å±‚ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
layer_0: 1e-5 * 0.95^11 â‰ˆ 6e-6
layer_1: 1e-5 * 0.95^10 â‰ˆ 6.3e-6
...
layer_11: 1e-5 * 0.95^0 = 1e-5
```

### 2. å­¦ä¹ ç‡è°ƒåº¦

æ¨èä½¿ç”¨Cosineè°ƒåº¦å™¨ï¼š
- å¹³æ»‘çš„å­¦ä¹ ç‡è¡°å‡
- é¿å…çªç„¶çš„æ€§èƒ½ä¸‹é™
- å¯èƒ½æ‰¾åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜

### 3. æ··åˆç²¾åº¦è®­ç»ƒ

**ä¼˜ç‚¹**:
- æ˜¾å­˜å ç”¨å‡åŠ
- è®­ç»ƒé€Ÿåº¦æå‡1.5-2x
- å‡ ä¹ä¸æŸå¤±ç²¾åº¦

**æ³¨æ„**:
- éœ€è¦NVIDIA GPUï¼ˆVoltaæ¶æ„åŠä»¥ä¸Šï¼‰
- å¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡

### 4. æ¢¯åº¦ç´¯ç§¯

æ¨¡æ‹Ÿæ›´å¤§çš„batch sizeï¼š
```
æœ‰æ•ˆbatch size = batch_size Ã— gradient_accumulation_steps
ä¾‹å¦‚: 8 Ã— 4 = 32
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å…¨å‚æ•°å¾®è°ƒæ•™ç¨‹](../../../docs/02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/03-å…¨å‚æ•°å¾®è°ƒ.md)
- [LoRAå¾®è°ƒå®è·µ](../../../docs/02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/02-LoRAå¾®è°ƒå®è·µ.md)
- [å¾®è°ƒç†è®ºåŸºç¡€](../../../docs/02-æ¨¡å‹å¾®è°ƒæŠ€æœ¯/01-å¾®è°ƒç†è®ºåŸºç¡€.md)

## ğŸ”„ ä¸LoRAçš„æ¯”è¾ƒ

### ä½•æ—¶é€‰æ‹©å…¨å‚æ•°å¾®è°ƒï¼Ÿ

```
æ•°æ®é‡ï¼Ÿ
  â”œâ”€ <1Kæ ·æœ¬ â†’ LoRA/QLoRA
  â”œâ”€ 1K-10K â†’ LoRAï¼ˆæ¨èï¼‰æˆ–å…¨å‚æ•°
  â””â”€ >10K â†’ å…¨å‚æ•°å¾®è°ƒï¼ˆå¦‚æœèµ„æºå…è®¸ï¼‰

æ˜¾å­˜ï¼Ÿ
  â”œâ”€ <16GB â†’ QLoRA
  â”œâ”€ 16-24GB â†’ LoRA
  â””â”€ >24GB â†’ å…¨å‚æ•°å¾®è°ƒ

æ€§èƒ½è¦æ±‚ï¼Ÿ
  â”œâ”€ å¤Ÿç”¨å³å¯ â†’ LoRA
  â””â”€ è¿½æ±‚æè‡´ â†’ å…¨å‚æ•°å¾®è°ƒ
```

### å®éªŒå¯¹æ¯”ï¼ˆ10ç±»çŠ¬ç§ï¼Œ5Kå›¾åƒï¼‰

| æ–¹æ³• | å‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜ | æ¨¡å‹å¤§å° |
|------|--------|---------|------|---------|
| LoRA | 85.3% | 1å°æ—¶ | 12GB | 0.6GB + 5MB |
| å…¨å‚æ•° | 88.7% | 4å°æ—¶ | 28GB | 0.6GB |

**ç»“è®º**: å…¨å‚æ•°å¾®è°ƒæå‡3.4%å‡†ç¡®ç‡ï¼Œä½†éœ€è¦4å€è®­ç»ƒæ—¶é—´å’Œ2.3å€æ˜¾å­˜ã€‚

## ğŸ¤ è´¡çŒ®

å¦‚æœæ‚¨å‘ç°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿æäº¤Issueæˆ–Pull Requestã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªMITè®¸å¯è¯ã€‚

