"""
LoRAå¾®è°ƒæ¨¡å‹è¯„ä¼°è„šæœ¬

è¯„ä¼°å¾®è°ƒåçš„CLIPæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
"""

import os
import sys
import argparse
import yaml
from pathlib import Path
from typing import Dict, Any, List
import json

import torch
import torch.nn as nn
from transformers import CLIPModel, CLIPProcessor
from peft import PeftModel
from tqdm import tqdm
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œå½“å‰ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
current_dir = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(current_dir))

# å¯¼å…¥å½“å‰ç›®å½•çš„æ¨¡å—
from train import CLIPClassifier, load_config
from dataset import DogBreedDataset


def load_model(checkpoint_dir: str, num_classes: int, device: torch.device):
    """
    åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
    
    Args:
        checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
        num_classes: ç±»åˆ«æ•°
        device: è®¾å¤‡
        
    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {checkpoint_dir}")
    
    # åŠ è½½CLIPæ¨¡å‹ï¼ˆå¸¦LoRAï¼‰
    clip_model = CLIPModel.from_pretrained(checkpoint_dir)
    
    # åˆ›å»ºåˆ†ç±»å™¨
    model = CLIPClassifier(clip_model, num_classes)
    
    # åŠ è½½åˆ†ç±»å¤´æƒé‡
    classifier_path = os.path.join(checkpoint_dir, 'classifier.pt')
    if os.path.exists(classifier_path):
        model.classifier.load_state_dict(torch.load(classifier_path, map_location=device))
        print("âœ… åˆ†ç±»å¤´æƒé‡åŠ è½½æˆåŠŸ")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°åˆ†ç±»å¤´æƒé‡æ–‡ä»¶")
    
    model = model.to(device)
    model.eval()
    
    return model


@torch.no_grad()
def evaluate_model(
    model: nn.Module,
    dataloader: torch.utils.data.DataLoader,
    device: torch.device,
    class_names: List[str]
) -> Dict[str, Any]:
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    model.eval()
    
    all_preds = []
    all_labels = []
    all_probs = []
    
    total_correct = 0
    total_samples = 0
    
    print("\nğŸ” è¯„ä¼°ä¸­...")
    for pixel_values, labels in tqdm(dataloader):
        pixel_values = pixel_values.to(device)
        labels = labels.to(device)
        
        # å‰å‘ä¼ æ’­
        logits = model(pixel_values)
        probs = torch.softmax(logits, dim=1)
        
        # é¢„æµ‹
        _, predicted = torch.max(logits, 1)
        
        # ç»Ÿè®¡
        total_correct += (predicted == labels).sum().item()
        total_samples += labels.size(0)
        
        # æ”¶é›†ç»“æœ
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs.cpu().numpy())
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = 100.0 * total_correct / total_samples
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    report = classification_report(
        all_labels,
        all_preds,
        target_names=class_names,
        output_dict=True
    )
    
    # ç”Ÿæˆæ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    
    # è®¡ç®—Top-5å‡†ç¡®ç‡ï¼ˆå¦‚æœç±»åˆ«æ•°>=5ï¼‰
    top5_acc = None
    if len(class_names) >= 5:
        top5_preds = np.argsort(all_probs, axis=1)[:, -5:]
        top5_correct = sum([label in top5_preds[i] for i, label in enumerate(all_labels)])
        top5_acc = 100.0 * top5_correct / total_samples
    
    results = {
        'accuracy': accuracy,
        'top5_accuracy': top5_acc,
        'classification_report': report,
        'confusion_matrix': cm,
        'predictions': all_preds,
        'labels': all_labels,
        'probabilities': all_probs
    }
    
    return results


def plot_confusion_matrix(
    cm: np.ndarray,
    class_names: List[str],
    save_path: str
):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    
    Args:
        cm: æ··æ·†çŸ©é˜µ
        class_names: ç±»åˆ«åç§°
        save_path: ä¿å­˜è·¯å¾„
    """
    plt.figure(figsize=(12, 10))
    
    # å½’ä¸€åŒ–
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(
        cm_normalized,
        annot=True,
        fmt='.2f',
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names,
        cbar_kws={'label': 'Normalized Count'}
    )
    
    plt.title('Confusion Matrix (Normalized)', fontsize=16, pad=20)
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    plt.close()


def plot_class_performance(
    report: Dict[str, Any],
    class_names: List[str],
    save_path: str
):
    """
    ç»˜åˆ¶å„ç±»åˆ«æ€§èƒ½
    
    Args:
        report: åˆ†ç±»æŠ¥å‘Š
        class_names: ç±»åˆ«åç§°
        save_path: ä¿å­˜è·¯å¾„
    """
    # æå–å„ç±»åˆ«çš„æŒ‡æ ‡
    precisions = [report[name]['precision'] for name in class_names]
    recalls = [report[name]['recall'] for name in class_names]
    f1_scores = [report[name]['f1-score'] for name in class_names]
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    x = np.arange(len(class_names))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(14, 6))
    
    ax.bar(x - width, precisions, width, label='Precision', alpha=0.8)
    ax.bar(x, recalls, width, label='Recall', alpha=0.8)
    ax.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8)
    
    ax.set_xlabel('Class', fontsize=12)
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('Per-Class Performance Metrics', fontsize=16, pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels(class_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim([0, 1.1])
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ç±»åˆ«æ€§èƒ½å›¾å·²ä¿å­˜: {save_path}")
    plt.close()


def save_results(results: Dict[str, Any], output_dir: str, class_names: List[str]):
    """
    ä¿å­˜è¯„ä¼°ç»“æœ
    
    Args:
        results: è¯„ä¼°ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        class_names: ç±»åˆ«åç§°
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
    report_path = os.path.join(output_dir, 'evaluation_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        f.write(f"æ€»ä½“å‡†ç¡®ç‡: {results['accuracy']:.2f}%\n")
        if results['top5_accuracy']:
            f.write(f"Top-5å‡†ç¡®ç‡: {results['top5_accuracy']:.2f}%\n")
        f.write("\n")
        
        f.write("å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:\n")
        f.write("-" * 60 + "\n")
        report = results['classification_report']
        for class_name in class_names:
            metrics = report[class_name]
            f.write(f"\n{class_name}:\n")
            f.write(f"  Precision: {metrics['precision']:.4f}\n")
            f.write(f"  Recall:    {metrics['recall']:.4f}\n")
            f.write(f"  F1-Score:  {metrics['f1-score']:.4f}\n")
            f.write(f"  Support:   {metrics['support']}\n")
        
        f.write("\n" + "-" * 60 + "\n")
        f.write(f"\nåŠ æƒå¹³å‡:\n")
        f.write(f"  Precision: {report['weighted avg']['precision']:.4f}\n")
        f.write(f"  Recall:    {report['weighted avg']['recall']:.4f}\n")
        f.write(f"  F1-Score:  {report['weighted avg']['f1-score']:.4f}\n")
    
    print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # ä¿å­˜JSONæ ¼å¼
    json_path = os.path.join(output_dir, 'evaluation_results.json')
    json_results = {
        'accuracy': float(results['accuracy']),
        'top5_accuracy': float(results['top5_accuracy']) if results['top5_accuracy'] else None,
        'classification_report': results['classification_report']
    }
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… JSONç»“æœå·²ä¿å­˜: {json_path}")
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm_path = os.path.join(output_dir, 'confusion_matrix.png')
    plot_confusion_matrix(results['confusion_matrix'], class_names, cm_path)
    
    # ç»˜åˆ¶ç±»åˆ«æ€§èƒ½
    perf_path = os.path.join(output_dir, 'class_performance.png')
    plot_class_performance(results['classification_report'], class_names, perf_path)


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°LoRAå¾®è°ƒæ¨¡å‹")
    parser.add_argument(
        '--checkpoint',
        type=str,
        required=True,
        help='æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•'
    )
    parser.add_argument(
        '--data_dir',
        type=str,
        default='data/dogs',
        help='æ•°æ®é›†ç›®å½•'
    )
    parser.add_argument(
        '--split',
        type=str,
        default='test',
        choices=['train', 'val', 'test'],
        help='è¯„ä¼°çš„æ•°æ®é›†åˆ†å‰²'
    )
    parser.add_argument(
        '--batch_size',
        type=int,
        default=32,
        help='æ‰¹æ¬¡å¤§å°'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='outputs/evaluation',
        help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("LoRAå¾®è°ƒæ¨¡å‹è¯„ä¼°")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½å¤„ç†å™¨
    print("\nğŸ“¦ åŠ è½½CLIPå¤„ç†å™¨...")
    processor = CLIPProcessor.from_pretrained(args.checkpoint)
    
    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“Š åŠ è½½{args.split}æ•°æ®é›†...")
    dataset = DogBreedDataset(
        data_dir=args.data_dir,
        split=args.split,
        processor=processor
    )
    
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4
    )
    
    class_names = dataset.classes
    num_classes = len(class_names)
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, num_classes, device)
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_model(model, dataloader, device, class_names)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 60)
    print("è¯„ä¼°ç»“æœ")
    print("=" * 60)
    print(f"å‡†ç¡®ç‡: {results['accuracy']:.2f}%")
    if results['top5_accuracy']:
        print(f"Top-5å‡†ç¡®ç‡: {results['top5_accuracy']:.2f}%")
    
    # ä¿å­˜ç»“æœ
    save_results(results, args.output_dir, class_names)
    
    print("\nâœ… è¯„ä¼°å®Œæˆï¼")


if __name__ == '__main__':
    main()

