# LoRA微调配置文件

# 模型配置
model:
  name: "openai/clip-vit-base-patch32"  # 预训练模型名称
  cache_dir: "models/"  # 模型缓存目录
  
# LoRA配置
lora:
  r: 8  # LoRA秩（rank）
  lora_alpha: 32  # LoRA缩放系数
  target_modules:  # 要应用LoRA的模块
    - "q_proj"
    - "v_proj"
  lora_dropout: 0.1  # LoRA dropout率
  bias: "none"  # 偏置处理方式: "none", "all", "lora_only"
  
# 数据配置
data:
  data_dir: "data/dogs"  # 数据集根目录
  batch_size: 32  # 批次大小
  num_workers: 4  # 数据加载线程数
  pin_memory: true  # 是否使用固定内存（GPU训练推荐）
  
# 训练配置
training:
  num_epochs: 10  # 训练轮数
  learning_rate: 5.0e-4  # 学习率
  weight_decay: 0.01  # 权重衰减
  warmup_ratio: 0.1  # 预热比例
  gradient_accumulation_steps: 1  # 梯度累积步数
  max_grad_norm: 1.0  # 梯度裁剪阈值
  
  # 学习率调度器
  lr_scheduler:
    type: "cosine"  # 类型: "linear", "cosine", "constant"
    num_cycles: 0.5  # cosine调度器的周期数
  
  # 早停
  early_stopping:
    enabled: true
    patience: 3  # 容忍轮数
    min_delta: 0.001  # 最小改善量
  
# 评估配置
evaluation:
  eval_steps: 100  # 每N步评估一次
  save_steps: 200  # 每N步保存一次
  logging_steps: 50  # 每N步记录一次
  
# 输出配置
output:
  output_dir: "outputs/lora_finetuning"  # 输出目录
  save_total_limit: 3  # 最多保存N个检查点
  log_dir: "logs/lora_finetuning"  # 日志目录
  
# 硬件配置
hardware:
  device: "cuda"  # 设备: "cuda", "cpu", "mps"
  mixed_precision: true  # 是否使用混合精度训练（FP16）
  
# 随机种子
seed: 42

