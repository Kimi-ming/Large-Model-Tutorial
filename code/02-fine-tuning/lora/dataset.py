"""
çŠ¬ç§åˆ†ç±»æ•°æ®é›†

ç”¨äºLoRAå¾®è°ƒçš„å›¾åƒåˆ†ç±»æ•°æ®é›†å®ç°
"""

import os
from pathlib import Path
from typing import Optional, Callable, Tuple, List
import torch
from torch.utils.data import Dataset
from PIL import Image
from transformers import CLIPProcessor


class DogBreedDataset(Dataset):
    """
    çŠ¬ç§åˆ†ç±»æ•°æ®é›†
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        split: 'train', 'val', æˆ– 'test'
        processor: CLIPå¤„ç†å™¨
        transform: é¢å¤–çš„å›¾åƒå˜æ¢ï¼ˆå¯é€‰ï¼‰
    """
    
    def __init__(
        self, 
        data_dir: str, 
        split: str = 'train', 
        processor: Optional[CLIPProcessor] = None,
        transform: Optional[Callable] = None
    ):
        self.data_dir = os.path.join(data_dir, split)
        self.processor = processor
        self.transform = transform
        self.split = split
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.data_dir):
            raise ValueError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        # åŠ è½½ç±»åˆ«å’Œå›¾åƒè·¯å¾„
        self.classes = sorted([
            d for d in os.listdir(self.data_dir) 
            if os.path.isdir(os.path.join(self.data_dir, d))
        ])
        
        if len(self.classes) == 0:
            raise ValueError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•ç±»åˆ«ç›®å½•")
        
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}
        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}
        
        # åŠ è½½æ‰€æœ‰æ ·æœ¬
        self.samples = []
        for class_name in self.classes:
            class_dir = os.path.join(self.data_dir, class_name)
            for img_name in os.listdir(class_dir):
                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                    img_path = os.path.join(class_dir, img_name)
                    self.samples.append((img_path, self.class_to_idx[class_name]))
        
        if len(self.samples) == 0:
            raise ValueError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
        
        print(f"âœ… åŠ è½½ {split} é›†: {len(self.samples)} å¼ å›¾åƒ")
        print(f"   ç±»åˆ«æ•°: {len(self.classes)}")
        print(f"   ç±»åˆ«: {', '.join(self.classes)}")
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        img_path, label = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å›¾åƒå¤±è´¥: {img_path}, é”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªç©ºç™½å›¾åƒä½œä¸ºåå¤‡
            image = Image.new('RGB', (224, 224), color='white')
        
        # åº”ç”¨é¢å¤–çš„å˜æ¢ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.transform:
            image = self.transform(image)
        
        # ä½¿ç”¨CLIPå¤„ç†å™¨å¤„ç†å›¾åƒ
        if self.processor:
            # å¤„ç†å™¨è¿”å›çš„æ˜¯å­—å…¸ï¼Œæˆ‘ä»¬éœ€è¦æå–pixel_values
            inputs = self.processor(images=image, return_tensors="pt")
            pixel_values = inputs['pixel_values'].squeeze(0)  # ç§»é™¤batchç»´åº¦
            return pixel_values, label
        else:
            # å¦‚æœæ²¡æœ‰å¤„ç†å™¨ï¼Œè¿”å›PILå›¾åƒ
            return image, label
    
    def get_class_name(self, idx: int) -> str:
        """æ ¹æ®ç´¢å¼•è·å–ç±»åˆ«åç§°"""
        return self.idx_to_class.get(idx, "unknown")
    
    def get_class_distribution(self) -> dict:
        """è·å–ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡"""
        distribution = {cls: 0 for cls in self.classes}
        for _, label in self.samples:
            class_name = self.idx_to_class[label]
            distribution[class_name] += 1
        return distribution


def create_dataloaders(
    data_dir: str,
    processor: CLIPProcessor,
    batch_size: int = 32,
    num_workers: int = 4,
    pin_memory: bool = True
) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
    """
    åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    
    Args:
        data_dir: æ•°æ®é›†æ ¹ç›®å½•
        processor: CLIPå¤„ç†å™¨
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°
        pin_memory: æ˜¯å¦ä½¿ç”¨å›ºå®šå†…å­˜ï¼ˆGPUè®­ç»ƒæ—¶æ¨èï¼‰
        
    Returns:
        train_loader, val_loader, test_loader
    """
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = DogBreedDataset(data_dir, split='train', processor=processor)
    val_dataset = DogBreedDataset(data_dir, split='val', processor=processor)
    test_dataset = DogBreedDataset(data_dir, split='test', processor=processor)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    print("\nğŸ“Š æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, {len(train_loader)} æ‰¹æ¬¡")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬, {len(val_loader)} æ‰¹æ¬¡")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬, {len(test_loader)} æ‰¹æ¬¡")
    
    return train_loader, val_loader, test_loader


def test_dataset():
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    from transformers import CLIPProcessor
    
    print("=" * 60)
    print("æ•°æ®é›†æµ‹è¯•")
    print("=" * 60)
    
    # åŠ è½½å¤„ç†å™¨
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    
    # æµ‹è¯•æ•°æ®é›†
    try:
        dataset = DogBreedDataset(
            data_dir="data/dogs",
            split="train",
            processor=processor
        )
        
        print(f"\nâœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
        print(f"   æ ·æœ¬æ•°: {len(dataset)}")
        print(f"   ç±»åˆ«æ•°: {len(dataset.classes)}")
        
        # æµ‹è¯•è·å–ä¸€ä¸ªæ ·æœ¬
        if len(dataset) > 0:
            pixel_values, label = dataset[0]
            print(f"\nğŸ“¦ æ ·æœ¬æµ‹è¯•:")
            print(f"   å›¾åƒå¼ é‡å½¢çŠ¶: {pixel_values.shape}")
            print(f"   æ ‡ç­¾: {label} ({dataset.get_class_name(label)})")
        
        # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
        distribution = dataset.get_class_distribution()
        for class_name, count in distribution.items():
            print(f"   {class_name}: {count} å¼ ")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿:")
        print("1. å·²è¿è¡Œ python scripts/prepare_dog_dataset.py")
        print("2. å·²å°†å›¾åƒæ–‡ä»¶æ”¾å…¥ data/dogs/ ç›®å½•")


if __name__ == '__main__':
    test_dataset()

