"""
PyTorchæ¨¡å‹è½¬æ¢ä¸ºæ˜‡è…¾OMæ ¼å¼

ä½¿ç”¨ATCå·¥å…·å°†ONNXæ¨¡å‹è½¬æ¢ä¸ºæ˜‡è…¾ä¼˜åŒ–çš„OMæ ¼å¼
"""

import os
import subprocess
import argparse
from pathlib import Path
import json


class ModelConverter:
    """æ¨¡å‹è½¬æ¢å™¨"""
    
    SUPPORTED_SOC = ['Ascend310', 'Ascend910', 'Ascend310P', 'Ascend910B']
    
    def __init__(self, soc_version: str = 'Ascend910'):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            soc_version: ç›®æ ‡èŠ¯ç‰‡ç‰ˆæœ¬
        """
        if soc_version not in self.SUPPORTED_SOC:
            raise ValueError(f"Unsupported SOC: {soc_version}, must be one of {self.SUPPORTED_SOC}")
        
        self.soc_version = soc_version
        
        # æ£€æŸ¥atcæ˜¯å¦å¯ç”¨
        try:
            subprocess.run(['atc', '--help'], capture_output=True, check=True)
            print(f"âœ… ATCå·¥å…·å¯ç”¨")
        except (subprocess.CalledProcessError, FileNotFoundError):
            raise RuntimeError("ATC tool not found. Please install CANN toolkit.")
    
    def convert_onnx_to_om(
        self,
        onnx_path: str,
        output_path: str,
        input_shape: str = None,
        dynamic_dims: str = None,
        **kwargs
    ) -> bool:
        """
        è½¬æ¢ONNXæ¨¡å‹ä¸ºOMæ ¼å¼
        
        Args:
            onnx_path: ONNXæ¨¡å‹è·¯å¾„
            output_path: è¾“å‡ºOMæ¨¡å‹è·¯å¾„
            input_shape: è¾“å…¥shapeï¼Œæ ¼å¼: "input1:1,3,224,224;input2:1,512"
            dynamic_dims: åŠ¨æ€ç»´åº¦ï¼Œæ ¼å¼: "1;2;4;8"
            **kwargs: å…¶ä»–ATCå‚æ•°
            
        Returns:
            è½¬æ¢æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(onnx_path):
            raise FileNotFoundError(f"ONNX model not found: {onnx_path}")
        
        # æ„å»ºATCå‘½ä»¤
        cmd = [
            'atc',
            f'--model={onnx_path}',
            '--framework=5',  # 5 = ONNX
            f'--output={output_path}',
            f'--soc_version={self.soc_version}',
            '--log=error',
        ]
        
        # æ·»åŠ è¾“å…¥shape
        if input_shape:
            cmd.append(f'--input_shape={input_shape}')
            cmd.append('--input_format=ND')
        
        # æ·»åŠ åŠ¨æ€ç»´åº¦
        if dynamic_dims:
            cmd.append(f'--dynamic_dims={dynamic_dims}')
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        for key, value in kwargs.items():
            if value is not None:
                cmd.append(f'--{key}={value}')
        
        print(f"ğŸ”„ å¼€å§‹è½¬æ¢: {onnx_path} -> {output_path}.om")
        print(f"ğŸ“ å‘½ä»¤: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=True
            )
            
            print(f"âœ… è½¬æ¢æˆåŠŸ!")
            print(f"   è¾“å‡º: {output_path}.om")
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            om_file = f"{output_path}.om"
            if os.path.exists(om_file):
                size_mb = os.path.getsize(om_file) / 1024 / 1024
                print(f"   å¤§å°: {size_mb:.2f}MB")
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ è½¬æ¢å¤±è´¥!")
            print(f"é”™è¯¯è¾“å‡º:\n{e.stderr}")
            return False
    
    def convert_pytorch_to_onnx(
        self,
        model,
        dummy_input: dict,
        output_path: str,
        input_names: list,
        output_names: list,
        dynamic_axes: dict = None,
        opset_version: int = 11
    ) -> bool:
        """
        è½¬æ¢PyTorchæ¨¡å‹ä¸ºONNX
        
        Args:
            model: PyTorchæ¨¡å‹
            dummy_input: ç¤ºä¾‹è¾“å…¥
            output_path: è¾“å‡ºONNXè·¯å¾„
            input_names: è¾“å…¥åç§°åˆ—è¡¨
            output_names: è¾“å‡ºåç§°åˆ—è¡¨
            dynamic_axes: åŠ¨æ€ç»´åº¦å®šä¹‰
            opset_version: ONNX opsetç‰ˆæœ¬
            
        Returns:
            è½¬æ¢æ˜¯å¦æˆåŠŸ
        """
        import torch
        
        model.eval()
        
        print(f"ğŸ”„ å¯¼å‡ºONNXæ¨¡å‹: {output_path}")
        
        try:
            torch.onnx.export(
                model,
                dummy_input,
                output_path,
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=input_names,
                output_names=output_names,
                dynamic_axes=dynamic_axes
            )
            
            print(f"âœ… ONNXå¯¼å‡ºæˆåŠŸ")
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(output_path)
            onnx.checker.check_model(onnx_model)
            print(f"âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
            return False
    
    def convert_clip_model(
        self,
        model_path: str,
        output_dir: str,
        batch_size: int = 1,
        image_size: int = 224,
        text_length: int = 77,
        dynamic_batch: bool = False
    ) -> dict:
        """
        è½¬æ¢CLIPæ¨¡å‹ä¸ºOMæ ¼å¼
        
        Args:
            model_path: HuggingFaceæ¨¡å‹è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            batch_size: æ‰¹å¤§å°
            image_size: å›¾åƒå¤§å°
            text_length: æ–‡æœ¬é•¿åº¦
            dynamic_batch: æ˜¯å¦æ”¯æŒåŠ¨æ€batch
            
        Returns:
            è½¬æ¢ç»“æœä¿¡æ¯
        """
        from transformers import CLIPModel
        import torch
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
        model = CLIPModel.from_pretrained(model_path)
        model.eval()
        
        # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
        dummy_input = {
            'input_ids': torch.randint(0, 1000, (batch_size, text_length)),
            'pixel_values': torch.randn(batch_size, 3, image_size, image_size),
            'attention_mask': torch.ones(batch_size, text_length, dtype=torch.long)
        }
        
        # å¯¼å‡ºONNX
        onnx_path = str(output_dir / "clip_model.onnx")
        
        dynamic_axes = None
        if dynamic_batch:
            dynamic_axes = {
                'input_ids': {0: 'batch_size'},
                'pixel_values': {0: 'batch_size'},
                'attention_mask': {0: 'batch_size'},
                'logits_per_image': {0: 'batch_size'},
                'logits_per_text': {0: 'batch_size'}
            }
        
        success = self.convert_pytorch_to_onnx(
            model=model,
            dummy_input=(dummy_input,),
            output_path=onnx_path,
            input_names=['input_ids', 'pixel_values', 'attention_mask'],
            output_names=['logits_per_image', 'logits_per_text'],
            dynamic_axes=dynamic_axes,
            opset_version=11
        )
        
        if not success:
            return {'success': False, 'error': 'ONNX export failed'}
        
        # è½¬æ¢ä¸ºOM
        om_path = str(output_dir / "clip_model")
        
        if dynamic_batch:
            input_shape = f"input_ids:-1,{text_length};pixel_values:-1,3,{image_size},{image_size};attention_mask:-1,{text_length}"
            dynamic_dims = "1;2;4;8"
        else:
            input_shape = f"input_ids:{batch_size},{text_length};pixel_values:{batch_size},3,{image_size},{image_size};attention_mask:{batch_size},{text_length}"
            dynamic_dims = None
        
        success = self.convert_onnx_to_om(
            onnx_path=onnx_path,
            output_path=om_path,
            input_shape=input_shape,
            dynamic_dims=dynamic_dims
        )
        
        result = {
            'success': success,
            'onnx_path': onnx_path,
            'om_path': f"{om_path}.om" if success else None,
            'config': {
                'batch_size': batch_size,
                'image_size': image_size,
                'text_length': text_length,
                'dynamic_batch': dynamic_batch,
                'soc_version': self.soc_version
            }
        }
        
        # ä¿å­˜é…ç½®
        if success:
            config_path = output_dir / "model_config.json"
            with open(config_path, 'w') as f:
                json.dump(result['config'], f, indent=2)
            print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜: {config_path}")
        
        return result


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='PyTorchæ¨¡å‹è½¬æ¢ä¸ºæ˜‡è…¾OMæ ¼å¼')
    
    subparsers = parser.add_subparsers(dest='command', help='è½¬æ¢å‘½ä»¤')
    
    # ONNX to OM
    onnx_parser = subparsers.add_parser('onnx', help='è½¬æ¢ONNXæ¨¡å‹ä¸ºOM')
    onnx_parser.add_argument('--model', type=str, required=True, help='ONNXæ¨¡å‹è·¯å¾„')
    onnx_parser.add_argument('--output', type=str, required=True, help='è¾“å‡ºOMæ¨¡å‹è·¯å¾„')
    onnx_parser.add_argument('--input-shape', type=str, help='è¾“å…¥shape')
    onnx_parser.add_argument('--dynamic-dims', type=str, help='åŠ¨æ€ç»´åº¦')
    onnx_parser.add_argument('--soc-version', type=str, default='Ascend910',
                            choices=ModelConverter.SUPPORTED_SOC, help='ç›®æ ‡èŠ¯ç‰‡')
    
    # CLIP model
    clip_parser = subparsers.add_parser('clip', help='è½¬æ¢CLIPæ¨¡å‹')
    clip_parser.add_argument('--model', type=str, default='openai/clip-vit-base-patch32',
                            help='HuggingFaceæ¨¡å‹è·¯å¾„')
    clip_parser.add_argument('--output-dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    clip_parser.add_argument('--batch-size', type=int, default=1, help='æ‰¹å¤§å°')
    clip_parser.add_argument('--image-size', type=int, default=224, help='å›¾åƒå¤§å°')
    clip_parser.add_argument('--text-length', type=int, default=77, help='æ–‡æœ¬é•¿åº¦')
    clip_parser.add_argument('--dynamic-batch', action='store_true', help='åŠ¨æ€batch')
    clip_parser.add_argument('--soc-version', type=str, default='Ascend910',
                            choices=ModelConverter.SUPPORTED_SOC, help='ç›®æ ‡èŠ¯ç‰‡')
    
    args = parser.parse_args()
    
    if args.command == 'onnx':
        converter = ModelConverter(soc_version=args.soc_version)
        converter.convert_onnx_to_om(
            onnx_path=args.model,
            output_path=args.output,
            input_shape=args.input_shape,
            dynamic_dims=args.dynamic_dims
        )
    
    elif args.command == 'clip':
        converter = ModelConverter(soc_version=args.soc_version)
        result = converter.convert_clip_model(
            model_path=args.model,
            output_dir=args.output_dir,
            batch_size=args.batch_size,
            image_size=args.image_size,
            text_length=args.text_length,
            dynamic_batch=args.dynamic_batch
        )
        
        if result['success']:
            print(f"\nâœ… è½¬æ¢æˆåŠŸ!")
            print(f"   OMæ¨¡å‹: {result['om_path']}")
        else:
            print(f"\nâŒ è½¬æ¢å¤±è´¥")
            if 'error' in result:
                print(f"   é”™è¯¯: {result['error']}")
    
    else:
        parser.print_help()


if __name__ == '__main__':
    main()

