#!/bin/bash

###############################################################################
# åä¸ºæ˜‡è…¾è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬
#
# åŠŸèƒ½ï¼š
# - æ£€æŸ¥ç¯å¢ƒä¾èµ–
# - ä¸‹è½½å’Œè½¬æ¢æ¨¡å‹
# - è¿è¡Œæ€§èƒ½æµ‹è¯•
# - ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š
###############################################################################

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é»˜è®¤å‚æ•°
MODEL="openai/clip-vit-base-patch32"
OUTPUT_DIR="./deployed_models"
SOC_VERSION="Ascend910"
BATCH_SIZE=1
DYNAMIC_BATCH=false
RUN_BENCHMARK=true
NUM_RUNS=100

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

function print_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

function print_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

function print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

function print_header() {
    echo ""
    echo "============================================================"
    echo "$1"
    echo "============================================================"
}

# è§£æå‘½ä»¤è¡Œå‚æ•°
while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL="$2"
            shift 2
            ;;
        --output-dir)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        --soc-version)
            SOC_VERSION="$2"
            shift 2
            ;;
        --batch-size)
            BATCH_SIZE="$2"
            shift 2
            ;;
        --dynamic-batch)
            DYNAMIC_BATCH=true
            shift
            ;;
        --no-benchmark)
            RUN_BENCHMARK=false
            shift
            ;;
        --num-runs)
            NUM_RUNS="$2"
            shift 2
            ;;
        -h|--help)
            echo "åä¸ºæ˜‡è…¾è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬"
            echo ""
            echo "ç”¨æ³•: $0 [é€‰é¡¹]"
            echo ""
            echo "é€‰é¡¹:"
            echo "  --model MODEL              æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: openai/clip-vit-base-patch32ï¼‰"
            echo "  --output-dir DIR           è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: ./deployed_modelsï¼‰"
            echo "  --soc-version VERSION      ç›®æ ‡èŠ¯ç‰‡ï¼ˆé»˜è®¤: Ascend910ï¼‰"
            echo "  --batch-size N             æ‰¹å¤§å°ï¼ˆé»˜è®¤: 1ï¼‰"
            echo "  --dynamic-batch            å¯ç”¨åŠ¨æ€batch"
            echo "  --no-benchmark             è·³è¿‡æ€§èƒ½æµ‹è¯•"
            echo "  --num-runs N               æµ‹è¯•è¿è¡Œæ¬¡æ•°ï¼ˆé»˜è®¤: 100ï¼‰"
            echo "  -h, --help                 æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"
            echo ""
            exit 0
            ;;
        *)
            print_error "æœªçŸ¥å‚æ•°: $1"
            exit 1
            ;;
    esac
done

print_header "åä¸ºæ˜‡è…¾æ¨¡å‹éƒ¨ç½²è„šæœ¬"
print_info "æ¨¡å‹: $MODEL"
print_info "è¾“å‡ºç›®å½•: $OUTPUT_DIR"
print_info "ç›®æ ‡èŠ¯ç‰‡: $SOC_VERSION"
print_info "æ‰¹å¤§å°: $BATCH_SIZE"
print_info "åŠ¨æ€batch: $DYNAMIC_BATCH"

# 1. æ£€æŸ¥ç¯å¢ƒ
print_header "æ­¥éª¤ 1/5: æ£€æŸ¥ç¯å¢ƒ"

# æ£€æŸ¥Python
if ! command -v python &> /dev/null; then
    print_error "Pythonæœªå®‰è£…"
    exit 1
fi
print_info "âœ“ Python: $(python --version)"

# æ£€æŸ¥NPU
print_info "æ£€æŸ¥NPUè®¾å¤‡..."
if command -v npu-smi &> /dev/null; then
    npu-smi info
    print_info "âœ“ NPUè®¾å¤‡å¯ç”¨"
else
    print_warn "npu-smiæœªæ‰¾åˆ°ï¼Œè¯·ç¡®è®¤CANNå·²æ­£ç¡®å®‰è£…"
fi

# æ£€æŸ¥torch_npu
print_info "æ£€æŸ¥torch_npu..."
if python -c "import torch_npu" 2>/dev/null; then
    print_info "âœ“ torch_npuå·²å®‰è£…"
    python -c "import torch; import torch_npu; print(f'NPUå¯ç”¨: {torch.npu.is_available()}')"
else
    print_error "torch_npuæœªå®‰è£…æˆ–æ— æ³•å¯¼å…¥"
    print_info "å®‰è£…æ–¹æ³•:"
    print_info "  pip install torch-npu==1.11.0 -i https://repo.huaweicloud.com/repository/pypi/simple"
    exit 1
fi

# æ£€æŸ¥ATCï¼ˆç”¨äºæ¨¡å‹è½¬æ¢ï¼‰
if command -v atc &> /dev/null; then
    print_info "âœ“ ATCå·¥å…·å¯ç”¨"
else
    print_warn "ATCå·¥å…·æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡OMè½¬æ¢"
    print_info "è¯·è®¾ç½®CANNç¯å¢ƒï¼š"
    print_info "  source /usr/local/Ascend/ascend-toolkit/set_env.sh"
fi

# 2. åˆ›å»ºè¾“å‡ºç›®å½•
print_header "æ­¥éª¤ 2/5: åˆ›å»ºè¾“å‡ºç›®å½•"
mkdir -p "$OUTPUT_DIR"
print_info "è¾“å‡ºç›®å½•: $OUTPUT_DIR"

# 3. ä¸‹è½½æµ‹è¯•å›¾åƒï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
print_header "æ­¥éª¤ 3/5: å‡†å¤‡æµ‹è¯•æ•°æ®"
TEST_IMAGE="$OUTPUT_DIR/test_image.jpg"

if [ ! -f "$TEST_IMAGE" ]; then
    print_info "ç”Ÿæˆæµ‹è¯•å›¾åƒ..."
    python - <<EOF
from PIL import Image, ImageDraw
import numpy as np

# åˆ›å»ºæµ‹è¯•å›¾åƒ
img = Image.new('RGB', (400, 300))
pixels = img.load()

for i in range(300):
    for j in range(400):
        r = int(100 + 155 * (j / 400))
        g = int(150 + 105 * (i / 300))
        b = int(200 - 100 * ((i + j) / 700))
        pixels[j, i] = (r, g, b)

draw = ImageDraw.Draw(img)
draw.rectangle([100, 100, 300, 200], outline='blue', width=3)
draw.ellipse([180, 130, 220, 170], outline='red', width=3)

img.save("$TEST_IMAGE")
print(f"âœ“ æµ‹è¯•å›¾åƒå·²ä¿å­˜: $TEST_IMAGE")
EOF
else
    print_info "ä½¿ç”¨ç°æœ‰æµ‹è¯•å›¾åƒ: $TEST_IMAGE"
fi

# 4. è¿è¡ŒPyTorch-NPUæ¨ç†
print_header "æ­¥éª¤ 4/5: PyTorch-NPUæ¨ç†æµ‹è¯•"

print_info "è¿è¡Œå•æ¬¡æ¨ç†..."
python pytorch_npu_inference.py \
    --model "$MODEL" \
    --image "$TEST_IMAGE" \
    --texts "a colorful abstract pattern" "a geometric shape" "a gradient background" \
    --device auto \
    --fp16

# 5. æ€§èƒ½æµ‹è¯•
if [ "$RUN_BENCHMARK" = true ]; then
    print_header "æ­¥éª¤ 5/5: æ€§èƒ½åŸºå‡†æµ‹è¯•"
    
    BENCHMARK_OUTPUT="$OUTPUT_DIR/benchmark_results.json"
    
    print_info "è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼ˆ$NUM_RUNS æ¬¡è¿­ä»£ï¼‰..."
    python benchmark.py \
        --model "$MODEL" \
        --image "$TEST_IMAGE" \
        --texts "a colorful pattern" "a geometric shape" "a gradient" \
        --num-runs "$NUM_RUNS" \
        --output "$BENCHMARK_OUTPUT"
    
    print_info "âœ“ æ€§èƒ½æµ‹è¯•å®Œæˆ"
    print_info "ç»“æœå·²ä¿å­˜: $BENCHMARK_OUTPUT"
else
    print_info "è·³è¿‡æ€§èƒ½æµ‹è¯•"
fi

# 6. ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š
print_header "éƒ¨ç½²å®Œæˆï¼"

cat << EOF

ğŸ“¦ éƒ¨ç½²æ‘˜è¦
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ è¾“å‡ºç›®å½•: $OUTPUT_DIR
ğŸ“„ æµ‹è¯•å›¾åƒ: $TEST_IMAGE

ğŸ”§ é…ç½®
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  æ¨¡å‹: $MODEL
  ç›®æ ‡èŠ¯ç‰‡: $SOC_VERSION
  æ‰¹å¤§å°: $BATCH_SIZE
  åŠ¨æ€batch: $DYNAMIC_BATCH

ğŸ“Š æ–‡ä»¶æ¸…å•
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
EOF

ls -lh "$OUTPUT_DIR" | tail -n +2

cat << EOF

âœ… ä¸‹ä¸€æ­¥
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. æŸ¥çœ‹æ€§èƒ½æµ‹è¯•ç»“æœ:
   cat $OUTPUT_DIR/benchmark_results.json

2. ä½¿ç”¨Python APIè¿›è¡Œæ¨ç†:
   from pytorch_npu_inference import CLIPInferenceService
   service = CLIPInferenceService(model_path="$MODEL", device="npu")
   result = service.predict("image.jpg", ["text1", "text2"])

3. è½¬æ¢ä¸ºOMæ ¼å¼ä»¥è·å¾—æ›´å¥½æ€§èƒ½:
   python convert_to_om.py clip \\
       --model $MODEL \\
       --output-dir $OUTPUT_DIR/om \\
       --soc-version $SOC_VERSION

4. é˜…è¯»å®Œæ•´æ–‡æ¡£:
   ../../../docs/04-å¤šå¹³å°éƒ¨ç½²/03-åä¸ºæ˜‡è…¾éƒ¨ç½².md

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
EOF

print_info "éƒ¨ç½²è„šæœ¬æ‰§è¡Œå®Œæˆï¼"

