"""
PyTorch-NPUæ¨ç†æœåŠ¡

æä¾›åŸºäºåä¸ºæ˜‡è…¾NPUçš„CLIPæ¨¡å‹æ¨ç†æœåŠ¡
æ”¯æŒè‡ªåŠ¨è®¾å¤‡é€‰æ‹©ï¼ˆNPU/CUDA/CPUï¼‰
"""

import torch
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
from typing import List, Dict, Union, Tuple, Optional
import time
from pathlib import Path
import warnings

# å°è¯•å¯¼å…¥torch_npu
try:
    import torch_npu
    NPU_AVAILABLE = True
except ImportError:
    NPU_AVAILABLE = False
    warnings.warn("torch_npu not available, NPU will not be used")


class CLIPInferenceService:
    """
    CLIPæ¨ç†æœåŠ¡ï¼ˆæ˜‡è…¾NPUé€‚é…ç‰ˆï¼‰
    
    æ”¯æŒå›¾æ–‡åŒ¹é…ã€å›¾åƒç‰¹å¾æå–ã€æ–‡æœ¬ç‰¹å¾æå–
    è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡ï¼šNPU > CUDA > CPU
    """
    
    def __init__(
        self,
        model_path: str,
        device: str = "auto",
        use_fp16: bool = False
    ):
        """
        åˆå§‹åŒ–æ¨ç†æœåŠ¡
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡ ("auto", "npu", "cuda", "cpu")
            use_fp16: æ˜¯å¦ä½¿ç”¨FP16æ··åˆç²¾åº¦
        """
        self.device = self._get_device(device)
        self.use_fp16 = use_fp16 and self.device.type in ['npu', 'cuda']
        
        print(f"ğŸš€ åˆå§‹åŒ–CLIPæ¨ç†æœåŠ¡ï¼ˆæ˜‡è…¾é€‚é…ç‰ˆï¼‰...")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   FP16: {self.use_fp16}")
        print(f"   NPUå¯ç”¨: {NPU_AVAILABLE and torch.npu.is_available()}")
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        self.model = CLIPModel.from_pretrained(model_path)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        # è½¬æ¢ä¸ºFP16
        if self.use_fp16:
            self.model = self.model.half()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _get_device(self, device: str) -> torch.device:
        """
        æ™ºèƒ½é€‰æ‹©è®¾å¤‡
        
        Args:
            device: è®¾å¤‡å­—ç¬¦ä¸²
            
        Returns:
            torch.deviceå¯¹è±¡
        """
        if device == "auto":
            # ä¼˜å…ˆçº§ï¼šNPU > CUDA > CPU
            if NPU_AVAILABLE and torch.npu.is_available():
                return torch.device("npu:0")
            elif torch.cuda.is_available():
                return torch.device("cuda:0")
            else:
                return torch.device("cpu")
        elif device.startswith("npu"):
            if not NPU_AVAILABLE:
                raise RuntimeError("torch_npu not installed")
            if not torch.npu.is_available():
                raise RuntimeError("NPU not available")
            return torch.device(device)
        else:
            return torch.device(device)
    
    def predict(
        self,
        image: Union[str, Image.Image],
        texts: List[str]
    ) -> Dict[str, any]:
        """
        å›¾æ–‡åŒ¹é…æ¨ç†
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            texts: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        # åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=texts,
            images=image,
            return_tensors="pt",
            padding=True
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # è½¬æ¢ä¸ºFP16
        if self.use_fp16:
            if 'pixel_values' in inputs:
                inputs['pixel_values'] = inputs['pixel_values'].half()
        
        # æ¨ç†
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1)
        
        # åŒæ­¥è®¾å¤‡ï¼ˆå¯¹äºNPUå¾ˆé‡è¦ï¼‰
        if self.device.type == 'npu':
            torch.npu.synchronize()
        elif self.device.type == 'cuda':
            torch.cuda.synchronize()
        
        inference_time = time.time() - start_time
        
        # ç»“æœå¤„ç†
        probs_cpu = probs.cpu().numpy()
        
        results = {
            'texts': texts,
            'probabilities': probs_cpu[0].tolist(),
            'best_match': texts[probs_cpu[0].argmax()],
            'best_score': float(probs_cpu[0].max()),
            'inference_time_ms': inference_time * 1000,
            'device': str(self.device)
        }
        
        return results
    
    def extract_image_features(
        self,
        images: Union[List[str], List[Image.Image]],
        normalize: bool = True
    ) -> torch.Tensor:
        """
        æå–å›¾åƒç‰¹å¾
        
        Args:
            images: å›¾åƒåˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾
            
        Returns:
            å›¾åƒç‰¹å¾å¼ é‡ [batch_size, feature_dim]
        """
        # åŠ è½½å›¾åƒ
        pil_images = []
        for img in images:
            if isinstance(img, str):
                pil_images.append(Image.open(img).convert('RGB'))
            else:
                pil_images.append(img)
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            images=pil_images,
            return_tensors="pt"
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs['pixel_values'] = inputs['pixel_values'].half()
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            image_features = self.model.get_image_features(**inputs)
            
            if normalize:
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        return image_features
    
    def extract_text_features(
        self,
        texts: List[str],
        normalize: bool = True
    ) -> torch.Tensor:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾
            
        Returns:
            æ–‡æœ¬ç‰¹å¾å¼ é‡ [batch_size, feature_dim]
        """
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=texts,
            return_tensors="pt",
            padding=True
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            text_features = self.model.get_text_features(**inputs)
            
            if normalize:
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        return text_features
    
    def batch_predict(
        self,
        images: List[Union[str, Image.Image]],
        texts: List[str],
        batch_size: int = 4
    ) -> List[Dict]:
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            images: å›¾åƒåˆ—è¡¨
            texts: æ–‡æœ¬åˆ—è¡¨ï¼ˆå¯¹æ‰€æœ‰å›¾åƒé€šç”¨ï¼‰
            batch_size: æ‰¹å¤§å°
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i+batch_size]
            
            for img in batch_images:
                result = self.predict(img, texts)
                results.append(result)
        
        return results
    
    def benchmark(
        self,
        image: Union[str, Image.Image],
        texts: List[str],
        num_runs: int = 100,
        warmup_runs: int = 10
    ) -> Dict[str, float]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            image: æµ‹è¯•å›¾åƒ
            texts: æµ‹è¯•æ–‡æœ¬
            num_runs: æµ‹è¯•æ¬¡æ•°
            warmup_runs: é¢„çƒ­æ¬¡æ•°
            
        Returns:
            æ€§èƒ½ç»Ÿè®¡
        """
        print(f"ğŸ”¥ é¢„çƒ­ä¸­... ({warmup_runs}æ¬¡)")
        for _ in range(warmup_runs):
            self.predict(image, texts)
        
        print(f"â±ï¸  å¼€å§‹åŸºå‡†æµ‹è¯•... ({num_runs}æ¬¡)")
        times = []
        
        for i in range(num_runs):
            result = self.predict(image, texts)
            times.append(result['inference_time_ms'])
            
            if (i + 1) % 20 == 0:
                print(f"   è¿›åº¦: {i + 1}/{num_runs}")
        
        import numpy as np
        times = np.array(times)
        
        stats = {
            'mean_ms': float(times.mean()),
            'std_ms': float(times.std()),
            'min_ms': float(times.min()),
            'max_ms': float(times.max()),
            'p50_ms': float(np.percentile(times, 50)),
            'p95_ms': float(np.percentile(times, 95)),
            'p99_ms': float(np.percentile(times, 99)),
            'throughput_per_sec': 1000.0 / times.mean(),
            'device': str(self.device)
        }
        
        # æ˜¾å­˜ç»Ÿè®¡
        if self.device.type == 'npu' and NPU_AVAILABLE:
            stats['memory_allocated_mb'] = torch.npu.memory_allocated() / 1024 / 1024
            stats['memory_reserved_mb'] = torch.npu.memory_reserved() / 1024 / 1024
        elif self.device.type == 'cuda':
            stats['memory_allocated_mb'] = torch.cuda.memory_allocated() / 1024 / 1024
            stats['memory_reserved_mb'] = torch.cuda.memory_reserved() / 1024 / 1024
        
        return stats
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'device'):
            if self.device.type == 'npu' and NPU_AVAILABLE:
                torch.npu.empty_cache()
            elif self.device.type == 'cuda':
                torch.cuda.empty_cache()


def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    import argparse
    
    parser = argparse.ArgumentParser(description='CLIPæ¨ç†æœåŠ¡ï¼ˆæ˜‡è…¾NPUé€‚é…ï¼‰')
    parser.add_argument('--model', type=str, default='openai/clip-vit-base-patch32',
                        help='æ¨¡å‹è·¯å¾„æˆ–åç§°')
    parser.add_argument('--image', type=str, required=True,
                        help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--texts', type=str, nargs='+',
                        default=['a photo of a cat', 'a photo of a dog'],
                        help='å€™é€‰æ–‡æœ¬')
    parser.add_argument('--device', type=str, default='auto',
                        choices=['auto', 'npu', 'cuda', 'cpu'],
                        help='è®¾å¤‡é€‰æ‹©')
    parser.add_argument('--fp16', action='store_true',
                        help='ä½¿ç”¨FP16')
    parser.add_argument('--benchmark', action='store_true',
                        help='è¿è¡ŒåŸºå‡†æµ‹è¯•')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æœåŠ¡
    service = CLIPInferenceService(
        model_path=args.model,
        device=args.device,
        use_fp16=args.fp16
    )
    
    if args.benchmark:
        # åŸºå‡†æµ‹è¯•
        stats = service.benchmark(args.image, args.texts, num_runs=100)
        
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"   å¹³å‡å»¶è¿Ÿ: {stats['mean_ms']:.2f}ms (Â±{stats['std_ms']:.2f}ms)")
        print(f"   P50: {stats['p50_ms']:.2f}ms")
        print(f"   P95: {stats['p95_ms']:.2f}ms")
        print(f"   P99: {stats['p99_ms']:.2f}ms)")
        print(f"   ååé‡: {stats['throughput_per_sec']:.2f} images/sec")
        if 'memory_allocated_mb' in stats:
            print(f"   æ˜¾å­˜å ç”¨: {stats['memory_allocated_mb']:.2f}MB")
    else:
        # å•æ¬¡æ¨ç†
        result = service.predict(args.image, args.texts)
        
        print(f"\nğŸ“ æ¨ç†ç»“æœ:")
        for text, prob in zip(result['texts'], result['probabilities']):
            print(f"   {text}: {prob:.4f}")
        print(f"\nğŸ† æœ€ä½³åŒ¹é…: {result['best_match']} ({result['best_score']:.4f})")
        print(f"â±ï¸  æ¨ç†æ—¶é—´: {result['inference_time_ms']:.2f}ms")
        print(f"ğŸ’» è®¾å¤‡: {result['device']}")


if __name__ == '__main__':
    main()

