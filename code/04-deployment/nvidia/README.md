# NVIDIAå¹³å°éƒ¨ç½²ä»£ç 

CLIPæ¨¡å‹åœ¨NVIDIA GPUä¸Šçš„éƒ¨ç½²å®ç°ï¼ŒåŒ…æ‹¬PyTorchæ¨ç†ã€ONNXè½¬æ¢å’ŒAPIæœåŠ¡ã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
nvidia/
â”œâ”€â”€ basic/
â”‚   â””â”€â”€ pytorch_inference.py    # PyTorchæ¨ç†æœåŠ¡
â”œâ”€â”€ onnx/
â”‚   â”œâ”€â”€ convert_to_onnx.py      # ONNXè½¬æ¢è„šæœ¬
â”‚   â””â”€â”€ onnx_inference.py       # ONNXæ¨ç†æœåŠ¡
â””â”€â”€ README.md                    # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. PyTorchæ¨ç†

```bash
# å›¾æ–‡åŒ¹é…æ¨ç†
python code/04-deployment/nvidia/basic/pytorch_inference.py \
    --model openai/clip-vit-base-patch32 \
    --image dog.jpg \
    --texts "a photo of a dog" "a photo of a cat" "a photo of a bird" \
    --device cuda \
    --fp16
```

**è¾“å‡ºç¤ºä¾‹**:
```
ğŸš€ åˆå§‹åŒ–CLIPæ¨ç†æœåŠ¡...
   è®¾å¤‡: cuda
   FP16: True
âœ… æ¨¡å‹åŠ è½½å®Œæˆ: openai/clip-vit-base-patch32
ğŸ”¥ é¢„çƒ­æ¨¡å‹...
âœ… é¢„çƒ­å®Œæˆ

ğŸ–¼ï¸  å›¾åƒ: dog.jpg
ğŸ“ å€™é€‰æ–‡æœ¬: ['a photo of a dog', 'a photo of a cat', 'a photo of a bird']
============================================================

â±ï¸  æ¨ç†æ—¶é—´: 12.34ms
ğŸ–¥ï¸  è®¾å¤‡: cuda
ğŸ”¢ FP16: True

ğŸ“Š é¢„æµ‹ç»“æœ:
  1. a photo of a dog
     å¾—åˆ†: 0.9234
  2. a photo of a cat
     å¾—åˆ†: 0.0543
  3. a photo of a bird
     å¾—åˆ†: 0.0223
```

### 2. ONNXè½¬æ¢

```bash
# è½¬æ¢CLIPæ¨¡å‹ä¸ºONNX
python code/04-deployment/nvidia/onnx/convert_to_onnx.py \
    --model openai/clip-vit-base-patch32 \
    --output_dir onnx_models \
    --optimize
```

**è¾“å‡ºæ–‡ä»¶**:
- `onnx_models/clip_vision.onnx` - è§†è§‰ç¼–ç å™¨
- `onnx_models/clip_text.onnx` - æ–‡æœ¬ç¼–ç å™¨
- `onnx_models/clip_vision_optimized.onnx` - ä¼˜åŒ–åçš„è§†è§‰ç¼–ç å™¨
- `onnx_models/clip_text_optimized.onnx` - ä¼˜åŒ–åçš„æ–‡æœ¬ç¼–ç å™¨

### 3. ONNXæ¨ç†

```bash
# ä½¿ç”¨ONNXæ¨¡å‹æ¨ç†
python code/04-deployment/nvidia/onnx/onnx_inference.py \
    --vision_model onnx_models/clip_vision.onnx \
    --text_model onnx_models/clip_text.onnx \
    --image dog.jpg \
    --texts "a photo of a dog" "a photo of a cat"
```

### 4. APIæœåŠ¡

```bash
# å¯åŠ¨FastAPIæœåŠ¡
cd code/04-deployment/api-server
python app.py

# æˆ–ä½¿ç”¨uvicorn
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

**æµ‹è¯•API**:
```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# å›¾æ–‡åŒ¹é…æ¨ç†
curl -X POST "http://localhost:8000/predict" \
  -F "image=@dog.jpg" \
  -F "texts=a photo of a dog,a photo of a cat"

# æå–å›¾åƒç‰¹å¾
curl -X POST "http://localhost:8000/image_features" \
  -F "image=@dog.jpg" \
  -F "normalize=true"
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | å»¶è¿Ÿ (ms) | ååé‡ (img/s) | æ˜¾å­˜ (GB) |
|------|-----------|---------------|----------|
| PyTorch FP32 | 20 | 50 | 2.5 |
| PyTorch FP16 | 12 | 80 | 1.3 |
| ONNX Runtime (CPU) | 45 | 22 | - |
| ONNX Runtime (GPU) | 15 | 65 | 2.0 |

*æµ‹è¯•ç¯å¢ƒ: NVIDIA RTX 3090, Batch Size=1*

## ğŸ”§ é«˜çº§ç”¨æ³•

### æ‰¹é‡æ¨ç†

```python
from code.deployment.nvidia.basic.pytorch_inference import CLIPInferenceService

service = CLIPInferenceService("openai/clip-vit-base-patch32")

# æ‰¹é‡æå–å›¾åƒç‰¹å¾
image_paths = ["img1.jpg", "img2.jpg", "img3.jpg"]
features = service.get_image_features(image_paths)
print(f"ç‰¹å¾å½¢çŠ¶: {features.shape}")  # (3, 512)
```

### ç‰¹å¾æå–å’Œç›¸ä¼¼åº¦è®¡ç®—

```python
# æå–ç‰¹å¾
image_features = service.get_image_features(["dog.jpg"])
text_features = service.get_text_features(["a dog", "a cat"])

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = service.compute_similarity(image_features, text_features)
print(f"ç›¸ä¼¼åº¦çŸ©é˜µ: {similarity}")
```

### è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„

```python
# ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
service = CLIPInferenceService(
    model_path="outputs/lora_finetuning/checkpoint-epoch-10",
    device="cuda",
    use_fp16=True
)
```

## ğŸ“¦ ä¾èµ–å®‰è£…

```bash
# åŸºç¡€ä¾èµ–
pip install torch torchvision transformers pillow

# ONNXç›¸å…³
pip install onnx onnxruntime-gpu

# APIæœåŠ¡
pip install fastapi uvicorn python-multipart

# å¯é€‰ï¼šONNXä¼˜åŒ–
pip install onnxruntime-tools
```

## ğŸ³ Dockeréƒ¨ç½²

### æ„å»ºé•œåƒ

```bash
cd code/04-deployment
docker build -t clip-service:latest -f docker/Dockerfile .
```

### è¿è¡Œå®¹å™¨

```bash
# GPUæ”¯æŒ
docker run --gpus all -p 8000:8000 clip-service:latest

# CPU only
docker run -p 8000:8000 clip-service:latest
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„éƒ¨ç½²æ–¹æ¡ˆ

- **å¼€å‘/åŸå‹**: PyTorchç›´æ¥æ¨ç†
- **ç”Ÿäº§ç¯å¢ƒ**: ONNX Runtimeæˆ–TorchScript
- **é«˜æ€§èƒ½éœ€æ±‚**: TensorRTï¼ˆå¾…å®ç°ï¼‰
- **ä¼ä¸šçº§**: Tritonæ¨ç†æœåŠ¡å™¨ï¼ˆå¾…å®ç°ï¼‰

### 2. æ€§èƒ½ä¼˜åŒ–

- âœ… ä½¿ç”¨FP16æ··åˆç²¾åº¦
- âœ… å¯ç”¨æ‰¹å¤„ç†
- âœ… é¢„çƒ­æ¨¡å‹
- âœ… ç¼“å­˜æ–‡æœ¬ç‰¹å¾
- âœ… ä½¿ç”¨ONNXä¼˜åŒ–

### 3. ç”Ÿäº§éƒ¨ç½²

- âœ… ä½¿ç”¨Gunicorn + Uvicornï¼ˆå¤šworkerï¼‰
- âœ… æ·»åŠ Nginxåå‘ä»£ç†
- âœ… å®ç°å¥åº·æ£€æŸ¥
- âœ… æ·»åŠ ç›‘æ§å’Œæ—¥å¿—
- âœ… ä½¿ç”¨Dockerå®¹å™¨åŒ–

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [NVIDIAéƒ¨ç½²åŸºç¡€](../../../docs/04-å¤šå¹³å°éƒ¨ç½²/01-NVIDIAéƒ¨ç½²åŸºç¡€.md)
- [æ¨¡å‹æœåŠ¡åŒ–](../../../docs/04-å¤šå¹³å°éƒ¨ç½²/02-æ¨¡å‹æœåŠ¡åŒ–.md)

## ğŸ¤ è´¡çŒ®

å¦‚æœæ‚¨å‘ç°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿æäº¤Issueæˆ–Pull Requestã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªMITè®¸å¯è¯ã€‚

