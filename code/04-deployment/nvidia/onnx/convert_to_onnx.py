"""
ONNXæ¨¡å‹è½¬æ¢è„šæœ¬

å°†PyTorch CLIPæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
"""

import torch
import onnx
import onnxruntime as ort
from transformers import CLIPModel, CLIPProcessor
import argparse
from pathlib import Path
import numpy as np


def convert_vision_model(
    model_path: str,
    output_path: str,
    opset_version: int = 14,
    dynamic_batch: bool = True
):
    """
    è½¬æ¢CLIPè§†è§‰ç¼–ç å™¨ä¸ºONNX
    
    Args:
        model_path: PyTorchæ¨¡å‹è·¯å¾„
        output_path: ONNXæ¨¡å‹è¾“å‡ºè·¯å¾„
        opset_version: ONNX opsetç‰ˆæœ¬
        dynamic_batch: æ˜¯å¦æ”¯æŒåŠ¨æ€batch size
    """
    print(f"ğŸ”„ è½¬æ¢è§†è§‰ç¼–ç å™¨: {model_path} -> {output_path}")
    
    # åŠ è½½æ¨¡å‹
    model = CLIPModel.from_pretrained(model_path)
    model.eval()
    
    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(1, 3, 224, 224)
    
    # åŠ¨æ€ç»´åº¦é…ç½®
    if dynamic_batch:
        dynamic_axes = {
            'pixel_values': {0: 'batch_size'},
            'pooler_output': {0: 'batch_size'},
            'last_hidden_state': {0: 'batch_size'}
        }
    else:
        dynamic_axes = None
    
    # å¯¼å‡ºONNX
    torch.onnx.export(
        model.vision_model,
        dummy_input,
        output_path,
        input_names=['pixel_values'],
        output_names=['pooler_output', 'last_hidden_state'],
        dynamic_axes=dynamic_axes,
        opset_version=opset_version,
        do_constant_folding=True,
        verbose=False
    )
    
    print(f"âœ… è§†è§‰ç¼–ç å™¨è½¬æ¢å®Œæˆ")
    
    # éªŒè¯æ¨¡å‹
    verify_onnx_model(output_path, dummy_input.numpy())


def convert_text_model(
    model_path: str,
    output_path: str,
    max_length: int = 77,
    opset_version: int = 14,
    dynamic_batch: bool = True
):
    """
    è½¬æ¢CLIPæ–‡æœ¬ç¼–ç å™¨ä¸ºONNX
    
    Args:
        model_path: PyTorchæ¨¡å‹è·¯å¾„
        output_path: ONNXæ¨¡å‹è¾“å‡ºè·¯å¾„
        max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
        opset_version: ONNX opsetç‰ˆæœ¬
        dynamic_batch: æ˜¯å¦æ”¯æŒåŠ¨æ€batch size
    """
    print(f"ğŸ”„ è½¬æ¢æ–‡æœ¬ç¼–ç å™¨: {model_path} -> {output_path}")
    
    # åŠ è½½æ¨¡å‹
    model = CLIPModel.from_pretrained(model_path)
    model.eval()
    
    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    dummy_input_ids = torch.randint(0, 49408, (1, max_length))
    dummy_attention_mask = torch.ones(1, max_length, dtype=torch.long)
    
    # åŠ¨æ€ç»´åº¦é…ç½®
    if dynamic_batch:
        dynamic_axes = {
            'input_ids': {0: 'batch_size'},
            'attention_mask': {0: 'batch_size'},
            'pooler_output': {0: 'batch_size'},
            'last_hidden_state': {0: 'batch_size'}
        }
    else:
        dynamic_axes = None
    
    # å¯¼å‡ºONNX
    torch.onnx.export(
        model.text_model,
        (dummy_input_ids, dummy_attention_mask),
        output_path,
        input_names=['input_ids', 'attention_mask'],
        output_names=['pooler_output', 'last_hidden_state'],
        dynamic_axes=dynamic_axes,
        opset_version=opset_version,
        do_constant_folding=True,
        verbose=False
    )
    
    print(f"âœ… æ–‡æœ¬ç¼–ç å™¨è½¬æ¢å®Œæˆ")
    
    # éªŒè¯æ¨¡å‹
    verify_onnx_model(
        output_path,
        {
            'input_ids': dummy_input_ids.numpy(),
            'attention_mask': dummy_attention_mask.numpy()
        }
    )


def verify_onnx_model(onnx_path: str, dummy_input):
    """
    éªŒè¯ONNXæ¨¡å‹
    
    Args:
        onnx_path: ONNXæ¨¡å‹è·¯å¾„
        dummy_input: ç¤ºä¾‹è¾“å…¥
    """
    print(f"ğŸ” éªŒè¯ONNXæ¨¡å‹: {onnx_path}")
    
    # åŠ è½½å¹¶æ£€æŸ¥æ¨¡å‹
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    print("  âœ“ æ¨¡å‹ç»“æ„éªŒè¯é€šè¿‡")
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"  âœ“ IRç‰ˆæœ¬: {onnx_model.ir_version}")
    print(f"  âœ“ Opsetç‰ˆæœ¬: {onnx_model.opset_import[0].version}")
    
    # æ‰“å°è¾“å…¥è¾“å‡º
    print("  âœ“ è¾“å…¥:")
    for inp in onnx_model.graph.input:
        shape = [d.dim_value if d.dim_value > 0 else 'dynamic' 
                for d in inp.type.tensor_type.shape.dim]
        print(f"      {inp.name}: {shape}")
    
    print("  âœ“ è¾“å‡º:")
    for out in onnx_model.graph.output:
        shape = [d.dim_value if d.dim_value > 0 else 'dynamic' 
                for d in out.type.tensor_type.shape.dim]
        print(f"      {out.name}: {shape}")
    
    # ä½¿ç”¨ONNX Runtimeæµ‹è¯•æ¨ç†
    try:
        session = ort.InferenceSession(
            onnx_path,
            providers=['CPUExecutionProvider']
        )
        
        if isinstance(dummy_input, dict):
            outputs = session.run(None, dummy_input)
        else:
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: dummy_input})
        
        print(f"  âœ“ ONNX Runtimeæ¨ç†æµ‹è¯•é€šè¿‡")
        print(f"  âœ“ è¾“å‡ºå½¢çŠ¶: {[out.shape for out in outputs]}")
    
    except Exception as e:
        print(f"  âœ— ONNX Runtimeæ¨ç†æµ‹è¯•å¤±è´¥: {e}")


def optimize_onnx_model(
    input_path: str,
    output_path: str
):
    """
    ä¼˜åŒ–ONNXæ¨¡å‹
    
    Args:
        input_path: è¾“å…¥ONNXæ¨¡å‹è·¯å¾„
        output_path: ä¼˜åŒ–åçš„è¾“å‡ºè·¯å¾„
    """
    print(f"âš¡ ä¼˜åŒ–ONNXæ¨¡å‹: {input_path} -> {output_path}")
    
    try:
        from onnxruntime.transformers import optimizer
        
        # ä¼˜åŒ–æ¨¡å‹
        optimized_model = optimizer.optimize_model(
            input_path,
            model_type='bert',  # CLIPä½¿ç”¨Transformeræ¶æ„
            num_heads=12,
            hidden_size=768,
        )
        
        # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
        optimized_model.save_model_to_file(output_path)
        
        print(f"âœ… æ¨¡å‹ä¼˜åŒ–å®Œæˆ")
        
        # æ¯”è¾ƒæ¨¡å‹å¤§å°
        import os
        original_size = os.path.getsize(input_path) / (1024 * 1024)
        optimized_size = os.path.getsize(output_path) / (1024 * 1024)
        
        print(f"  åŸå§‹å¤§å°: {original_size:.2f} MB")
        print(f"  ä¼˜åŒ–åå¤§å°: {optimized_size:.2f} MB")
        print(f"  å‹ç¼©æ¯”: {(1 - optimized_size/original_size)*100:.1f}%")
    
    except ImportError:
        print("âš ï¸  æœªå®‰è£…onnxruntime.transformersï¼Œè·³è¿‡ä¼˜åŒ–")
        print("   å®‰è£…å‘½ä»¤: pip install onnxruntime-tools")
    except Exception as e:
        print(f"âš ï¸  ä¼˜åŒ–å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description="CLIPæ¨¡å‹ONNXè½¬æ¢")
    parser.add_argument(
        '--model',
        type=str,
        default='openai/clip-vit-base-patch32',
        help='PyTorchæ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='onnx_models',
        help='ONNXæ¨¡å‹è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--vision_only',
        action='store_true',
        help='åªè½¬æ¢è§†è§‰ç¼–ç å™¨'
    )
    parser.add_argument(
        '--text_only',
        action='store_true',
        help='åªè½¬æ¢æ–‡æœ¬ç¼–ç å™¨'
    )
    parser.add_argument(
        '--opset_version',
        type=int,
        default=14,
        help='ONNX opsetç‰ˆæœ¬'
    )
    parser.add_argument(
        '--optimize',
        action='store_true',
        help='ä¼˜åŒ–ONNXæ¨¡å‹'
    )
    parser.add_argument(
        '--static_batch',
        action='store_true',
        help='ä½¿ç”¨é™æ€batch sizeï¼ˆä¸æ”¯æŒåŠ¨æ€ï¼‰'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("CLIPæ¨¡å‹ONNXè½¬æ¢å·¥å…·")
    print("=" * 60)
    print(f"æ¨¡å‹: {args.model}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"Opsetç‰ˆæœ¬: {args.opset_version}")
    print(f"åŠ¨æ€batch: {not args.static_batch}")
    print("=" * 60)
    
    # è½¬æ¢è§†è§‰ç¼–ç å™¨
    if not args.text_only:
        vision_output = output_dir / "clip_vision.onnx"
        convert_vision_model(
            model_path=args.model,
            output_path=str(vision_output),
            opset_version=args.opset_version,
            dynamic_batch=not args.static_batch
        )
        
        # ä¼˜åŒ–
        if args.optimize:
            vision_optimized = output_dir / "clip_vision_optimized.onnx"
            optimize_onnx_model(str(vision_output), str(vision_optimized))
    
    # è½¬æ¢æ–‡æœ¬ç¼–ç å™¨
    if not args.vision_only:
        text_output = output_dir / "clip_text.onnx"
        convert_text_model(
            model_path=args.model,
            output_path=str(text_output),
            opset_version=args.opset_version,
            dynamic_batch=not args.static_batch
        )
        
        # ä¼˜åŒ–
        if args.optimize:
            text_optimized = output_dir / "clip_text_optimized.onnx"
            optimize_onnx_model(str(text_output), str(text_optimized))
    
    print("\n" + "=" * 60)
    print("âœ… è½¬æ¢å®Œæˆï¼")
    print("=" * 60)
    print(f"\nè¾“å‡ºæ–‡ä»¶ä½äº: {output_dir}")
    print("\nä½¿ç”¨æ–¹å¼:")
    print(f"  python code/04-deployment/nvidia/onnx/onnx_inference.py \\")
    print(f"    --vision_model {output_dir}/clip_vision.onnx \\")
    print(f"    --text_model {output_dir}/clip_text.onnx \\")
    print(f"    --image your_image.jpg \\")
    print(f"    --texts 'text1' 'text2' 'text3'")


if __name__ == '__main__':
    main()

