"""
ONNXæ¨ç†æœåŠ¡

ä½¿ç”¨ONNX Runtimeè¿›è¡ŒCLIPæ¨¡å‹æ¨ç†
"""

import onnxruntime as ort
import numpy as np
from PIL import Image
from typing import List, Dict, Union
import time
from torchvision import transforms


class ONNXCLIPInferenceService:
    """
    åŸºäºONNX Runtimeçš„CLIPæ¨ç†æœåŠ¡
    """
    
    def __init__(
        self,
        vision_model_path: str,
        text_model_path: str = None,
        use_gpu: bool = True
    ):
        """
        åˆå§‹åŒ–ONNXæ¨ç†æœåŠ¡
        
        Args:
            vision_model_path: è§†è§‰ç¼–ç å™¨ONNXæ¨¡å‹è·¯å¾„
            text_model_path: æ–‡æœ¬ç¼–ç å™¨ONNXæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        """
        print(f"ğŸš€ åˆå§‹åŒ–ONNX CLIPæ¨ç†æœåŠ¡...")
        
        # æ£€æŸ¥å¯ç”¨çš„providers
        available_providers = ort.get_available_providers()
        print(f"ğŸ“‹ å¯ç”¨çš„Execution Providers: {available_providers}")
        
        # æ™ºèƒ½é€‰æ‹©providers
        if use_gpu:
            if 'CUDAExecutionProvider' in available_providers:
                providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
                print(f"âœ… ä½¿ç”¨GPUæ¨ç† (CUDA)")
            else:
                providers = ['CPUExecutionProvider']
                print(f"âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨ç†")
                print(f"ğŸ’¡ æç¤º: å®‰è£… onnxruntime-gpu ä»¥å¯ç”¨GPUåŠ é€Ÿ")
                print(f"   pip install onnxruntime-gpu")
        else:
            providers = ['CPUExecutionProvider']
            print(f"âœ… ä½¿ç”¨CPUæ¨ç†")
        
        # åŠ è½½è§†è§‰ç¼–ç å™¨
        print(f"ğŸ“¦ åŠ è½½è§†è§‰ç¼–ç å™¨: {vision_model_path}")
        self.vision_session = ort.InferenceSession(
            vision_model_path,
            providers=providers
        )
        
        # åŠ è½½æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚æœæä¾›ï¼‰
        if text_model_path:
            print(f"ğŸ“¦ åŠ è½½æ–‡æœ¬ç¼–ç å™¨: {text_model_path}")
            self.text_session = ort.InferenceSession(
                text_model_path,
                providers=providers
            )
        else:
            self.text_session = None
        
        # è·å–è¾“å…¥è¾“å‡ºåç§°
        self.vision_input_name = self.vision_session.get_inputs()[0].name
        self.vision_output_name = self.vision_session.get_outputs()[0].name
        
        if self.text_session:
            self.text_input_names = [inp.name for inp in self.text_session.get_inputs()]
            self.text_output_name = self.text_session.get_outputs()[0].name
        
        # æ‰“å°providerä¿¡æ¯
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   è§†è§‰ç¼–ç å™¨ Provider: {self.vision_session.get_providers()[0]}")
        if self.text_session:
            print(f"   æ–‡æœ¬ç¼–ç å™¨ Provider: {self.text_session.get_providers()[0]}")
        
        # å›¾åƒé¢„å¤„ç†
        self.image_transform = transforms.Compose([
            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711]
            ),
        ])
        
        # é¢„çƒ­
        self._warmup()
    
    def _warmup(self):
        """é¢„çƒ­æ¨¡å‹"""
        print("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        dummy_image = Image.new('RGB', (224, 224), color='white')
        _ = self.get_image_features([dummy_image])
        print("âœ… é¢„çƒ­å®Œæˆ")
    
    def preprocess_image(self, image: Union[str, Image.Image]) -> np.ndarray:
        """
        å›¾åƒé¢„å¤„ç†
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            
        Returns:
            é¢„å¤„ç†åçš„numpyæ•°ç»„
        """
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif not isinstance(image, Image.Image):
            raise ValueError("imageå¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„æˆ–PIL.Imageå¯¹è±¡")
        
        # åº”ç”¨å˜æ¢
        image_tensor = self.image_transform(image)
        
        # è½¬æ¢ä¸ºnumpyå¹¶æ·»åŠ batchç»´åº¦
        image_np = image_tensor.numpy()[np.newaxis, :]
        
        return image_np.astype(np.float32)
    
    def get_image_features(
        self,
        images: List[Union[str, Image.Image]],
        normalize: bool = True
    ) -> np.ndarray:
        """
        æå–å›¾åƒç‰¹å¾
        
        Args:
            images: å›¾åƒåˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾
            
        Returns:
            å›¾åƒç‰¹å¾æ•°ç»„ (batch_size, feature_dim)
        """
        # é¢„å¤„ç†æ‰€æœ‰å›¾åƒ
        image_arrays = [self.preprocess_image(img) for img in images]
        batch_images = np.concatenate(image_arrays, axis=0)
        
        # æ¨ç†
        features = self.vision_session.run(
            [self.vision_output_name],
            {self.vision_input_name: batch_images}
        )[0]
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / np.linalg.norm(features, axis=-1, keepdims=True)
        
        return features
    
    def get_text_features(
        self,
        input_ids: np.ndarray,
        attention_mask: np.ndarray,
        normalize: bool = True
    ) -> np.ndarray:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Args:
            input_ids: token IDs
            attention_mask: æ³¨æ„åŠ›æ©ç 
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾
            
        Returns:
            æ–‡æœ¬ç‰¹å¾æ•°ç»„ (batch_size, feature_dim)
        """
        if self.text_session is None:
            raise ValueError("æ–‡æœ¬ç¼–ç å™¨æœªåŠ è½½")
        
        # æ¨ç†
        features = self.text_session.run(
            [self.text_output_name],
            {
                self.text_input_names[0]: input_ids,
                self.text_input_names[1]: attention_mask
            }
        )[0]
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / np.linalg.norm(features, axis=-1, keepdims=True)
        
        return features
    
    def compute_similarity(
        self,
        image_features: np.ndarray,
        text_features: np.ndarray,
        logit_scale: float = 100.0
    ) -> np.ndarray:
        """
        è®¡ç®—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ç›¸ä¼¼åº¦
        
        Args:
            image_features: å›¾åƒç‰¹å¾ (N, D)
            text_features: æ–‡æœ¬ç‰¹å¾ (M, D)
            logit_scale: logitç¼©æ”¾å› å­
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ (N, M)
        """
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = image_features @ text_features.T
        
        # ç¼©æ”¾
        similarity = similarity * logit_scale
        
        return similarity
    
    def predict_image_text(
        self,
        image: Union[str, Image.Image],
        input_ids: np.ndarray,
        attention_mask: np.ndarray,
        return_probs: bool = True
    ) -> Dict:
        """
        å›¾æ–‡åŒ¹é…æ¨ç†
        
        Args:
            image: å›¾åƒ
            input_ids: æ–‡æœ¬token IDs
            attention_mask: æ³¨æ„åŠ›æ©ç 
            return_probs: æ˜¯å¦è¿”å›æ¦‚ç‡
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        # æå–ç‰¹å¾
        image_features = self.get_image_features([image])
        text_features = self.get_text_features(input_ids, attention_mask)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logits = self.compute_similarity(image_features, text_features)[0]
        
        # è®¡ç®—æ¦‚ç‡
        if return_probs:
            exp_logits = np.exp(logits - np.max(logits))
            probs = exp_logits / np.sum(exp_logits)
            scores = probs
        else:
            scores = logits
        
        inference_time = time.time() - start_time
        
        return {
            "scores": scores,
            "inference_time_ms": inference_time * 1000
        }


def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    import argparse
    from transformers import CLIPTokenizer
    
    parser = argparse.ArgumentParser(description="ONNX CLIPæ¨ç†ç¤ºä¾‹")
    parser.add_argument(
        '--vision_model',
        type=str,
        required=True,
        help='è§†è§‰ç¼–ç å™¨ONNXæ¨¡å‹è·¯å¾„'
    )
    parser.add_argument(
        '--text_model',
        type=str,
        required=True,
        help='æ–‡æœ¬ç¼–ç å™¨ONNXæ¨¡å‹è·¯å¾„'
    )
    parser.add_argument(
        '--image',
        type=str,
        required=True,
        help='å›¾åƒè·¯å¾„'
    )
    parser.add_argument(
        '--texts',
        type=str,
        nargs='+',
        required=True,
        help='å€™é€‰æ–‡æœ¬åˆ—è¡¨'
    )
    parser.add_argument(
        '--cpu',
        action='store_true',
        help='ä½¿ç”¨CPUæ¨ç†'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†æœåŠ¡
    service = ONNXCLIPInferenceService(
        vision_model_path=args.vision_model,
        text_model_path=args.text_model,
        use_gpu=not args.cpu
    )
    
    # åŠ è½½tokenizer
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    
    # å¤„ç†æ–‡æœ¬
    text_inputs = tokenizer(
        args.texts,
        padding='max_length',
        max_length=77,
        truncation=True,
        return_tensors='np'
    )
    
    # æ¨ç†
    print(f"\nğŸ–¼ï¸  å›¾åƒ: {args.image}")
    print(f"ğŸ“ å€™é€‰æ–‡æœ¬: {args.texts}")
    print("\n" + "=" * 60)
    
    results = service.predict_image_text(
        image=args.image,
        input_ids=text_inputs['input_ids'],
        attention_mask=text_inputs['attention_mask']
    )
    
    print(f"\nâ±ï¸  æ¨ç†æ—¶é—´: {results['inference_time_ms']:.2f}ms")
    
    print("\nğŸ“Š é¢„æµ‹ç»“æœ:")
    sorted_indices = np.argsort(results['scores'])[::-1]
    for idx, i in enumerate(sorted_indices, 1):
        print(f"  {idx}. {args.texts[i]}")
        print(f"     å¾—åˆ†: {results['scores'][i]:.4f}")


if __name__ == '__main__':
    main()

