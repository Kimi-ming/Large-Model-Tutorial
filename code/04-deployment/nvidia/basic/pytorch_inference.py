"""
PyTorchæ¨ç†æœåŠ¡

æä¾›åŸºäºPyTorchçš„CLIPæ¨¡å‹æ¨ç†æœåŠ¡
"""

import torch
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
from typing import List, Dict, Union, Tuple
import time
from pathlib import Path


class CLIPInferenceService:
    """
    CLIPæ¨ç†æœåŠ¡
    
    æ”¯æŒå›¾æ–‡åŒ¹é…ã€å›¾åƒç‰¹å¾æå–ã€æ–‡æœ¬ç‰¹å¾æå–
    """
    
    def __init__(
        self,
        model_path: str,
        device: str = "cuda",
        use_fp16: bool = False
    ):
        """
        åˆå§‹åŒ–æ¨ç†æœåŠ¡
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡ ("cuda", "cpu", "mps")
            use_fp16: æ˜¯å¦ä½¿ç”¨FP16æ··åˆç²¾åº¦
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.use_fp16 = use_fp16 and torch.cuda.is_available()
        
        print(f"ğŸš€ åˆå§‹åŒ–CLIPæ¨ç†æœåŠ¡...")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   FP16: {self.use_fp16}")
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        self.model = CLIPModel.from_pretrained(model_path)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        # è½¬æ¢ä¸ºFP16
        if self.use_fp16:
            self.model = self.model.half()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
        
        # é¢„çƒ­
        self._warmup()
    
    def _warmup(self):
        """é¢„çƒ­æ¨¡å‹"""
        print("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        dummy_image = Image.new('RGB', (224, 224), color='white')
        dummy_text = ["warmup"]
        
        with torch.no_grad():
            inputs = self.processor(
                text=dummy_text,
                images=dummy_image,
                return_tensors="pt",
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            if self.use_fp16:
                inputs = {k: v.half() if v.dtype == torch.float32 else v 
                         for k, v in inputs.items()}
            
            _ = self.model(**inputs)
        
        print("âœ… é¢„çƒ­å®Œæˆ")
    
    @torch.no_grad()
    def predict_image_text(
        self,
        image: Union[str, Image.Image],
        texts: List[str],
        return_probs: bool = True
    ) -> Dict:
        """
        å›¾æ–‡åŒ¹é…æ¨ç†
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            texts: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            return_probs: æ˜¯å¦è¿”å›æ¦‚ç‡ï¼ˆå¦åˆ™è¿”å›logitsï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        # åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif not isinstance(image, Image.Image):
            raise ValueError("imageå¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„æˆ–PIL.Imageå¯¹è±¡")
        
        # é¢„å¤„ç†
        inputs = self.processor(
            text=texts,
            images=image,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æ¨ç†
        outputs = self.model(**inputs)
        logits = outputs.logits_per_image[0]
        
        # è®¡ç®—æ¦‚ç‡æˆ–è¿”å›logits
        if return_probs:
            scores = logits.softmax(dim=0)
        else:
            scores = logits
        
        # æ„å»ºç»“æœ
        results = [
            {
                "text": text,
                "score": float(score),
                "rank": idx + 1
            }
            for idx, (text, score) in enumerate(
                sorted(zip(texts, scores.cpu().numpy()), 
                      key=lambda x: x[1], reverse=True)
            )
        ]
        
        inference_time = time.time() - start_time
        
        return {
            "results": results,
            "inference_time_ms": inference_time * 1000,
            "device": str(self.device),
            "fp16": self.use_fp16
        }
    
    @torch.no_grad()
    def get_image_features(
        self,
        images: Union[List[str], List[Image.Image]],
        normalize: bool = True
    ) -> torch.Tensor:
        """
        æå–å›¾åƒç‰¹å¾
        
        Args:
            images: å›¾åƒè·¯å¾„åˆ—è¡¨æˆ–PIL Imageåˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            
        Returns:
            å›¾åƒç‰¹å¾å¼ é‡ (batch_size, feature_dim)
        """
        # åŠ è½½å›¾åƒ
        pil_images = []
        for img in images:
            if isinstance(img, str):
                pil_images.append(Image.open(img).convert('RGB'))
            elif isinstance(img, Image.Image):
                pil_images.append(img)
            else:
                raise ValueError("å›¾åƒå¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„æˆ–PIL.Imageå¯¹è±¡")
        
        # é¢„å¤„ç†
        inputs = self.processor(
            images=pil_images,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        features = self.model.get_image_features(**inputs)
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / features.norm(dim=-1, keepdim=True)
        
        return features.cpu()
    
    @torch.no_grad()
    def get_text_features(
        self,
        texts: List[str],
        normalize: bool = True
    ) -> torch.Tensor:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            
        Returns:
            æ–‡æœ¬ç‰¹å¾å¼ é‡ (batch_size, feature_dim)
        """
        # é¢„å¤„ç†
        inputs = self.processor(
            text=texts,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        features = self.model.get_text_features(**inputs)
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / features.norm(dim=-1, keepdim=True)
        
        return features.cpu()
    
    def compute_similarity(
        self,
        image_features: torch.Tensor,
        text_features: torch.Tensor
    ) -> torch.Tensor:
        """
        è®¡ç®—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ç›¸ä¼¼åº¦
        
        Args:
            image_features: å›¾åƒç‰¹å¾ (N, D)
            text_features: æ–‡æœ¬ç‰¹å¾ (M, D)
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ (N, M)
        """
        # ç¡®ä¿ç‰¹å¾å·²å½’ä¸€åŒ–
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = image_features @ text_features.T
        
        # ç¼©æ”¾ï¼ˆCLIPçš„logit_scaleï¼‰
        similarity = similarity * self.model.logit_scale.exp().item()
        
        return similarity


def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    import argparse
    
    parser = argparse.ArgumentParser(description="CLIPæ¨ç†æœåŠ¡ç¤ºä¾‹")
    parser.add_argument(
        '--model',
        type=str,
        default='openai/clip-vit-base-patch32',
        help='æ¨¡å‹è·¯å¾„æˆ–åç§°'
    )
    parser.add_argument(
        '--image',
        type=str,
        required=True,
        help='å›¾åƒè·¯å¾„'
    )
    parser.add_argument(
        '--texts',
        type=str,
        nargs='+',
        required=True,
        help='å€™é€‰æ–‡æœ¬åˆ—è¡¨'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        choices=['cuda', 'cpu', 'mps'],
        help='è®¡ç®—è®¾å¤‡'
    )
    parser.add_argument(
        '--fp16',
        action='store_true',
        help='ä½¿ç”¨FP16æ··åˆç²¾åº¦'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†æœåŠ¡
    service = CLIPInferenceService(
        model_path=args.model,
        device=args.device,
        use_fp16=args.fp16
    )
    
    # æ¨ç†
    print(f"\nğŸ–¼ï¸  å›¾åƒ: {args.image}")
    print(f"ğŸ“ å€™é€‰æ–‡æœ¬: {args.texts}")
    print("\n" + "=" * 60)
    
    results = service.predict_image_text(
        image=args.image,
        texts=args.texts
    )
    
    print(f"\nâ±ï¸  æ¨ç†æ—¶é—´: {results['inference_time_ms']:.2f}ms")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {results['device']}")
    print(f"ğŸ”¢ FP16: {results['fp16']}")
    
    print("\nğŸ“Š é¢„æµ‹ç»“æœ:")
    for result in results['results']:
        print(f"  {result['rank']}. {result['text']}")
        print(f"     å¾—åˆ†: {result['score']:.4f}")


if __name__ == '__main__':
    main()

