"""
CLIPæ¨ç†APIæœåŠ¡

åŸºäºFastAPIçš„CLIPæ¨¡å‹æ¨ç†æœåŠ¡
"""

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor
import io
from typing import List, Dict, Union
import time


class CLIPInferenceService:
    """
    CLIPæ¨ç†æœåŠ¡ï¼ˆå†…åµŒç‰ˆæœ¬ï¼‰
    
    æ”¯æŒå›¾æ–‡åŒ¹é…ã€å›¾åƒç‰¹å¾æå–ã€æ–‡æœ¬ç‰¹å¾æå–
    """
    
    def __init__(
        self,
        model_path: str,
        device: str = "cuda",
        use_fp16: bool = False
    ):
        """
        åˆå§‹åŒ–æ¨ç†æœåŠ¡
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡ ("cuda", "cpu", "mps")
            use_fp16: æ˜¯å¦ä½¿ç”¨FP16æ··åˆç²¾åº¦
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.use_fp16 = use_fp16 and torch.cuda.is_available()
        
        print(f"ğŸš€ åˆå§‹åŒ–CLIPæ¨ç†æœåŠ¡...")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   FP16: {self.use_fp16}")
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        self.model = CLIPModel.from_pretrained(model_path)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        # è½¬æ¢ä¸ºFP16
        if self.use_fp16:
            self.model = self.model.half()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
    
    @torch.no_grad()
    def predict_image_text(
        self,
        image: Union[str, Image.Image],
        texts: List[str],
        return_probs: bool = True
    ) -> Dict:
        """
        å›¾æ–‡åŒ¹é…æ¨ç†
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            texts: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            return_probs: æ˜¯å¦è¿”å›æ¦‚ç‡ï¼ˆå¦åˆ™è¿”å›logitsï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        # åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif not isinstance(image, Image.Image):
            raise ValueError("imageå¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„æˆ–PIL.Imageå¯¹è±¡")
        
        # é¢„å¤„ç†
        inputs = self.processor(
            text=texts,
            images=image,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æ¨ç†
        outputs = self.model(**inputs)
        logits = outputs.logits_per_image[0]
        
        # è®¡ç®—æ¦‚ç‡æˆ–è¿”å›logits
        if return_probs:
            scores = logits.softmax(dim=0)
        else:
            scores = logits
        
        # æ„å»ºç»“æœ
        results = [
            {
                "text": text,
                "score": float(score),
                "rank": idx + 1
            }
            for idx, (text, score) in enumerate(
                sorted(zip(texts, scores.cpu().numpy()), 
                      key=lambda x: x[1], reverse=True)
            )
        ]
        
        inference_time = time.time() - start_time
        
        return {
            "results": results,
            "inference_time_ms": inference_time * 1000,
            "device": str(self.device),
            "fp16": self.use_fp16
        }
    
    @torch.no_grad()
    def get_image_features(
        self,
        images: Union[List[str], List[Image.Image]],
        normalize: bool = True
    ) -> torch.Tensor:
        """
        æå–å›¾åƒç‰¹å¾
        
        Args:
            images: å›¾åƒè·¯å¾„åˆ—è¡¨æˆ–PIL Imageåˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            
        Returns:
            å›¾åƒç‰¹å¾å¼ é‡ (batch_size, feature_dim)
        """
        # åŠ è½½å›¾åƒ
        pil_images = []
        for img in images:
            if isinstance(img, str):
                pil_images.append(Image.open(img).convert('RGB'))
            elif isinstance(img, Image.Image):
                pil_images.append(img)
            else:
                raise ValueError("å›¾åƒå¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„æˆ–PIL.Imageå¯¹è±¡")
        
        # é¢„å¤„ç†
        inputs = self.processor(
            images=pil_images,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        features = self.model.get_image_features(**inputs)
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / features.norm(dim=-1, keepdim=True)
        
        return features.cpu()
    
    @torch.no_grad()
    def get_text_features(
        self,
        texts: List[str],
        normalize: bool = True
    ) -> torch.Tensor:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            
        Returns:
            æ–‡æœ¬ç‰¹å¾å¼ é‡ (batch_size, feature_dim)
        """
        # é¢„å¤„ç†
        inputs = self.processor(
            text=texts,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        features = self.model.get_text_features(**inputs)
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / features.norm(dim=-1, keepdim=True)
        
        return features.cpu()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="CLIPæ¨ç†æœåŠ¡",
    description="åŸºäºCLIPçš„å›¾æ–‡åŒ¹é…æ¨ç†API",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ¨¡å‹å®ä¾‹
model_service = None


@app.on_event("startup")
async def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model_service
    
    print("=" * 60)
    print("å¯åŠ¨CLIPæ¨ç†æœåŠ¡")
    print("=" * 60)
    
    try:
        model_service = CLIPInferenceService(
            model_path="openai/clip-vit-base-patch32",
            device="cuda",
            use_fp16=True
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


@app.on_event("shutdown")
async def shutdown():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    print("ğŸ›‘ å…³é—­æœåŠ¡...")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "CLIPæ¨ç†æœåŠ¡",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "image_features": "/image_features",
            "docs": "/docs"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": model_service is not None,
        "gpu_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
    }


@app.post("/predict")
async def predict(
    image: UploadFile = File(..., description="ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶"),
    texts: str = Form(..., description="é€—å·åˆ†éš”çš„å€™é€‰æ–‡æœ¬")
):
    """
    å›¾æ–‡åŒ¹é…æ¨ç†
    
    Args:
        image: ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
        texts: é€—å·åˆ†éš”çš„å€™é€‰æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        é¢„æµ‹ç»“æœ
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è§£ææ–‡æœ¬
        text_list = [t.strip() for t in texts.split(',')]
        
        if len(text_list) == 0:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # è¯»å–å›¾åƒ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # æ¨ç†
        results = model_service.predict_image_text(
            image=pil_image,
            texts=text_list
        )
        
        return JSONResponse({
            "success": True,
            "data": results
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


@app.post("/image_features")
async def extract_image_features(
    image: UploadFile = File(..., description="ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶"),
    normalize: bool = Form(True, description="æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾")
):
    """
    æå–å›¾åƒç‰¹å¾
    
    Args:
        image: ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
        normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
        
    Returns:
        å›¾åƒç‰¹å¾å‘é‡
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è¯»å–å›¾åƒ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # æå–ç‰¹å¾
        features = model_service.get_image_features(
            images=[pil_image],
            normalize=normalize
        )
        
        return JSONResponse({
            "success": True,
            "data": {
                "features": features[0].tolist(),
                "shape": list(features.shape),
                "normalized": normalize
            }
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


@app.post("/text_features")
async def extract_text_features(
    texts: str = Form(..., description="é€—å·åˆ†éš”çš„æ–‡æœ¬åˆ—è¡¨"),
    normalize: bool = Form(True, description="æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾")
):
    """
    æå–æ–‡æœ¬ç‰¹å¾
    
    Args:
        texts: é€—å·åˆ†éš”çš„æ–‡æœ¬åˆ—è¡¨
        normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
        
    Returns:
        æ–‡æœ¬ç‰¹å¾å‘é‡
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è§£ææ–‡æœ¬
        text_list = [t.strip() for t in texts.split(',')]
        
        if len(text_list) == 0:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # æå–ç‰¹å¾
        features = model_service.get_text_features(
            texts=text_list,
            normalize=normalize
        )
        
        return JSONResponse({
            "success": True,
            "data": {
                "features": features.tolist(),
                "shape": list(features.shape),
                "normalized": normalize,
                "texts": text_list
            }
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )

