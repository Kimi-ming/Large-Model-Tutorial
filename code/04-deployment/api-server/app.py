"""
CLIPæ¨ç†APIæœåŠ¡

åŸºäºFastAPIçš„CLIPæ¨¡å‹æ¨ç†æœåŠ¡
"""

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor
import io
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from code.deployment.nvidia.basic.pytorch_inference import CLIPInferenceService

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="CLIPæ¨ç†æœåŠ¡",
    description="åŸºäºCLIPçš„å›¾æ–‡åŒ¹é…æ¨ç†API",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ¨¡å‹å®ä¾‹
model_service = None


@app.on_event("startup")
async def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model_service
    
    print("=" * 60)
    print("å¯åŠ¨CLIPæ¨ç†æœåŠ¡")
    print("=" * 60)
    
    try:
        model_service = CLIPInferenceService(
            model_path="openai/clip-vit-base-patch32",
            device="cuda",
            use_fp16=True
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


@app.on_event("shutdown")
async def shutdown():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    print("ğŸ›‘ å…³é—­æœåŠ¡...")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "CLIPæ¨ç†æœåŠ¡",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "image_features": "/image_features",
            "docs": "/docs"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": model_service is not None,
        "gpu_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
    }


@app.post("/predict")
async def predict(
    image: UploadFile = File(..., description="ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶"),
    texts: str = Form(..., description="é€—å·åˆ†éš”çš„å€™é€‰æ–‡æœ¬")
):
    """
    å›¾æ–‡åŒ¹é…æ¨ç†
    
    Args:
        image: ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
        texts: é€—å·åˆ†éš”çš„å€™é€‰æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        é¢„æµ‹ç»“æœ
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è§£ææ–‡æœ¬
        text_list = [t.strip() for t in texts.split(',')]
        
        if len(text_list) == 0:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # è¯»å–å›¾åƒ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # æ¨ç†
        results = model_service.predict_image_text(
            image=pil_image,
            texts=text_list
        )
        
        return JSONResponse({
            "success": True,
            "data": results
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


@app.post("/image_features")
async def extract_image_features(
    image: UploadFile = File(..., description="ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶"),
    normalize: bool = Form(True, description="æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾")
):
    """
    æå–å›¾åƒç‰¹å¾
    
    Args:
        image: ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
        normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
        
    Returns:
        å›¾åƒç‰¹å¾å‘é‡
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è¯»å–å›¾åƒ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # æå–ç‰¹å¾
        features = model_service.get_image_features(
            images=[pil_image],
            normalize=normalize
        )
        
        return JSONResponse({
            "success": True,
            "data": {
                "features": features[0].tolist(),
                "shape": list(features.shape),
                "normalized": normalize
            }
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


@app.post("/text_features")
async def extract_text_features(
    texts: str = Form(..., description="é€—å·åˆ†éš”çš„æ–‡æœ¬åˆ—è¡¨"),
    normalize: bool = Form(True, description="æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾")
):
    """
    æå–æ–‡æœ¬ç‰¹å¾
    
    Args:
        texts: é€—å·åˆ†éš”çš„æ–‡æœ¬åˆ—è¡¨
        normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
        
    Returns:
        æ–‡æœ¬ç‰¹å¾å‘é‡
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è§£ææ–‡æœ¬
        text_list = [t.strip() for t in texts.split(',')]
        
        if len(text_list) == 0:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # æå–ç‰¹å¾
        features = model_service.get_text_features(
            texts=text_list,
            normalize=normalize
        )
        
        return JSONResponse({
            "success": True,
            "data": {
                "features": features.tolist(),
                "shape": list(features.shape),
                "normalized": normalize,
                "texts": text_list
            }
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )

