"""
CLIPæ¨ç†APIæœåŠ¡

åŸºäºFastAPIçš„CLIPæ¨¡å‹æ¨ç†æœåŠ¡
"""

from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor
import io
from typing import List, Dict, Union
import time
import os
from collections import defaultdict
from datetime import datetime, timedelta

# å°è¯•å¯¼å…¥slowapiï¼ˆå¯é€‰ä¾èµ–ï¼‰
try:
    from slowapi import Limiter, _rate_limit_exceeded_handler
    from slowapi.util import get_remote_address
    from slowapi.errors import RateLimitExceeded
    SLOWAPI_AVAILABLE = True
except ImportError:
    print("âš ï¸  slowapiæœªå®‰è£…ï¼Œé™æµåŠŸèƒ½ä¸å¯ç”¨")
    print("   å®‰è£…æ–¹æ³•: pip install slowapi")
    SLOWAPI_AVAILABLE = False
    Limiter = None


class CLIPInferenceService:
    """
    CLIPæ¨ç†æœåŠ¡ï¼ˆå†…åµŒç‰ˆæœ¬ï¼‰
    
    æ”¯æŒå›¾æ–‡åŒ¹é…ã€å›¾åƒç‰¹å¾æå–ã€æ–‡æœ¬ç‰¹å¾æå–
    """
    
    def __init__(
        self,
        model_path: str,
        device: str = "cuda",
        use_fp16: bool = False
    ):
        """
        åˆå§‹åŒ–æ¨ç†æœåŠ¡
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡ ("cuda", "cpu", "mps")
            use_fp16: æ˜¯å¦ä½¿ç”¨FP16æ··åˆç²¾åº¦
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.use_fp16 = use_fp16 and torch.cuda.is_available()
        
        print(f"ğŸš€ åˆå§‹åŒ–CLIPæ¨ç†æœåŠ¡...")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   FP16: {self.use_fp16}")
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        self.model = CLIPModel.from_pretrained(model_path)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        # è½¬æ¢ä¸ºFP16
        if self.use_fp16:
            self.model = self.model.half()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
    
    @torch.no_grad()
    def predict_image_text(
        self,
        image: Union[str, Image.Image],
        texts: List[str],
        return_probs: bool = True
    ) -> Dict:
        """
        å›¾æ–‡åŒ¹é…æ¨ç†
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            texts: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            return_probs: æ˜¯å¦è¿”å›æ¦‚ç‡ï¼ˆå¦åˆ™è¿”å›logitsï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        # åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif not isinstance(image, Image.Image):
            raise ValueError("imageå¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„æˆ–PIL.Imageå¯¹è±¡")
        
        # é¢„å¤„ç†
        inputs = self.processor(
            text=texts,
            images=image,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æ¨ç†
        outputs = self.model(**inputs)
        logits = outputs.logits_per_image[0]
        
        # è®¡ç®—æ¦‚ç‡æˆ–è¿”å›logits
        if return_probs:
            scores = logits.softmax(dim=0)
        else:
            scores = logits
        
        # æ„å»ºç»“æœ
        results = [
            {
                "text": text,
                "score": float(score),
                "rank": idx + 1
            }
            for idx, (text, score) in enumerate(
                sorted(zip(texts, scores.cpu().numpy()), 
                      key=lambda x: x[1], reverse=True)
            )
        ]
        
        inference_time = time.time() - start_time
        
        return {
            "results": results,
            "inference_time_ms": inference_time * 1000,
            "device": str(self.device),
            "fp16": self.use_fp16
        }
    
    @torch.no_grad()
    def get_image_features(
        self,
        images: Union[List[str], List[Image.Image]],
        normalize: bool = True
    ) -> torch.Tensor:
        """
        æå–å›¾åƒç‰¹å¾
        
        Args:
            images: å›¾åƒè·¯å¾„åˆ—è¡¨æˆ–PIL Imageåˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            
        Returns:
            å›¾åƒç‰¹å¾å¼ é‡ (batch_size, feature_dim)
        """
        # åŠ è½½å›¾åƒ
        pil_images = []
        for img in images:
            if isinstance(img, str):
                pil_images.append(Image.open(img).convert('RGB'))
            elif isinstance(img, Image.Image):
                pil_images.append(img)
            else:
                raise ValueError("å›¾åƒå¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„æˆ–PIL.Imageå¯¹è±¡")
        
        # é¢„å¤„ç†
        inputs = self.processor(
            images=pil_images,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        features = self.model.get_image_features(**inputs)
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / features.norm(dim=-1, keepdim=True)
        
        return features.cpu()
    
    @torch.no_grad()
    def get_text_features(
        self,
        texts: List[str],
        normalize: bool = True
    ) -> torch.Tensor:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            
        Returns:
            æ–‡æœ¬ç‰¹å¾å¼ é‡ (batch_size, feature_dim)
        """
        # é¢„å¤„ç†
        inputs = self.processor(
            text=texts,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        features = self.model.get_text_features(**inputs)
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / features.norm(dim=-1, keepdim=True)
        
        return features.cpu()

# é…ç½®
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
ALLOWED_IMAGE_TYPES = {"image/jpeg", "image/png", "image/jpg", "image/webp"}

# åˆ›å»ºé™æµå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if SLOWAPI_AVAILABLE:
    limiter = Limiter(key_func=get_remote_address)
else:
    # å¦‚æœslowapiä¸å¯ç”¨ï¼Œä½¿ç”¨ç©ºè£…é¥°å™¨
    class DummyLimiter:
        def limit(self, *args, **kwargs):
            def decorator(func):
                return func
            return decorator
    limiter = DummyLimiter()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="CLIPæ¨ç†æœåŠ¡",
    description="åŸºäºCLIPçš„å›¾æ–‡åŒ¹é…æ¨ç†API",
    version="1.0.0"
)

# æ³¨å†Œé™æµå¼‚å¸¸å¤„ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if SLOWAPI_AVAILABLE:
    app.state.limiter = limiter
    app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ¨¡å‹å®ä¾‹
model_service = None


def validate_image_file(file: UploadFile) -> None:
    """
    éªŒè¯ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶
        
    Raises:
        HTTPException: æ–‡ä»¶éªŒè¯å¤±è´¥
    """
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if file.content_type not in ALLOWED_IMAGE_TYPES:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.content_type}. æ”¯æŒçš„ç±»å‹: {', '.join(ALLOWED_IMAGE_TYPES)}"
        )
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file.file.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
    file_size = file.file.tell()
    file.file.seek(0)  # é‡ç½®åˆ°æ–‡ä»¶å¼€å¤´
    
    if file_size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=413,
            detail=f"æ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.2f}MB. æœ€å¤§å…è®¸: {MAX_FILE_SIZE / 1024 / 1024}MB"
        )
    
    if file_size == 0:
        raise HTTPException(
            status_code=400,
            detail="æ–‡ä»¶ä¸ºç©º"
        )


@app.on_event("startup")
async def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model_service
    
    print("=" * 60)
    print("å¯åŠ¨CLIPæ¨ç†æœåŠ¡")
    print("=" * 60)
    
    try:
        model_service = CLIPInferenceService(
            model_path="openai/clip-vit-base-patch32",
            device="cuda",
            use_fp16=True
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


@app.on_event("shutdown")
async def shutdown():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    print("ğŸ›‘ å…³é—­æœåŠ¡...")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "CLIPæ¨ç†æœåŠ¡",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "image_features": "/image_features",
            "docs": "/docs"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": model_service is not None,
        "gpu_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
    }


@app.post("/predict")
@limiter.limit("30/minute")  # é™åˆ¶æ¯åˆ†é’Ÿ30æ¬¡è¯·æ±‚
async def predict(
    request: Request,
    image: UploadFile = File(..., description="ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶"),
    texts: str = Form(..., description="é€—å·åˆ†éš”çš„å€™é€‰æ–‡æœ¬")
):
    """
    å›¾æ–‡åŒ¹é…æ¨ç†
    
    é™æµ: 30æ¬¡/åˆ†é’Ÿ
    æ–‡ä»¶å¤§å°é™åˆ¶: 10MB
    æ”¯æŒæ ¼å¼: JPEG, PNG, WEBP
    
    Args:
        image: ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
        texts: é€—å·åˆ†éš”çš„å€™é€‰æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        é¢„æµ‹ç»“æœ
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    # éªŒè¯æ–‡ä»¶
    validate_image_file(image)
    
    try:
        # è§£ææ–‡æœ¬
        text_list = [t.strip() for t in texts.split(',')]
        
        if len(text_list) == 0:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # è¯»å–å›¾åƒ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # æ¨ç†
        results = model_service.predict_image_text(
            image=pil_image,
            texts=text_list
        )
        
        return JSONResponse({
            "success": True,
            "data": results
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


@app.post("/image_features")
@limiter.limit("20/minute")  # é™åˆ¶æ¯åˆ†é’Ÿ20æ¬¡è¯·æ±‚
async def extract_image_features(
    request: Request,
    image: UploadFile = File(..., description="ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶"),
    normalize: bool = Form(True, description="æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾")
):
    """
    æå–å›¾åƒç‰¹å¾
    
    é™æµ: 20æ¬¡/åˆ†é’Ÿ
    æ–‡ä»¶å¤§å°é™åˆ¶: 10MB
    
    Args:
        image: ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
        normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
        
    Returns:
        å›¾åƒç‰¹å¾å‘é‡
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    # éªŒè¯æ–‡ä»¶
    validate_image_file(image)
    
    try:
        # è¯»å–å›¾åƒ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # æå–ç‰¹å¾
        features = model_service.get_image_features(
            images=[pil_image],
            normalize=normalize
        )
        
        return JSONResponse({
            "success": True,
            "data": {
                "features": features[0].tolist(),
                "shape": list(features.shape),
                "normalized": normalize
            }
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


@app.post("/text_features")
@limiter.limit("50/minute")  # é™åˆ¶æ¯åˆ†é’Ÿ50æ¬¡è¯·æ±‚ï¼ˆæ–‡æœ¬ç‰¹å¾æå–è¾ƒå¿«ï¼‰
async def extract_text_features(
    request: Request,
    texts: str = Form(..., description="é€—å·åˆ†éš”çš„æ–‡æœ¬åˆ—è¡¨"),
    normalize: bool = Form(True, description="æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾")
):
    """
    æå–æ–‡æœ¬ç‰¹å¾
    
    é™æµ: 50æ¬¡/åˆ†é’Ÿ
    
    Args:
        texts: é€—å·åˆ†éš”çš„æ–‡æœ¬åˆ—è¡¨
        normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
        
    Returns:
        æ–‡æœ¬ç‰¹å¾å‘é‡
    """
    if model_service is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è§£ææ–‡æœ¬
        text_list = [t.strip() for t in texts.split(',')]
        
        if len(text_list) == 0:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # æå–ç‰¹å¾
        features = model_service.get_text_features(
            texts=text_list,
            normalize=normalize
        )
        
        return JSONResponse({
            "success": True,
            "data": {
                "features": features.tolist(),
                "shape": list(features.shape),
                "normalized": normalize,
                "texts": text_list
            }
        })
    
    except Exception as e:
        return JSONResponse(
            {
                "success": False,
                "error": str(e)
            },
            status_code=500
        )


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )

