"""
å•†å“è¯†åˆ«å™¨

ä½¿ç”¨CLIPæ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬å•†å“è¯†åˆ«
"""

import torch
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
import numpy as np
from typing import List, Dict, Union
import json
from pathlib import Path


class ProductRecognizer:
    """
    å•†å“è¯†åˆ«å™¨
    
    ä½¿ç”¨CLIPè¿›è¡Œé›¶æ ·æœ¬å•†å“è¯†åˆ«ï¼Œæ”¯æŒï¼š
    - SKUçº§åˆ«è¯†åˆ«
    - å¤šå•†å“åŒ¹é…
    - ç½®ä¿¡åº¦è¯„ä¼°
    """
    
    def __init__(
        self,
        model_path: str = "openai/clip-vit-base-patch32",
        product_database: str = None,
        device: str = "auto",
        confidence_threshold: float = 0.7
    ):
        """
        åˆå§‹åŒ–å•†å“è¯†åˆ«å™¨
        
        Args:
            model_path: CLIPæ¨¡å‹è·¯å¾„
            product_database: å•†å“æ•°æ®åº“JSONæ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.device = self._get_device(device)
        self.confidence_threshold = confidence_threshold
        
        print(f"ğŸš€ åˆå§‹åŒ–å•†å“è¯†åˆ«å™¨...")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
        
        # åŠ è½½æ¨¡å‹
        self.model = CLIPModel.from_pretrained(model_path)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        self.model = self.model.to(self.device)
        self.model.eval()
        
        # åŠ è½½å•†å“æ•°æ®åº“
        self.products = self._load_products(product_database)
        print(f"   å•†å“æ•°é‡: {len(self.products)}")
        
        # é¢„è®¡ç®—å•†å“æ–‡æœ¬ç‰¹å¾
        self._precompute_text_features()
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
    
    def _get_device(self, device: str) -> torch.device:
        """é€‰æ‹©è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return torch.device("cuda:0")
            else:
                return torch.device("cpu")
        return torch.device(device)
    
    def _load_products(self, database_path: str) -> List[Dict]:
        """åŠ è½½å•†å“æ•°æ®åº“"""
        if database_path and Path(database_path).exists():
            with open(database_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # é»˜è®¤æ¼”ç¤ºæ•°æ®
            return [
                {
                    "sku": "SKU-001",
                    "name": "å¯å£å¯ä¹ 330ml",
                    "category": "é¥®æ–™",
                    "brand": "å¯å£å¯ä¹",
                    "price": 3.5,
                    "description": "å¯å£å¯ä¹ç»å…¸ç½è£…é¥®æ–™ 330æ¯«å‡"
                },
                {
                    "sku": "SKU-002",
                    "name": "é›ªç¢§ 330ml",
                    "category": "é¥®æ–™",
                    "brand": "å¯å£å¯ä¹",
                    "price": 3.5,
                    "description": "é›ªç¢§æŸ æª¬å‘³æ±½æ°´ 330æ¯«å‡"
                },
                {
                    "sku": "SKU-003",
                    "name": "å†œå¤«å±±æ³‰ 550ml",
                    "category": "é¥®æ–™",
                    "brand": "å†œå¤«å±±æ³‰",
                    "price": 2.0,
                    "description": "å†œå¤«å±±æ³‰å¤©ç„¶æ°´ 550æ¯«å‡"
                },
                {
                    "sku": "SKU-004",
                    "name": "å¥¥åˆ©å¥¥é¥¼å¹²",
                    "category": "é›¶é£Ÿ",
                    "brand": "å¥¥åˆ©å¥¥",
                    "price": 9.9,
                    "description": "å¥¥åˆ©å¥¥å¤¹å¿ƒé¥¼å¹²åŸå‘³"
                },
                {
                    "sku": "SKU-005",
                    "name": "æ—ºæ—ºé›ªé¥¼",
                    "category": "é›¶é£Ÿ",
                    "brand": "æ—ºæ—º",
                    "price": 5.5,
                    "description": "æ—ºæ—ºé›ªé¥¼è†¨åŒ–é£Ÿå“"
                }
            ]
    
    def _precompute_text_features(self):
        """é¢„è®¡ç®—æ‰€æœ‰å•†å“çš„æ–‡æœ¬ç‰¹å¾"""
        print(f"ğŸ”„ é¢„è®¡ç®—å•†å“æ–‡æœ¬ç‰¹å¾...")
        
        texts = [f"{p['name']} {p['description']}" for p in self.products]
        
        inputs = self.processor(
            text=texts,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            text_features = self.model.get_text_features(**inputs)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        self.text_features = text_features.cpu().numpy()
    
    def recognize(
        self,
        image: Union[str, Image.Image],
        top_k: int = 5
    ) -> Dict:
        """
        è¯†åˆ«å•†å“
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Image
            top_k: è¿”å›top-kç»“æœ
            
        Returns:
            è¯†åˆ«ç»“æœå­—å…¸
        """
        # åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # æå–å›¾åƒç‰¹å¾
        inputs = self.processor(
            images=image,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            image_features = self.model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        image_features = image_features.cpu().numpy()
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = (image_features @ self.text_features.T)[0]
        
        # è·å–top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            product = self.products[idx].copy()
            product['confidence'] = float(similarities[idx])
            product['match'] = similarities[idx] >= self.confidence_threshold
            results.append(product)
        
        # æœ€ä½³åŒ¹é…
        best = results[0]
        
        return {
            'best_match': best,
            'top_k_matches': results,
            'recognized': best['confidence'] >= self.confidence_threshold
        }
    
    def batch_recognize(
        self,
        images: List[Union[str, Image.Image]],
        top_k: int = 5
    ) -> List[Dict]:
        """
        æ‰¹é‡è¯†åˆ«å•†å“
        
        Args:
            images: å›¾åƒåˆ—è¡¨
            top_k: è¿”å›top-kç»“æœ
            
        Returns:
            è¯†åˆ«ç»“æœåˆ—è¡¨
        """
        results = []
        for image in images:
            result = self.recognize(image, top_k=top_k)
            results.append(result)
        return results


def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å•†å“è¯†åˆ«å™¨')
    parser.add_argument('--image', type=str, required=True, help='å›¾åƒè·¯å¾„')
    parser.add_argument('--database', type=str, help='å•†å“æ•°æ®åº“JSONæ–‡ä»¶')
    parser.add_argument('--top-k', type=int, default=5, help='è¿”å›top-kç»“æœ')
    parser.add_argument('--threshold', type=float, default=0.7, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–è¯†åˆ«å™¨
    recognizer = ProductRecognizer(
        product_database=args.database,
        confidence_threshold=args.threshold
    )
    
    # è¯†åˆ«å•†å“
    result = recognizer.recognize(args.image, top_k=args.top_k)
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ“ è¯†åˆ«ç»“æœ:")
    print(f"="*60)
    
    best = result['best_match']
    print(f"ğŸ† æœ€ä½³åŒ¹é…:")
    print(f"   å•†å“åç§°: {best['name']}")
    print(f"   SKU: {best['sku']}")
    print(f"   ç±»åˆ«: {best['category']}")
    print(f"   å“ç‰Œ: {best['brand']}")
    print(f"   ä»·æ ¼: Â¥{best['price']}")
    print(f"   ç½®ä¿¡åº¦: {best['confidence']:.2%}")
    print(f"   åŒ¹é…: {'âœ… æ˜¯' if best['match'] else 'âŒ å¦'}")
    
    print(f"\nğŸ“Š Top-{args.top_k} åŒ¹é…:")
    for i, match in enumerate(result['top_k_matches'], 1):
        print(f"{i}. {match['name']} (ç½®ä¿¡åº¦: {match['confidence']:.2%})")


if __name__ == '__main__':
    main()

