#!/usr/bin/env python3
"""
Qwen-VL æ¨ç†ç¤ºä¾‹

Qwen-VLæ˜¯é˜¿é‡Œå·´å·´å¼€å‘çš„ä¸­æ–‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¸­æ–‡åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚

åŠŸèƒ½:
- å›¾åƒæè¿°ç”Ÿæˆï¼ˆä¸­æ–‡ï¼‰
- è§†è§‰é—®ç­”ï¼ˆVQAï¼‰
- OCRæ–‡å­—è¯†åˆ«
- å¤šå›¾ç†è§£

ä¾èµ–:
    pip install transformers>=4.32.0 transformers_stream_generator
    pip install torch torchvision pillow

ä½œè€…: Large-Model-Tutorial Team
æ—¥æœŸ: 2025-11-06
ç‰ˆæœ¬: v1.1.0
"""

import argparse
import os
from typing import List, Optional, Dict, Any
from pathlib import Path

try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from PIL import Image
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–: {e}")
    print("è¯·å®‰è£…: pip install torch transformers pillow")
    exit(1)


class QwenVLInference:
    """Qwen-VL æ¨ç†ç±»"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen-VL-Chat",
        device: str = "auto",
        trust_remote_code: bool = True
    ):
        """
        åˆå§‹åŒ–Qwen-VLæ¨¡å‹
        
        å‚æ•°:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡ ('cuda', 'cpu', 'auto')
            trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆQwen-VLéœ€è¦ï¼‰
        """
        self.model_name = model_name
        self.device = self._setup_device(device)
        
        print(f"ğŸš€ åŠ è½½Qwen-VLæ¨¡å‹: {model_name}")
        print(f"ğŸ“ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=trust_remote_code
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map=device if device == "auto" else None,
                trust_remote_code=trust_remote_code,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            ).eval()
            
            if device != "auto" and device != "cpu":
                self.model = self.model.to(self.device)
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\nğŸ’¡ æç¤º:")
            print("   1. ç¡®ä¿å·²å®‰è£…: pip install transformers>=4.32.0 transformers_stream_generator")
            print("   2. é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼ˆçº¦10GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…")
            print("   3. ç¡®ä¿ç½‘ç»œç•…é€šæˆ–é…ç½®HuggingFaceé•œåƒ")
            raise
    
    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è¿è¡Œè®¾å¤‡"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def generate_caption(
        self,
        image_path: str,
        prompt: str = "æè¿°è¿™å¼ å›¾ç‰‡",
        max_length: int = 256
    ) -> str:
        """
        ç”Ÿæˆå›¾åƒæè¿°
        
        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            prompt: æç¤ºæ–‡æœ¬
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        è¿”å›:
            ç”Ÿæˆçš„æè¿°æ–‡æœ¬
        """
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        
        # æ„å»ºæŸ¥è¯¢
        query = self.tokenizer.from_list_format([
            {'image': image_path},
            {'text': prompt},
        ])
        
        # ç”Ÿæˆå“åº”
        response, _ = self.model.chat(
            self.tokenizer,
            query=query,
            history=None,
            max_length=max_length
        )
        
        return response
    
    def visual_question_answering(
        self,
        image_path: str,
        question: str,
        max_length: int = 256
    ) -> str:
        """
        è§†è§‰é—®ç­”
        
        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            question: é—®é¢˜æ–‡æœ¬
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        è¿”å›:
            ç­”æ¡ˆæ–‡æœ¬
        """
        return self.generate_caption(image_path, question, max_length)
    
    def multi_image_understanding(
        self,
        image_paths: List[str],
        prompt: str,
        max_length: int = 512
    ) -> str:
        """
        å¤šå›¾ç†è§£
        
        å‚æ•°:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            prompt: æç¤ºæ–‡æœ¬
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        è¿”å›:
            ç†è§£ç»“æœ
        """
        # æ„å»ºå¤šå›¾æŸ¥è¯¢
        query_list = []
        for img_path in image_paths:
            if not os.path.exists(img_path):
                raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
            query_list.append({'image': img_path})
        
        query_list.append({'text': prompt})
        
        query = self.tokenizer.from_list_format(query_list)
        
        # ç”Ÿæˆå“åº”
        response, _ = self.model.chat(
            self.tokenizer,
            query=query,
            history=None,
            max_length=max_length
        )
        
        return response
    
    def ocr_recognition(
        self,
        image_path: str,
        prompt: str = "è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—",
        max_length: int = 512
    ) -> str:
        """
        OCRæ–‡å­—è¯†åˆ«
        
        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            prompt: æç¤ºæ–‡æœ¬
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        è¿”å›:
            è¯†åˆ«çš„æ–‡å­—
        """
        return self.generate_caption(image_path, prompt, max_length)
    
    def chat(
        self,
        image_path: str,
        history: Optional[List] = None,
        max_length: int = 256
    ) -> tuple:
        """
        å¤šè½®å¯¹è¯
        
        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            history: å¯¹è¯å†å²
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        è¿”å›:
            (å“åº”, æ–°çš„å†å²)
        """
        # æ„å»ºæŸ¥è¯¢
        query = self.tokenizer.from_list_format([
            {'image': image_path},
        ])
        
        # è¿›è¡Œå¯¹è¯
        response, history = self.model.chat(
            self.tokenizer,
            query=query,
            history=history,
            max_length=max_length
        )
        
        return response, history


def demo_caption_generation(model: QwenVLInference, image_path: str):
    """æ¼”ç¤ºï¼šå›¾åƒæè¿°ç”Ÿæˆ"""
    print("\n" + "="*60)
    print("ğŸ“ æ¼”ç¤º1: å›¾åƒæè¿°ç”Ÿæˆ")
    print("="*60)
    
    prompts = [
        "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡",
        "ç”¨ä¸€å¥è¯æ¦‚æ‹¬å›¾ç‰‡å†…å®¹",
        "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆç‰©ä½“ï¼Ÿ"
    ]
    
    for prompt in prompts:
        print(f"\nâ“ æç¤º: {prompt}")
        try:
            caption = model.generate_caption(image_path, prompt)
            print(f"ğŸ’¬ å›ç­”: {caption}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def demo_vqa(model: QwenVLInference, image_path: str):
    """æ¼”ç¤ºï¼šè§†è§‰é—®ç­”"""
    print("\n" + "="*60)
    print("â“ æ¼”ç¤º2: è§†è§‰é—®ç­”ï¼ˆVQAï¼‰")
    print("="*60)
    
    questions = [
        "å›¾ç‰‡ä¸­æœ‰å¤šå°‘äººï¼Ÿ",
        "è¿™æ˜¯ä»€ä¹ˆåœºæ™¯ï¼Ÿ",
        "å›¾ç‰‡çš„ä¸»è¦é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å›¾ç‰‡æ‹æ‘„çš„æ˜¯ç™½å¤©è¿˜æ˜¯æ™šä¸Šï¼Ÿ"
    ]
    
    for question in questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        try:
            answer = model.visual_question_answering(image_path, question)
            print(f"ğŸ’¬ å›ç­”: {answer}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def demo_ocr(model: QwenVLInference, image_path: str):
    """æ¼”ç¤ºï¼šOCRæ–‡å­—è¯†åˆ«"""
    print("\n" + "="*60)
    print("ğŸ” æ¼”ç¤º3: OCRæ–‡å­—è¯†åˆ«")
    print("="*60)
    
    prompts = [
        "è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—",
        "æå–å›¾ç‰‡ä¸­çš„ä¸­æ–‡æ–‡æœ¬",
        "å›¾ç‰‡ä¸­æœ‰å“ªäº›æ•°å­—ï¼Ÿ"
    ]
    
    for prompt in prompts:
        print(f"\nâ“ æç¤º: {prompt}")
        try:
            text = model.ocr_recognition(image_path, prompt)
            print(f"ğŸ’¬ è¯†åˆ«ç»“æœ: {text}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def demo_multi_image(model: QwenVLInference, image_paths: List[str]):
    """æ¼”ç¤ºï¼šå¤šå›¾ç†è§£"""
    print("\n" + "="*60)
    print("ğŸ–¼ï¸  æ¼”ç¤º4: å¤šå›¾ç†è§£")
    print("="*60)
    
    if len(image_paths) < 2:
        print("âš ï¸  éœ€è¦è‡³å°‘2å¼ å›¾ç‰‡è¿›è¡Œå¤šå›¾ç†è§£æ¼”ç¤º")
        return
    
    prompts = [
        "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„å¼‚åŒ",
        "è¿™äº›å›¾ç‰‡æœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ",
        "æŒ‰ç…§æ—¶é—´é¡ºåºæè¿°è¿™äº›å›¾ç‰‡"
    ]
    
    for prompt in prompts:
        print(f"\nâ“ æç¤º: {prompt}")
        try:
            result = model.multi_image_understanding(image_paths[:2], prompt)
            print(f"ğŸ’¬ å›ç­”: {result}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def demo_chat(model: QwenVLInference, image_path: str):
    """æ¼”ç¤ºï¼šå¤šè½®å¯¹è¯"""
    print("\n" + "="*60)
    print("ğŸ’­ æ¼”ç¤º5: å¤šè½®å¯¹è¯")
    print("="*60)
    
    conversations = [
        "è¿™æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å®ƒçš„é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å®ƒé€šå¸¸ç”¨æ¥åšä»€ä¹ˆï¼Ÿ"
    ]
    
    history = None
    for i, question in enumerate(conversations, 1):
        print(f"\nç¬¬{i}è½®å¯¹è¯:")
        print(f"â“ ç”¨æˆ·: {question}")
        try:
            # æ„å»ºæŸ¥è¯¢
            if history is None:
                query = model.tokenizer.from_list_format([
                    {'image': image_path},
                    {'text': question},
                ])
            else:
                query = question
            
            response, history = model.model.chat(
                model.tokenizer,
                query=query,
                history=history
            )
            print(f"ğŸ’¬ Qwen-VL: {response}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            break


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Qwen-VLæ¨ç†ç¤ºä¾‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
    # åŸºç¡€æ¨ç†
    python qwen_vl_inference.py --image path/to/image.jpg
    
    # æŒ‡å®šæ¨¡å‹
    python qwen_vl_inference.py --image image.jpg --model Qwen/Qwen-VL-Chat
    
    # CPUæ¨¡å¼
    python qwen_vl_inference.py --image image.jpg --device cpu
    
    # å¤šå›¾ç†è§£
    python qwen_vl_inference.py --images img1.jpg img2.jpg --demo multi_image
    
    # ä»…è¿è¡Œç‰¹å®šæ¼”ç¤º
    python qwen_vl_inference.py --image image.jpg --demo caption
        """
    )
    
    parser.add_argument(
        "--image",
        type=str,
        help="è¾“å…¥å›¾åƒè·¯å¾„"
    )
    parser.add_argument(
        "--images",
        nargs="+",
        help="å¤šä¸ªå›¾åƒè·¯å¾„ï¼ˆç”¨äºå¤šå›¾ç†è§£ï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="Qwen/Qwen-VL-Chat",
        help="æ¨¡å‹åç§°æˆ–è·¯å¾„ (é»˜è®¤: Qwen/Qwen-VL-Chat)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cuda", "cpu"],
        help="è¿è¡Œè®¾å¤‡ (é»˜è®¤: auto)"
    )
    parser.add_argument(
        "--demo",
        type=str,
        choices=["all", "caption", "vqa", "ocr", "multi_image", "chat"],
        default="all",
        help="è¿è¡Œçš„æ¼”ç¤º (é»˜è®¤: all)"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥
    if not args.image and not args.images:
        parser.error("è¯·æä¾› --image æˆ– --images å‚æ•°")
    
    image_path = args.image or args.images[0]
    image_paths = args.images or [args.image]
    
    # æ£€æŸ¥å›¾åƒæ–‡ä»¶
    for img_path in image_paths:
        if not os.path.exists(img_path):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
            return
    
    print("ğŸ¨ Qwen-VL æ¨ç†ç¤ºä¾‹")
    print("="*60)
    print(f"ğŸ“ å›¾åƒ: {', '.join(image_paths)}")
    print(f"ğŸ¤– æ¨¡å‹: {args.model}")
    print(f"ğŸ’» è®¾å¤‡: {args.device}")
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = QwenVLInference(
            model_name=args.model,
            device=args.device
        )
        
        # è¿è¡Œæ¼”ç¤º
        if args.demo == "all" or args.demo == "caption":
            demo_caption_generation(model, image_path)
        
        if args.demo == "all" or args.demo == "vqa":
            demo_vqa(model, image_path)
        
        if args.demo == "all" or args.demo == "ocr":
            demo_ocr(model, image_path)
        
        if args.demo == "all" or args.demo == "multi_image":
            demo_multi_image(model, image_paths)
        
        if args.demo == "all" or args.demo == "chat":
            demo_chat(model, image_path)
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

