"""
BLIP-2æ¨ç†ç¤ºä¾‹

æœ¬è„šæœ¬æ¼”ç¤ºBLIP-2æ¨¡å‹çš„å„ç§æ¨ç†åœºæ™¯ï¼š
- å›¾åƒæè¿° (Image Captioning)
- è§†è§‰é—®ç­” (Visual Question Answering)
- å›¾åƒ-æ–‡æœ¬æ£€ç´¢ (Image-Text Retrieval)
- æ‰¹é‡æ¨ç†
- æ€§èƒ½è¯„ä¼°

æ”¯æŒçš„æ¨¡å‹ï¼š
- Salesforce/blip2-opt-2.7b
- Salesforce/blip2-opt-6.7b
- Salesforce/blip2-flan-t5-xl
- Salesforce/blip2-flan-t5-xxl

ä½œè€…: Large-Model-Tutorial
æ—¥æœŸ: 2025-11-02
"""

import argparse
import os
import sys
import time
import json
from pathlib import Path
from typing import List, Dict, Optional, Union, Tuple
import warnings

import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np
from tqdm import tqdm

# å¯¼å…¥transformers
try:
    from transformers import Blip2Processor, Blip2ForConditionalGeneration
except ImportError:
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£…transformersåº“")
    print("   å®‰è£…æ–¹æ³•: pip install transformers")
    sys.exit(1)

# å¯é€‰ä¾èµ–
try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: matplotlibæœªå®‰è£…ï¼Œå¯è§†åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")

warnings.filterwarnings('ignore')


class BLIP2InferenceService:
    """BLIP-2æ¨ç†æœåŠ¡ç±»"""
    
    SUPPORTED_MODELS = {
        'opt-2.7b': 'Salesforce/blip2-opt-2.7b',
        'opt-6.7b': 'Salesforce/blip2-opt-6.7b',
        'flan-t5-xl': 'Salesforce/blip2-flan-t5-xl',
        'flan-t5-xxl': 'Salesforce/blip2-flan-t5-xxl',
    }
    
    def __init__(
        self,
        model_name: str = 'opt-2.7b',
        device: Optional[str] = None,
        torch_dtype: str = 'float16',
        cache_dir: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–BLIP-2æ¨ç†æœåŠ¡
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡ ('cuda', 'cpu', æˆ–Noneè‡ªåŠ¨é€‰æ‹©)
            torch_dtype: æ•°æ®ç±»å‹ ('float16' æˆ– 'float32')
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        self.model_name = model_name
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.dtype = torch.float16 if torch_dtype == 'float16' else torch.float32
        self.cache_dir = cache_dir
        
        print(f"ğŸš€ åˆå§‹åŒ–BLIP-2æ¨ç†æœåŠ¡")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   æ•°æ®ç±»å‹: {torch_dtype}")
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        # è·å–æ¨¡å‹è·¯å¾„
        if self.model_name in self.SUPPORTED_MODELS:
            model_path = self.SUPPORTED_MODELS[self.model_name]
        else:
            model_path = self.model_name
        
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
        start_time = time.time()
        
        try:
            # åŠ è½½å¤„ç†å™¨
            self.processor = Blip2Processor.from_pretrained(
                model_path,
                cache_dir=self.cache_dir
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = Blip2ForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=self.dtype,
                cache_dir=self.cache_dir
            )
            self.model.to(self.device)
            self.model.eval()
            
            load_time = time.time() - start_time
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}ç§’)")
            
            # æ‰“å°æ¨¡å‹ä¿¡æ¯
            self._print_model_info()
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print("\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   æ€»å‚æ•°: {total_params / 1e9:.2f}B")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e9:.2f}B")
        
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3
            print(f"   æ˜¾å­˜å ç”¨: {memory_allocated:.2f}GB")
    
    def generate_caption(
        self,
        image: Union[str, Image.Image],
        prompt: Optional[str] = None,
        max_new_tokens: int = 50,
        **generate_kwargs
    ) -> str:
        """
        ç”Ÿæˆå›¾åƒæè¿°
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            prompt: å¯é€‰çš„æç¤ºæ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            **generate_kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æè¿°æ–‡æœ¬
        """
        # åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # å¤„ç†è¾“å…¥
        if prompt:
            inputs = self.processor(images=image, text=prompt, return_tensors="pt")
        else:
            inputs = self.processor(images=image, return_tensors="pt")
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                **generate_kwargs
            )
        
        # è§£ç 
        generated_text = self.processor.batch_decode(
            generated_ids,
            skip_special_tokens=True
        )[0].strip()
        
        return generated_text
    
    def visual_question_answering(
        self,
        image: Union[str, Image.Image],
        question: str,
        max_new_tokens: int = 30,
        **generate_kwargs
    ) -> str:
        """
        è§†è§‰é—®ç­”
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            question: é—®é¢˜æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            **generate_kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            ç­”æ¡ˆæ–‡æœ¬
        """
        # æ„å»ºæç¤º
        prompt = f"Question: {question} Answer:"
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_caption(
            image=image,
            prompt=prompt,
            max_new_tokens=max_new_tokens,
            **generate_kwargs
        )
        
        # æ¸…ç†ç­”æ¡ˆï¼ˆç§»é™¤é‡å¤çš„promptï¼‰
        if answer.startswith(prompt):
            answer = answer[len(prompt):].strip()
        
        return answer
    
    def batch_inference(
        self,
        images: List[Union[str, Image.Image]],
        prompts: Optional[List[str]] = None,
        batch_size: int = 4,
        max_new_tokens: int = 50,
        **generate_kwargs
    ) -> List[str]:
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            images: å›¾åƒåˆ—è¡¨
            prompts: æç¤ºåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            batch_size: æ‰¹å¤§å°
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            **generate_kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        results = []
        
        # å¦‚æœæ²¡æœ‰æä¾›promptsï¼Œä½¿ç”¨Noneåˆ—è¡¨
        if prompts is None:
            prompts = [None] * len(images)
        
        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(images), batch_size), desc="æ‰¹é‡æ¨ç†"):
            batch_images = images[i:i+batch_size]
            batch_prompts = prompts[i:i+batch_size]
            
            # åŠ è½½å›¾åƒ
            pil_images = []
            for img in batch_images:
                if isinstance(img, str):
                    pil_images.append(Image.open(img).convert('RGB'))
                else:
                    pil_images.append(img)
            
            # å¤„ç†è¾“å…¥
            if any(p is not None for p in batch_prompts):
                # æœ‰æç¤ºçš„æƒ…å†µ
                inputs = self.processor(
                    images=pil_images,
                    text=batch_prompts,
                    return_tensors="pt",
                    padding=True
                )
            else:
                # æ— æç¤ºçš„æƒ…å†µ
                inputs = self.processor(
                    images=pil_images,
                    return_tensors="pt"
                )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆ
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    **generate_kwargs
                )
            
            # è§£ç 
            texts = self.processor.batch_decode(
                generated_ids,
                skip_special_tokens=True
            )
            
            results.extend([t.strip() for t in texts])
        
        return results
    
    def extract_features(
        self,
        image: Union[str, Image.Image],
        text: Optional[str] = None
    ) -> Dict[str, torch.Tensor]:
        """
        æå–å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            text: å¯é€‰çš„æ–‡æœ¬
        
        Returns:
            ç‰¹å¾å­—å…¸ï¼ŒåŒ…å«'image_embeds'å’Œå¯é€‰çš„'text_embeds'
        """
        # åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # å¤„ç†è¾“å…¥
        if text:
            inputs = self.processor(images=image, text=text, return_tensors="pt")
        else:
            inputs = self.processor(images=image, return_tensors="pt")
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
        
        # è¿”å›ç‰¹å¾
        features = {}
        
        # å›¾åƒç‰¹å¾ï¼ˆQ-Formerè¾“å‡ºï¼‰
        if hasattr(outputs, 'vision_outputs'):
            features['image_embeds'] = outputs.vision_outputs[0][:, 0, :]  # [CLS] token
        
        # æ–‡æœ¬ç‰¹å¾ï¼ˆå¦‚æœæä¾›ï¼‰
        if text and hasattr(outputs, 'language_model_outputs'):
            features['text_embeds'] = outputs.language_model_outputs.last_hidden_state.mean(dim=1)
        
        return features
    
    def compute_similarity(
        self,
        image: Union[str, Image.Image],
        text: str
    ) -> float:
        """
        è®¡ç®—å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦
        
        Args:
            image: å›¾åƒè·¯å¾„æˆ–PIL Imageå¯¹è±¡
            text: æ–‡æœ¬
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        features = self.extract_features(image, text)
        
        if 'image_embeds' in features and 'text_embeds' in features:
            image_embed = F.normalize(features['image_embeds'], dim=-1)
            text_embed = F.normalize(features['text_embeds'], dim=-1)
            
            similarity = (image_embed * text_embed).sum().item()
            return (similarity + 1) / 2  # å½’ä¸€åŒ–åˆ°0-1
        else:
            print("âš ï¸  è­¦å‘Š: æ— æ³•æå–ç‰¹å¾ï¼Œè¿”å›0")
            return 0.0
    
    def benchmark(
        self,
        image: Union[str, Image.Image],
        prompt: Optional[str] = None,
        num_runs: int = 10
    ) -> Dict[str, float]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            image: æµ‹è¯•å›¾åƒ
            prompt: æµ‹è¯•æç¤º
            num_runs: è¿è¡Œæ¬¡æ•°
        
        Returns:
            æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        # åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # å‡†å¤‡è¾“å…¥
        if prompt:
            inputs = self.processor(images=image, text=prompt, return_tensors="pt")
        else:
            inputs = self.processor(images=image, return_tensors="pt")
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # é¢„çƒ­
        print("ğŸ”¥ é¢„çƒ­ä¸­...")
        for _ in range(3):
            with torch.no_grad():
                _ = self.model.generate(**inputs, max_new_tokens=20)
        
        # æµ‹è¯•
        print(f"â±ï¸  è¿è¡ŒåŸºå‡†æµ‹è¯• ({num_runs}æ¬¡)...")
        times = []
        
        for i in range(num_runs):
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            start_time = time.time()
            
            with torch.no_grad():
                _ = self.model.generate(**inputs, max_new_tokens=50)
            
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            elapsed = time.time() - start_time
            times.append(elapsed)
            
            print(f"   è¿è¡Œ {i+1}/{num_runs}: {elapsed*1000:.2f}ms")
        
        # ç»Ÿè®¡
        times = np.array(times)
        stats = {
            'mean_ms': float(times.mean() * 1000),
            'std_ms': float(times.std() * 1000),
            'min_ms': float(times.min() * 1000),
            'max_ms': float(times.max() * 1000),
            'throughput_imgs_per_sec': float(1.0 / times.mean()),
        }
        
        # æ˜¾å­˜ç»Ÿè®¡
        if torch.cuda.is_available():
            stats['memory_allocated_gb'] = torch.cuda.memory_allocated(self.device) / 1024**3
            stats['memory_reserved_gb'] = torch.cuda.max_memory_reserved(self.device) / 1024**3
        
        return stats


# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description='BLIP-2æ¨ç†ç¤ºä¾‹')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, default='opt-2.7b',
                        choices=list(BLIP2InferenceService.SUPPORTED_MODELS.keys()),
                        help='æ¨¡å‹åç§°')
    parser.add_argument('--device', type=str, default=None,
                        help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--dtype', type=str, default='float16',
                        choices=['float16', 'float32'],
                        help='æ•°æ®ç±»å‹')
    parser.add_argument('--cache-dir', type=str, default=None,
                        help='æ¨¡å‹ç¼“å­˜ç›®å½•')
    
    # ä»»åŠ¡å‚æ•°
    parser.add_argument('--task', type=str, default='caption',
                        choices=['caption', 'vqa', 'batch', 'similarity', 'benchmark'],
                        help='ä»»åŠ¡ç±»å‹')
    parser.add_argument('--image', type=str, required=True,
                        help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--prompt', type=str, default=None,
                        help='æç¤ºæ–‡æœ¬')
    parser.add_argument('--question', type=str, default=None,
                        help='VQAé—®é¢˜')
    parser.add_argument('--text', type=str, default=None,
                        help='ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„æ–‡æœ¬')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--max-new-tokens', type=int, default=50,
                        help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    parser.add_argument('--num-beams', type=int, default=1,
                        help='æŸæœç´¢å¤§å°')
    parser.add_argument('--temperature', type=float, default=1.0,
                        help='é‡‡æ ·æ¸©åº¦')
    parser.add_argument('--top-p', type=float, default=1.0,
                        help='æ ¸é‡‡æ ·å‚æ•°')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--output', type=str, default=None,
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--benchmark-runs', type=int, default=10,
                        help='åŸºå‡†æµ‹è¯•è¿è¡Œæ¬¡æ•°')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æœåŠ¡
    service = BLIP2InferenceService(
        model_name=args.model,
        device=args.device,
        torch_dtype=args.dtype,
        cache_dir=args.cache_dir
    )
    
    # ç”Ÿæˆå‚æ•°
    generate_kwargs = {
        'num_beams': args.num_beams,
        'temperature': args.temperature,
        'top_p': args.top_p,
    }
    
    # æ‰§è¡Œä»»åŠ¡
    print(f"\nğŸ¯ æ‰§è¡Œä»»åŠ¡: {args.task}")
    
    if args.task == 'caption':
        # å›¾åƒæè¿°
        caption = service.generate_caption(
            image=args.image,
            prompt=args.prompt,
            max_new_tokens=args.max_new_tokens,
            **generate_kwargs
        )
        print(f"\nğŸ“ ç”Ÿæˆçš„æè¿°:")
        print(f"   {caption}")
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(caption)
            print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {args.output}")
    
    elif args.task == 'vqa':
        # è§†è§‰é—®ç­”
        if not args.question:
            print("âŒ é”™è¯¯: VQAä»»åŠ¡éœ€è¦æä¾›--questionå‚æ•°")
            return
        
        answer = service.visual_question_answering(
            image=args.image,
            question=args.question,
            max_new_tokens=args.max_new_tokens,
            **generate_kwargs
        )
        print(f"\nâ“ é—®é¢˜: {args.question}")
        print(f"ğŸ’¡ ç­”æ¡ˆ: {answer}")
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump({'question': args.question, 'answer': answer}, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {args.output}")
    
    elif args.task == 'similarity':
        # å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦
        if not args.text:
            print("âŒ é”™è¯¯: similarityä»»åŠ¡éœ€è¦æä¾›--textå‚æ•°")
            return
        
        similarity = service.compute_similarity(
            image=args.image,
            text=args.text
        )
        print(f"\nğŸ“Š ç›¸ä¼¼åº¦: {similarity:.4f}")
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump({'image': args.image, 'text': args.text, 'similarity': similarity}, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {args.output}")
    
    elif args.task == 'benchmark':
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        stats = service.benchmark(
            image=args.image,
            prompt=args.prompt,
            num_runs=args.benchmark_runs
        )
        
        print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(f"   å¹³å‡å»¶è¿Ÿ: {stats['mean_ms']:.2f}ms (Â±{stats['std_ms']:.2f}ms)")
        print(f"   æœ€å°/æœ€å¤§: {stats['min_ms']:.2f}ms / {stats['max_ms']:.2f}ms")
        print(f"   ååé‡: {stats['throughput_imgs_per_sec']:.2f} images/sec")
        
        if 'memory_allocated_gb' in stats:
            print(f"   æ˜¾å­˜å ç”¨: {stats['memory_allocated_gb']:.2f}GB")
            print(f"   æ˜¾å­˜å³°å€¼: {stats['memory_reserved_gb']:.2f}GB")
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {args.output}")
    
    print("\nâœ… å®Œæˆï¼")


if __name__ == '__main__':
    main()

