#!/usr/bin/env python3
"""
InternVL æ¨ç†ç¤ºä¾‹

InternVLæ˜¯ä¸Šæµ·AI Labå¼€å‘çš„å¼ºå¤§è§†è§‰è¯­è¨€æ¨¡å‹,æ¥è¿‘GPT-4Vçš„æ€§èƒ½è¡¨ç°ã€‚
InternVL3æ˜¯æœ€æ–°ç‰ˆæœ¬,æ”¯æŒå¤šç§è§†è§‰ä»»åŠ¡å’Œå¤šè¯­è¨€å¯¹è¯ã€‚

åŠŸèƒ½:
- å›¾åƒæè¿°ç”Ÿæˆ
- è§†è§‰é—®ç­”(VQA)
- OCRæ–‡å­—è¯†åˆ«
- å¤šå›¾ç†è§£
- å¤šè½®å¯¹è¯
- è§†é¢‘ç†è§£

æ¨¡å‹ç‰ˆæœ¬:
- InternVL2-8B: å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
- InternVL3-1B: è½»é‡çº§ç‰ˆæœ¬
- InternVL3-8B: é«˜æ€§èƒ½ç‰ˆæœ¬
- InternVL3-78B: æ——èˆ°ç‰ˆæœ¬

ä¾èµ–:
    pip install transformers>=4.37.2 torch torchvision pillow
    pip install accelerate

ä½œè€…: Large-Model-Tutorial Team
æ—¥æœŸ: 2025-11-10
ç‰ˆæœ¬: v1.1.0
"""

import argparse
import os
from typing import List, Optional, Dict, Any, Union
from pathlib import Path

try:
    import torch
    from transformers import AutoModel, AutoProcessor, AutoModelForImageTextToText
    from PIL import Image
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–: {e}")
    print("è¯·å®‰è£…: pip install torch transformers pillow accelerate")
    exit(1)


class InternVLInference:
    """InternVL æ¨ç†ç±»"""

    def __init__(
        self,
        model_name: str = "OpenGVLab/InternVL2-8B",
        device: str = "auto",
        torch_dtype: str = "bfloat16"
    ):
        """
        åˆå§‹åŒ–InternVLæ¨¡å‹

        å‚æ•°:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
                - OpenGVLab/InternVL2-8B (æ¨è)
                - OpenGVLab/InternVL3-1B (è½»é‡çº§)
                - OpenGVLab/InternVL3-8B (é«˜æ€§èƒ½)
            device: è®¾å¤‡ ('cuda', 'cpu', 'auto')
            torch_dtype: æ•°æ®ç±»å‹ ('bfloat16', 'float16', 'float32')
        """
        self.model_name = model_name
        self.device = self._setup_device(device)

        print(f"ğŸš€ åŠ è½½InternVLæ¨¡å‹: {model_name}")
        print(f"ğŸ“ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # è®¾ç½®æ•°æ®ç±»å‹ - æ ¹æ®è®¾å¤‡å’Œç”¨æˆ·é€‰æ‹©è‡ªåŠ¨è°ƒæ•´
        self.dtype = self._setup_dtype(torch_dtype, self.device)

        try:
            # åŠ è½½processor
            self.processor = AutoProcessor.from_pretrained(
                model_name,
                trust_remote_code=True
            )

            # åŠ è½½æ¨¡å‹
            # ä½¿ç”¨AutoModelForImageTextToText (Transformers >= 4.37.2)
            self.model = AutoModelForImageTextToText.from_pretrained(
                model_name,
                device_map=device if device == "auto" else None,
                torch_dtype=self.dtype,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            ).eval()

            if device != "auto" and device != "cpu":
                self.model = self.model.to(self.device)

            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\nğŸ’¡ æç¤º:")
            print("   1. ç¡®ä¿å·²å®‰è£…: pip install transformers>=4.37.2 accelerate")
            print("   2. é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹(çº¦16GB),è¯·è€å¿ƒç­‰å¾…")
            print("   3. ç¡®ä¿ç½‘ç»œç•…é€šæˆ–é…ç½®HuggingFaceé•œåƒ")
            print("   4. æ¨èä½¿ç”¨GPUè¿è¡Œ,è‡³å°‘éœ€è¦16GBæ˜¾å­˜")
            raise

    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è¿è¡Œè®¾å¤‡"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device

    def _setup_dtype(self, torch_dtype: str, device: str) -> torch.dtype:
        """
        è®¾ç½®æ•°æ®ç±»å‹ï¼Œæ ¹æ®è®¾å¤‡å’Œå…¼å®¹æ€§è‡ªåŠ¨è°ƒæ•´

        å‚æ•°:
            torch_dtype: ç”¨æˆ·æŒ‡å®šçš„æ•°æ®ç±»å‹
            device: è¿è¡Œè®¾å¤‡

        è¿”å›:
            torch.dtypeå¯¹è±¡
        """
        # CPUè®¾å¤‡å¿…é¡»ä½¿ç”¨float32
        if device == "cpu":
            if torch_dtype != "float32":
                print(f"âš ï¸  CPUè®¾å¤‡ä¸æ”¯æŒ{torch_dtype}ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°float32")
            print(f"ğŸ’» ä½¿ç”¨ç²¾åº¦: Float32 (CPUæ¨¡å¼)")
            return torch.float32

        # GPUè®¾å¤‡æ ¹æ®ç”¨æˆ·é€‰æ‹©
        if torch_dtype == "bfloat16":
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒBFloat16
            if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                print(f"âš¡ ä½¿ç”¨ç²¾åº¦: BFloat16 (æ¨è)")
                return torch.bfloat16
            else:
                print(f"âš ï¸  GPUä¸æ”¯æŒBFloat16ï¼Œåˆ‡æ¢åˆ°Float16")
                print(f"âš¡ ä½¿ç”¨ç²¾åº¦: Float16")
                return torch.float16
        elif torch_dtype == "float16":
            print(f"âš¡ ä½¿ç”¨ç²¾åº¦: Float16")
            return torch.float16
        else:
            print(f"ğŸ’» ä½¿ç”¨ç²¾åº¦: Float32")
            return torch.float32

    def _prepare_messages(
        self,
        image: Union[str, Image.Image, List[Union[str, Image.Image]]],
        text: str
    ) -> List[Dict[str, Any]]:
        """
        å‡†å¤‡èŠå¤©æ¶ˆæ¯æ ¼å¼

        å‚æ•°:
            image: å•å¼ å›¾ç‰‡æˆ–å›¾ç‰‡åˆ—è¡¨
            text: æ–‡æœ¬æç¤º

        è¿”å›:
            æ ¼å¼åŒ–çš„æ¶ˆæ¯åˆ—è¡¨
        """
        # å¤„ç†å›¾ç‰‡
        if isinstance(image, (str, Image.Image)):
            images = [image]
        else:
            images = image

        # åŠ è½½å›¾ç‰‡
        pil_images = []
        for img in images:
            if isinstance(img, str):
                if not os.path.exists(img):
                    raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img}")
                pil_images.append(Image.open(img).convert('RGB'))
            else:
                pil_images.append(img)

        # æ„å»ºæ¶ˆæ¯
        # InternVLä½¿ç”¨ç‰¹æ®Šçš„æ¶ˆæ¯æ ¼å¼
        content = []
        for _ in pil_images:
            content.append({"type": "image"})
        content.append({"type": "text", "text": text})

        messages = [
            {
                "role": "user",
                "content": content
            }
        ]

        return messages, pil_images

    def generate(
        self,
        image: Union[str, Image.Image, List[Union[str, Image.Image]]],
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ) -> str:
        """
        é€šç”¨ç”Ÿæˆæ–¹æ³•

        å‚æ•°:
            image: å•å¼ å›¾ç‰‡è·¯å¾„/å¯¹è±¡æˆ–å›¾ç‰‡åˆ—è¡¨
            prompt: æ–‡æœ¬æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus samplingå‚æ•°

        è¿”å›:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # å‡†å¤‡æ¶ˆæ¯
        messages, pil_images = self._prepare_messages(image, prompt)

        # åº”ç”¨èŠå¤©æ¨¡æ¿
        prompt_text = self.processor.apply_chat_template(
            messages,
            add_generation_prompt=True
        )

        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=prompt_text,
            images=pil_images,
            return_tensors="pt",
            padding=True
        )

        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device != "cpu":
            inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                     for k, v in inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                **kwargs
            )

        # è§£ç 
        generated_text = self.processor.batch_decode(
            outputs,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )[0]

        # æå–å›ç­”éƒ¨åˆ†(ç§»é™¤prompt)
        if prompt in generated_text:
            answer = generated_text.split(prompt)[-1].strip()
        else:
            # å°è¯•æå–assistantçš„å›ç­”
            if "assistant\n" in generated_text:
                answer = generated_text.split("assistant\n")[-1].strip()
            else:
                answer = generated_text.strip()

        return answer

    def generate_caption(
        self,
        image_path: str,
        prompt: str = "Please describe this image in detail.",
        max_new_tokens: int = 256
    ) -> str:
        """
        ç”Ÿæˆå›¾åƒæè¿°

        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            prompt: æç¤ºæ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦

        è¿”å›:
            ç”Ÿæˆçš„æè¿°æ–‡æœ¬
        """
        return self.generate(image_path, prompt, max_new_tokens)

    def visual_question_answering(
        self,
        image_path: str,
        question: str,
        max_new_tokens: int = 256
    ) -> str:
        """
        è§†è§‰é—®ç­”

        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            question: é—®é¢˜æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦

        è¿”å›:
            ç­”æ¡ˆæ–‡æœ¬
        """
        return self.generate(image_path, question, max_new_tokens)

    def multi_image_understanding(
        self,
        image_paths: List[str],
        prompt: str,
        max_new_tokens: int = 512
    ) -> str:
        """
        å¤šå›¾ç†è§£

        å‚æ•°:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            prompt: æç¤ºæ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦

        è¿”å›:
            ç†è§£ç»“æœ
        """
        return self.generate(image_paths, prompt, max_new_tokens)

    def ocr_recognition(
        self,
        image_path: str,
        prompt: str = "Please extract all text from this image.",
        max_new_tokens: int = 512
    ) -> str:
        """
        OCRæ–‡å­—è¯†åˆ«

        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            prompt: æç¤ºæ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦

        è¿”å›:
            è¯†åˆ«çš„æ–‡å­—
        """
        return self.generate(image_path, prompt, max_new_tokens)

    def chat(
        self,
        messages: List[Dict[str, Any]],
        images: List[Union[str, Image.Image]],
        max_new_tokens: int = 512
    ) -> str:
        """
        å¤šè½®å¯¹è¯

        å‚æ•°:
            messages: å¯¹è¯å†å²æ¶ˆæ¯åˆ—è¡¨
            images: å›¾åƒåˆ—è¡¨
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦

        è¿”å›:
            æ¨¡å‹å›å¤
        """
        # åŠ è½½å›¾ç‰‡
        pil_images = []
        for img in images:
            if isinstance(img, str):
                if not os.path.exists(img):
                    raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img}")
                pil_images.append(Image.open(img).convert('RGB'))
            else:
                pil_images.append(img)

        # åº”ç”¨èŠå¤©æ¨¡æ¿
        prompt_text = self.processor.apply_chat_template(
            messages,
            add_generation_prompt=True
        )

        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=prompt_text,
            images=pil_images,
            return_tensors="pt",
            padding=True
        )

        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device != "cpu":
            inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                     for k, v in inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True
            )

        # è§£ç 
        generated_text = self.processor.batch_decode(
            outputs,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )[0]

        # æå–æœ€åçš„å›ç­”
        if "assistant\n" in generated_text:
            answer = generated_text.split("assistant\n")[-1].strip()
        else:
            answer = generated_text.strip()

        return answer


def demo_caption_generation(model: InternVLInference, image_path: str):
    """æ¼”ç¤º:å›¾åƒæè¿°ç”Ÿæˆ"""
    print("\n" + "="*60)
    print("ğŸ“ æ¼”ç¤º1: å›¾åƒæè¿°ç”Ÿæˆ")
    print("="*60)

    prompts = [
        "Please describe this image in detail.",
        "What is the main subject of this image?",
        "Summarize this image in one sentence."
    ]

    for prompt in prompts:
        print(f"\nâ“ Prompt: {prompt}")
        try:
            caption = model.generate_caption(image_path, prompt)
            print(f"ğŸ’¬ Response: {caption}")
        except Exception as e:
            print(f"âŒ Error: {e}")


def demo_vqa(model: InternVLInference, image_path: str):
    """æ¼”ç¤º:è§†è§‰é—®ç­”"""
    print("\n" + "="*60)
    print("â“ æ¼”ç¤º2: è§†è§‰é—®ç­”(VQA)")
    print("="*60)

    questions = [
        "How many people are in this image?",
        "What is the weather like in this image?",
        "What colors are dominant in this image?",
        "Is this image taken during day or night?"
    ]

    for question in questions:
        print(f"\nâ“ Question: {question}")
        try:
            answer = model.visual_question_answering(image_path, question)
            print(f"ğŸ’¬ Answer: {answer}")
        except Exception as e:
            print(f"âŒ Error: {e}")


def demo_ocr(model: InternVLInference, image_path: str):
    """æ¼”ç¤º:OCRæ–‡å­—è¯†åˆ«"""
    print("\n" + "="*60)
    print("ğŸ” æ¼”ç¤º3: OCRæ–‡å­—è¯†åˆ«")
    print("="*60)

    prompts = [
        "Please extract all text from this image.",
        "What text can you see in this image?",
        "List all the words visible in this image."
    ]

    for prompt in prompts:
        print(f"\nâ“ Prompt: {prompt}")
        try:
            text = model.ocr_recognition(image_path, prompt)
            print(f"ğŸ’¬ Extracted Text: {text}")
        except Exception as e:
            print(f"âŒ Error: {e}")


def demo_multi_image(model: InternVLInference, image_paths: List[str]):
    """æ¼”ç¤º:å¤šå›¾ç†è§£"""
    print("\n" + "="*60)
    print("ğŸ–¼ï¸  æ¼”ç¤º4: å¤šå›¾ç†è§£")
    print("="*60)

    if len(image_paths) < 2:
        print("âš ï¸  éœ€è¦è‡³å°‘2å¼ å›¾ç‰‡è¿›è¡Œå¤šå›¾ç†è§£æ¼”ç¤º")
        return

    prompts = [
        "Compare and contrast these images.",
        "What do these images have in common?",
        "Describe the relationship between these images."
    ]

    for prompt in prompts:
        print(f"\nâ“ Prompt: {prompt}")
        try:
            result = model.multi_image_understanding(image_paths[:2], prompt)
            print(f"ğŸ’¬ Response: {result}")
        except Exception as e:
            print(f"âŒ Error: {e}")


def demo_chat(model: InternVLInference, image_path: str):
    """æ¼”ç¤º:å¤šè½®å¯¹è¯"""
    print("\n" + "="*60)
    print("ğŸ’­ æ¼”ç¤º5: å¤šè½®å¯¹è¯")
    print("="*60)

    # åŠ è½½å›¾ç‰‡
    image = Image.open(image_path).convert('RGB')

    # å®šä¹‰å¯¹è¯æµç¨‹
    conversation_turns = [
        "What do you see in this image?",
        "What color is it?",
        "What is it typically used for?"
    ]

    # æ„å»ºå¯¹è¯å†å²
    messages = []
    images = [image]

    for i, user_msg in enumerate(conversation_turns, 1):
        print(f"\nç¬¬{i}è½®å¯¹è¯:")
        print(f"â“ User: {user_msg}")

        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            content = [{"type": "image"}] if i == 1 else []
            content.append({"type": "text", "text": user_msg})
            messages.append({
                "role": "user",
                "content": content
            })

            # è·å–æ¨¡å‹å›å¤
            response = model.chat(messages, images)
            print(f"ğŸ’¬ InternVL: {response}")

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            messages.append({
                "role": "assistant",
                "content": [{"type": "text", "text": response}]
            })

        except Exception as e:
            print(f"âŒ Error: {e}")
            break


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="InternVLæ¨ç†ç¤ºä¾‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
    # åŸºç¡€æ¨ç†
    python internvl_inference.py --image path/to/image.jpg

    # æŒ‡å®šæ¨¡å‹
    python internvl_inference.py --image image.jpg --model OpenGVLab/InternVL2-8B

    # CPUæ¨¡å¼(ä¸æ¨è,å¾ˆæ…¢)
    python internvl_inference.py --image image.jpg --device cpu

    # å¤šå›¾ç†è§£
    python internvl_inference.py --images img1.jpg img2.jpg --demo multi_image

    # ä»…è¿è¡Œç‰¹å®šæ¼”ç¤º
    python internvl_inference.py --image image.jpg --demo caption
        """
    )

    parser.add_argument(
        "--image",
        type=str,
        help="è¾“å…¥å›¾åƒè·¯å¾„"
    )
    parser.add_argument(
        "--images",
        nargs="+",
        help="å¤šä¸ªå›¾åƒè·¯å¾„(ç”¨äºå¤šå›¾ç†è§£)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="OpenGVLab/InternVL2-8B",
        help="æ¨¡å‹åç§°æˆ–è·¯å¾„ (é»˜è®¤: OpenGVLab/InternVL2-8B)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cuda", "cpu"],
        help="è¿è¡Œè®¾å¤‡ (é»˜è®¤: auto)"
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="bfloat16",
        choices=["bfloat16", "float16", "float32"],
        help="æ•°æ®ç±»å‹ (é»˜è®¤: bfloat16)"
    )
    parser.add_argument(
        "--demo",
        type=str,
        choices=["all", "caption", "vqa", "ocr", "multi_image", "chat"],
        default="all",
        help="è¿è¡Œçš„æ¼”ç¤º (é»˜è®¤: all)"
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥
    if not args.image and not args.images:
        parser.error("è¯·æä¾› --image æˆ– --images å‚æ•°")

    image_path = args.image or args.images[0]
    image_paths = args.images or [args.image]

    # æ£€æŸ¥å›¾åƒæ–‡ä»¶
    for img_path in image_paths:
        if not os.path.exists(img_path):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
            return

    print("ğŸ¨ InternVL æ¨ç†ç¤ºä¾‹")
    print("="*60)
    print(f"ğŸ“ å›¾åƒ: {', '.join(image_paths)}")
    print(f"ğŸ¤– æ¨¡å‹: {args.model}")
    print(f"ğŸ’» è®¾å¤‡: {args.device}")

    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = InternVLInference(
            model_name=args.model,
            device=args.device,
            torch_dtype=args.dtype
        )

        # è¿è¡Œæ¼”ç¤º
        if args.demo == "all" or args.demo == "caption":
            demo_caption_generation(model, image_path)

        if args.demo == "all" or args.demo == "vqa":
            demo_vqa(model, image_path)

        if args.demo == "all" or args.demo == "ocr":
            demo_ocr(model, image_path)

        if args.demo == "all" or args.demo == "multi_image":
            demo_multi_image(model, image_paths)

        if args.demo == "all" or args.demo == "chat":
            demo_chat(model, image_path)

        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        print("="*60)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
