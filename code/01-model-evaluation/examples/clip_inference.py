#!/usr/bin/env python3
"""
CLIP æ¨ç†ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨CLIPæ¨¡å‹è¿›è¡Œå›¾æ–‡åŒ¹é…
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

import torch
from PIL import Image
import requests
from io import BytesIO

try:
    from transformers import CLIPProcessor, CLIPModel
except ImportError:
    print("âŒ é”™è¯¯: æœªå®‰è£…transformersåº“")
    print("è¯·è¿è¡Œ: pip install transformers")
    sys.exit(1)


def load_clip_model(model_name="openai/clip-vit-base-patch32", device=None):
    """
    åŠ è½½CLIPæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        device: è®¾å¤‡ (None=è‡ªåŠ¨æ£€æµ‹)
    
    Returns:
        (model, processor)
    """
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"[1/3] åŠ è½½æ¨¡å‹: {model_name}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        model = CLIPModel.from_pretrained(model_name)
        processor = CLIPProcessor.from_pretrained(model_name)
        model = model.to(device)
        model.eval()
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, processor, device
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\næç¤º:")
        print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("  2. å°è¯•æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹: ./scripts/download_models.sh clip")
        print("  3. ä½¿ç”¨é•œåƒæº: export HF_ENDPOINT=https://hf-mirror.com")
        sys.exit(1)


def load_image_from_url(url):
    """ä»URLåŠ è½½å›¾ç‰‡"""
    try:
        response = requests.get(url, timeout=10)
        image = Image.open(BytesIO(response.content))
        return image
    except Exception as e:
        print(f"âš ï¸ ä»URLåŠ è½½å›¾ç‰‡å¤±è´¥: {e}")
        return None


def load_image_from_file(path):
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å›¾ç‰‡"""
    try:
        image = Image.open(path)
        return image
    except Exception as e:
        print(f"âŒ ä»æ–‡ä»¶åŠ è½½å›¾ç‰‡å¤±è´¥: {e}")
        return None


def create_dummy_image():
    """åˆ›å»ºæµ‹è¯•å›¾ç‰‡"""
    import numpy as np
    from PIL import Image
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å½©è‰²æ¸å˜å›¾
    width, height = 224, 224
    array = np.zeros((height, width, 3), dtype=np.uint8)
    
    # çº¢è‰²æ¸å˜
    for i in range(height):
        array[i, :, 0] = int(255 * i / height)
    
    # ç»¿è‰²æ¸å˜
    for j in range(width):
        array[:, j, 1] = int(255 * j / width)
    
    # è“è‰²å›ºå®š
    array[:, :, 2] = 128
    
    return Image.fromarray(array)


def perform_inference(model, processor, image, text_candidates, device):
    """
    æ‰§è¡ŒCLIPæ¨ç†
    
    Args:
        model: CLIPæ¨¡å‹
        processor: CLIPå¤„ç†å™¨
        image: PIL Image
        text_candidates: æ–‡æœ¬å€™é€‰åˆ—è¡¨
        device: è®¾å¤‡
    
    Returns:
        æ¦‚ç‡åˆ†å¸ƒ
    """
    print("\n[3/3] æ‰§è¡Œæ¨ç†...")
    
    # å¤„ç†è¾“å…¥
    inputs = processor(
        text=text_candidates,
        images=image,
        return_tensors="pt",
        padding=True
    )
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # æ¨ç†
    with torch.no_grad():
        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1)
    
    print("âœ“ æ¨ç†å®Œæˆ")
    return probs


def display_results(text_candidates, probs):
    """æ˜¾ç¤ºç»“æœ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š åŒ¹é…ç»“æœ")
    print("=" * 60)
    
    # æ’åºç»“æœ
    sorted_indices = probs[0].argsort(descending=True)
    
    for rank, idx in enumerate(sorted_indices, 1):
        text = text_candidates[idx]
        confidence = probs[0][idx].item() * 100
        bar_length = int(confidence / 2)  # 50ä¸ªå­—ç¬¦æ»¡æ ¼
        bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
        
        print(f"\n{rank}. {text}")
        print(f"   {bar} {confidence:.2f}%")
    
    # æœ€ä½³åŒ¹é…
    best_idx = sorted_indices[0]
    best_text = text_candidates[best_idx]
    best_confidence = probs[0][best_idx].item() * 100
    
    print("\n" + "=" * 60)
    print(f"ğŸ¯ æœ€ä½³åŒ¹é…: {best_text} ({best_confidence:.2f}%)")
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("CLIP å›¾æ–‡åŒ¹é…ç¤ºä¾‹")
    print("=" * 60)
    print()
    
    # 1. åŠ è½½æ¨¡å‹
    model, processor, device = load_clip_model()
    
    # 2. å‡†å¤‡å›¾åƒ
    print("\n[2/3] å‡†å¤‡æµ‹è¯•å›¾åƒ")
    
    # å°è¯•ä»ç½‘ç»œåŠ è½½ç¤ºä¾‹å›¾ç‰‡
    image_url = "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"
    image = load_image_from_url(image_url)
    
    if image is None:
        print("âš ï¸ æ— æ³•ä»ç½‘ç»œåŠ è½½ç¤ºä¾‹å›¾ç‰‡")
        print("ä½¿ç”¨æœ¬åœ°ç”Ÿæˆçš„æµ‹è¯•å›¾ç‰‡")
        image = create_dummy_image()
        print("âœ“ åˆ›å»ºæµ‹è¯•å›¾ç‰‡æˆåŠŸ")
    else:
        print(f"âœ“ ä»ç½‘ç»œåŠ è½½å›¾ç‰‡æˆåŠŸ: {image.size}")
    
    # 3. å‡†å¤‡æ–‡æœ¬å€™é€‰
    text_candidates = [
        "ä¸€åªçŒ«",
        "ä¸€åªç‹—",
        "ä¸€ç¾¤é¹¦é¹‰",
        "ä¸€è¾†æ±½è½¦",
        "ä¸€åº§å»ºç­‘",
        "a photo of a cat",
        "a photo of a dog",
        "a photo of birds",
    ]
    
    print(f"\nå€™é€‰æ–‡æœ¬æ•°é‡: {len(text_candidates)}")
    for i, text in enumerate(text_candidates, 1):
        print(f"  {i}. {text}")
    
    # 4. æ‰§è¡Œæ¨ç†
    probs = perform_inference(model, processor, image, text_candidates, device)
    
    # 5. æ˜¾ç¤ºç»“æœ
    display_results(text_candidates, probs)
    
    # æç¤º
    print("\nâœ¨ æ­å–œï¼æ‚¨å·²æˆåŠŸè¿è¡ŒCLIPæ¨¡å‹æ¨ç†ï¼")
    print("\nğŸ“š æ¥ä¸‹æ¥å¯ä»¥:")
    print("  - ä¿®æ”¹ text_candidates å°è¯•ä¸åŒçš„æ–‡æœ¬")
    print("  - ä½¿ç”¨è‡ªå·±çš„å›¾ç‰‡: load_image_from_file('your_image.jpg')")
    print("  - æŸ¥çœ‹æ›´å¤šç¤ºä¾‹: code/01-model-evaluation/examples/")
    print("  - é˜…è¯»æ•™ç¨‹æ–‡æ¡£: docs/01-æ¨¡å‹è°ƒç ”ä¸é€‰å‹/")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

